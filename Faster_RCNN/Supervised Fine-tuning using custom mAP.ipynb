{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffad6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from evaluate_mAP import *\n",
    "import time\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758bcbd",
   "metadata": {},
   "source": [
    "### Create custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d8f4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vegeta(Dataset):\n",
    "    def __init__(self, root, annotation, transforms= None, subset_size=None):\n",
    "    \n",
    "        self.root = root #for storing the root directory path, will require later\n",
    "        self.annotation = annotation  #variable with location of the annotation file\n",
    "        self.transforms = transforms #assigning transformations applied to the images\n",
    "        self.coco = COCO(annotation)  #COCO is an object from pycocotools whcih provides access to different functions to access the dataset annotations in COCO format\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))  #self.coco.imgs gives keys are image IDs and the values contain metadata (such as file names, height, width, etc.) about each image in the dataset.\n",
    "\n",
    "        if subset_size is not None: #only take a subset of data\n",
    "            self.ids = self.ids[:subset_size] \n",
    "\n",
    "    def __getitem__(self, index): #way to access the class as a collection using the index\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]  #index will take you to the image id, we have sorted the ids above\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id) #gets the annotation ids for that image_id, an image_id can have multiple annotations id\n",
    "        coco_annotation = coco.loadAnns(ann_ids) #returns a list of annotations for that particular annotation_ids for an image_id\n",
    "       \n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        #but here the filename is in format '/qa/2070-full-qa/004ab015-2a61-4b42-8b75-3a5969a190ab_226059276-51.jpg'\n",
    "        image_filename = path.split('/')[-1]  # Extract filename from path\n",
    "        image_id = None\n",
    "        for img_info in coco.dataset['images']:\n",
    "            if image_filename in img_info['file_name'] : # stop when you find the image\n",
    "                image_id = img_info['id']\n",
    "                break\n",
    "        \n",
    "        if image_id is None:\n",
    "            print(f\"Warning: Image filename '{image_filename}' not found in annotations. Skipping this image.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            img = Image.open(os.path.join(self.root, image_filename)).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image file '{image_filename}' not found. Skipping this image.\")\n",
    "            return None  # Skip this image and continue\n",
    "  \n",
    "        \"\"\"\n",
    "        coco.loadImgs method loads metadata about the image with the given image_id, [0]\n",
    "        because the method returns a list of image \n",
    "        metadata dictionaries and we consider only first as we can get file path from any of them \n",
    "        \"\"\"\n",
    "        num_objs = len(coco_annotation) # returns the number of objects that are present in the image, could be 1 or 3 etc\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_objs): #since we can have multiple objs\n",
    "            ## annotation Format: [x, y, width, height]\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])  #add bb coordinates to list\n",
    "            labels.append(coco_annotation[i]['category_id'])  #add labels to list \n",
    "\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            image_id = torch.tensor([img_id])\n",
    "            \n",
    "            #value of 0 indicates that the object is not part of a crowd, and if 1, it would indicate that the object is part of a crowd \n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"image_id\"] = image_id\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):  #special function of class to find the length of the dataset\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88bccc",
   "metadata": {},
   "source": [
    "#### Load the training and validation data (can load a subset as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transform_list = [\n",
    "        transforms.Resize((600, 600)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    # if train:\n",
    "    #     transform_list.append(transforms.RandomHorizontalFlip(0.5)) # image transformation that randomly flips the image horizontally with a probability of 0.5 (50%)\n",
    "    return transforms.Compose(transform_list) \n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "     to define how to combine a list of samples (batches) into a single batch. Here we are removing empty items\n",
    "     from a batch. These items could come from the fact some of the annotations do not have the original image in\n",
    "     image folder\n",
    "    \"\"\"\n",
    "    # Filter out None values from the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None  # Return None if the entire batch is empty\n",
    "\n",
    "    # Separate images and targets\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    # Stack images into a single tensor\n",
    "    images = default_collate(images)\n",
    "\n",
    "    # Return images and targets as a tuple\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd43c40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.20s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_image_folder_path = './guardrail-damage/images/train'\n",
    "train_image_annotations_path = './guardrail-damage/annotation_files/train_filtered.json'\n",
    "\n",
    "val_image_folder_path = './guardrail-damage/images/validation'\n",
    "val_image_annotations_path = './guardrail-damage/annotation_files/validation.json'\n",
    "\n",
    "#load the training data\n",
    "# trainset = Vegeta(root=train_image_folder_path, annotation=train_image_annotations_path, transforms=get_transform(train=True), subset_size=1)\n",
    "trainset = Vegeta(root=train_image_folder_path, annotation=train_image_annotations_path, transforms=get_transform(train=True))\n",
    "train_loader = DataLoader(trainset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "# train_loader = DataLoader(trainset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "#load the validation data\n",
    "# validationset = Vegeta(root=val_image_folder_path, annotation=val_image_annotations_path, transforms=get_transform(train=False), subset_size=10)\n",
    "validationset = Vegeta(root=val_image_folder_path, annotation=val_image_annotations_path, transforms=get_transform(train=False))\n",
    "# val_loader = DataLoader(validationset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(validationset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e148d",
   "metadata": {},
   "source": [
    "#### Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ccffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "#shifting model to device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faster_rcnn_model_with_class(num_classes, model_checkpoint_path):\n",
    "    \"\"\"\n",
    "    Function to get a model last layer equalling your class\n",
    "    \"\"\"\n",
    "    # Load pre-trained Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    # Move the newly created layer to the same device as the model\n",
    "    model.roi_heads.box_predictor.to(device)\n",
    "    # Load trained model weights and map them to the correct device\n",
    "    model.load_state_dict(torch.load(model_checkpoint_path, map_location=device))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ce98997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/default/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/default/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_93549/3884932212.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_checkpoint_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "num_classes=2 #1 class + background\n",
    "model_checkpoint_path = \"./guardrail/models/guardrail_models_guardrails.pth\"\n",
    "model = get_faster_rcnn_model_with_class(num_classes, model_checkpoint_path)  # 1 classes + background\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40952a72",
   "metadata": {},
   "source": [
    "#### Training the model and saving every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ea4ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare predictions and ground truth for COCO evaluation\n",
    "def prepare_for_coco_eval(pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels, image_id):\n",
    "    # Convert predictions to COCO format\n",
    "    coco_preds = []\n",
    "    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        coco_preds.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": label,\n",
    "            \"bbox\": [xmin, ymin, width, height],\n",
    "            \"score\": score,\n",
    "        })\n",
    "\n",
    "    # Convert ground truth to COCO format\n",
    "    coco_gts = []\n",
    "    for box, label in zip(gt_boxes, gt_labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        coco_gts.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": label,\n",
    "            \"bbox\": [xmin, ymin, width, height],\n",
    "            \"iscrowd\": 0,  # Assuming no crowd annotations\n",
    "        })\n",
    "\n",
    "    return coco_preds, coco_gts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "218510d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_mAP(model, val_loader, device, coco_gt, confidence_threshold = 0.5, iou_threshold = 0.5, iou_function = 'GIoU'):\n",
    "    model.eval()\n",
    "    coco_preds = []\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(val_loader, desc=\"Processing Batches\", unit=\"batch\", leave=True, colour=\"green\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over all batches in the validation loader\n",
    "        for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Perform inference\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Process predictions for each image in the batch\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                boxes = prediction['boxes'].cpu().numpy()\n",
    "                scores = prediction['scores'].cpu().numpy()\n",
    "                labels = prediction['labels'].cpu().numpy()\n",
    "\n",
    "                # Filter predictions by confidence threshold\n",
    "                filtered_boxes = boxes[scores > confidence_threshold]\n",
    "                filtered_labels = labels[scores > confidence_threshold]\n",
    "                filtered_scores = scores[scores > confidence_threshold]\n",
    "\n",
    "                # print(\"filtered_boxes\", filtered_boxes)\n",
    "\n",
    "                # If no predictions are left after filtering, skip this image\n",
    "                if len(filtered_boxes) == 0:\n",
    "                    image_id = targets[i]['image_id'].item()\n",
    "                    print(f\"No predictions for image {image_id} after filtering.\")\n",
    "                    continue\n",
    "\n",
    "                # Find the image ID (assuming it's stored under 'image_id')\n",
    "                image_id = targets[i]['image_id'].item()\n",
    "\n",
    "\n",
    "                # Calculate IoU for each predicted box against all ground truth boxes\n",
    "                for pred_box, label, score in zip(filtered_boxes, filtered_labels, filtered_scores):\n",
    "                    pred_box = pred_box.tolist()  # Convert numpy array to list\n",
    "                    coco_preds.append({\n",
    "                        'image_id': image_id,\n",
    "                        'category_id': label,  # Category ID is from the prediction\n",
    "                        'bbox': [pred_box[0], pred_box[1], pred_box[2] - pred_box[0], pred_box[3] - pred_box[1]],  # Convert to COCO format (X, y, w, h)\n",
    "                        'score': score  # Prediction score\n",
    "                    })\n",
    "\n",
    "            # print(\"======== 1 image processed =====\")\n",
    "\n",
    "            # Update progress bar description with the current batch number\n",
    "            progress_bar.set_postfix(batch=batch_idx + 1)\n",
    "\n",
    "    print(f\"Total predictions collected: {len(coco_preds)}\")\n",
    "\n",
    "    \n",
    "    # Only proceed with COCO evaluation if there are predictions\n",
    "    if coco_preds:\n",
    "        # Running Custom mAP Function\n",
    "        mAP = evaluate_custom_mAP(coco_preds, ground_truths, iou_threshold, iou_function)\n",
    "        print(\"mAP:\", mAP)\n",
    "        return mAP\n",
    "    else:\n",
    "        print(\"No predictions to evaluate. mAP cannot be computed.\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7974e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005) #model hyperparameters and loss updation\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "coco_val_gt = COCO(val_image_annotations_path)\n",
    "best_mAP = 0.0\n",
    "confidence_threshold = 0.5\n",
    "iou_threshold = 0.5\n",
    "iou_function = 'GIoU'\n",
    "\n",
    "with open(val_image_annotations_path, \"r\") as f:\n",
    "    ground_truths = json.load(f)\n",
    "    ground_truths = ground_truths['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5535b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(0, num_epochs):\n",
    "#     # Start timing for training\n",
    "#     epoch_training_start_time = time.time()\n",
    "#     model.train() #setting model to training mode\n",
    "#     train_loss = 0  # Initialize epoch-level loss\n",
    "#     batch_losses = []  # List to store batch losses for logging\n",
    "    \n",
    "#     for batch_idx, (images, targets) in enumerate(train_loader):  # Iterate over data\n",
    "#         # Skip if the batch is empty (due to missing images)\n",
    "#         if images is None or targets is None:\n",
    "#             print(f\"Skipping empty batch {batch_idx + 1}\")\n",
    "#             continue\n",
    "#         # print(\"before\", device)\n",
    "#     # for images, targets in train_loader: #iterate over data\n",
    "#         images = list(image.to(device) for image in images) #moving all the images to device\n",
    "#         targets = [{key: value.to(device) for key, value in t.items()} for t in targets] #target will have labels and BB, used dict comprehension\n",
    "#         # print(\"after\", device)\n",
    "        \n",
    "#         #loss dictionary\n",
    "#         optimizer.zero_grad() #makes gradient zero for next iteration\n",
    "#         loss_dict = model(images, targets) #gets the loss \n",
    "#         losses = sum(loss for loss in loss_dict.values()) #adds the loss\n",
    "        \n",
    "#         losses.backward() #backprop\n",
    "#         optimizer.step() #\n",
    "\n",
    "#         train_loss += losses.item()\n",
    "\n",
    "#         batch_loss = losses.item()\n",
    "#         batch_losses.append(batch_loss)\n",
    "\n",
    "#         # Print batch loss\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}/{len(train_loader)}, Batch Loss: {batch_loss:.4f}\")\n",
    "\n",
    "#     # Calculate average epoch loss\n",
    "#     avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "#     # End timing for training\n",
    "#     epoch_training_end_time = time.time()\n",
    "#     epoch_training_time = epoch_training_end_time - epoch_training_start_time\n",
    "#     total_training_time += epoch_training_time\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs} - Average Train Loss: {avg_train_loss:.4f}\")\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs} - Training Time: {epoch_training_time:.2f} seconds\")\n",
    "\n",
    "#     #update learning rate \n",
    "#     lr_scheduler.step()\n",
    "    \n",
    "#     # validation file \n",
    "\n",
    "#     # Start timing for validation\n",
    "#     epoch_validation_start_time = time.time()\n",
    "    \n",
    "#     mAP = compute_mAP(model, val_loader, device, ground_truths, confidence_threshold, iou_threshold , iou_function)\n",
    "#     # print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss/len(train_loader):.4f}, mAP: {mAP:.4f}, mAP_50: {mAP_50:.4f}, mAP_75: {mAP_75:.4f}\")\n",
    "#     # print(f\"Epoch {epoch+1}/{num_epochs} -Train Loss: {train_loss/len(train_loader):.4f},  mAP: {mAP:.4f}\")\n",
    "\n",
    "#     # End timing for validation\n",
    "#     epoch_validation_end_time = time.time()\n",
    "#     epoch_validation_time = epoch_validation_end_time - epoch_validation_start_time\n",
    "#     total_validation_time += epoch_validation_time\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, mAP: {mAP:.4f}\")\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs} - Validation Time: {epoch_validation_time:.2f} seconds\")\n",
    "#     # Save the best model based on mAP\n",
    "#     if mAP > best_mAP:\n",
    "#         best_mAP = mAP\n",
    "#         # torch.save(model.state_dict(), 'best_model.pth')\n",
    "#         # Save the model's state dictionary after every epoch\n",
    "#     model_path = f\"fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n",
    "#     torch.save(model.state_dict(), model_path)\n",
    "#     # print(f\"Epoch {epoch+1}/{num_epochs} mAP: {losses.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e22697",
   "metadata": {},
   "source": [
    "#### writing the print statement to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683e850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 1/7596, Batch Loss: 34.8204\n",
      "Epoch 1/5, Batch 2/7596, Batch Loss: 42.2539\n",
      "Epoch 1/5, Batch 3/7596, Batch Loss: 30.5249\n",
      "Epoch 1/5, Batch 4/7596, Batch Loss: 26.7810\n",
      "Epoch 1/5, Batch 5/7596, Batch Loss: 16.0477\n",
      "Epoch 1/5, Batch 6/7596, Batch Loss: 29.0471\n",
      "Epoch 1/5, Batch 7/7596, Batch Loss: 13.0665\n",
      "Epoch 1/5, Batch 8/7596, Batch Loss: 13.8179\n",
      "Epoch 1/5, Batch 9/7596, Batch Loss: 12.5253\n",
      "Epoch 1/5, Batch 10/7596, Batch Loss: 14.2276\n",
      "Epoch 1/5, Batch 11/7596, Batch Loss: 24.4384\n",
      "Epoch 1/5, Batch 12/7596, Batch Loss: 13.1253\n",
      "Epoch 1/5, Batch 13/7596, Batch Loss: 21.2757\n",
      "Epoch 1/5, Batch 14/7596, Batch Loss: 18.7908\n",
      "Epoch 1/5, Batch 15/7596, Batch Loss: 5.2461\n",
      "Epoch 1/5, Batch 16/7596, Batch Loss: 10.0148\n",
      "Epoch 1/5, Batch 17/7596, Batch Loss: 12.6599\n",
      "Epoch 1/5, Batch 18/7596, Batch Loss: 6.8914\n",
      "Epoch 1/5, Batch 19/7596, Batch Loss: 8.1478\n",
      "Epoch 1/5, Batch 20/7596, Batch Loss: 13.8085\n",
      "Epoch 1/5, Batch 21/7596, Batch Loss: 9.7520\n",
      "Epoch 1/5, Batch 22/7596, Batch Loss: 11.3766\n",
      "Epoch 1/5, Batch 23/7596, Batch Loss: 7.2740\n",
      "Epoch 1/5, Batch 24/7596, Batch Loss: 35.8394\n",
      "Epoch 1/5, Batch 25/7596, Batch Loss: 11.2667\n",
      "Epoch 1/5, Batch 26/7596, Batch Loss: 10.0510\n",
      "Epoch 1/5, Batch 27/7596, Batch Loss: 12.0442\n",
      "Epoch 1/5, Batch 28/7596, Batch Loss: 8.6065\n",
      "Epoch 1/5, Batch 29/7596, Batch Loss: 7.5042\n",
      "Epoch 1/5, Batch 30/7596, Batch Loss: 20.5943\n",
      "Epoch 1/5, Batch 31/7596, Batch Loss: 15.4302\n",
      "Epoch 1/5, Batch 32/7596, Batch Loss: 12.4469\n",
      "Epoch 1/5, Batch 33/7596, Batch Loss: 15.9126\n",
      "Epoch 1/5, Batch 34/7596, Batch Loss: 14.5936\n",
      "Epoch 1/5, Batch 35/7596, Batch Loss: 288.7181\n",
      "Epoch 1/5, Batch 36/7596, Batch Loss: 9.6117\n",
      "Epoch 1/5, Batch 37/7596, Batch Loss: 18.2752\n",
      "Epoch 1/5, Batch 38/7596, Batch Loss: 20.4682\n",
      "Epoch 1/5, Batch 39/7596, Batch Loss: 18.7198\n",
      "Epoch 1/5, Batch 40/7596, Batch Loss: 11.0890\n",
      "Epoch 1/5, Batch 41/7596, Batch Loss: 18.9042\n",
      "Epoch 1/5, Batch 42/7596, Batch Loss: 11.6870\n",
      "Epoch 1/5, Batch 43/7596, Batch Loss: 9.1227\n",
      "Epoch 1/5, Batch 44/7596, Batch Loss: 15.3281\n",
      "Epoch 1/5, Batch 45/7596, Batch Loss: 11.2859\n",
      "Epoch 1/5, Batch 46/7596, Batch Loss: 10.2442\n",
      "Epoch 1/5, Batch 47/7596, Batch Loss: 10.1701\n",
      "Epoch 1/5, Batch 48/7596, Batch Loss: 13.6735\n",
      "Epoch 1/5, Batch 49/7596, Batch Loss: 11.3767\n",
      "Epoch 1/5, Batch 50/7596, Batch Loss: 9.0655\n",
      "Epoch 1/5, Batch 51/7596, Batch Loss: 7.3558\n",
      "Epoch 1/5, Batch 52/7596, Batch Loss: 10.3513\n",
      "Epoch 1/5, Batch 53/7596, Batch Loss: 12.1496\n",
      "Epoch 1/5, Batch 54/7596, Batch Loss: 6.5654\n",
      "Epoch 1/5, Batch 55/7596, Batch Loss: 8.6017\n",
      "Epoch 1/5, Batch 56/7596, Batch Loss: 7.2157\n",
      "Epoch 1/5, Batch 57/7596, Batch Loss: 5.7172\n",
      "Epoch 1/5, Batch 58/7596, Batch Loss: 5.5848\n",
      "Epoch 1/5, Batch 59/7596, Batch Loss: 14.1325\n",
      "Epoch 1/5, Batch 60/7596, Batch Loss: 15.2558\n",
      "Epoch 1/5, Batch 61/7596, Batch Loss: 13.1100\n",
      "Epoch 1/5, Batch 62/7596, Batch Loss: 34.2878\n",
      "Epoch 1/5, Batch 63/7596, Batch Loss: 10.7476\n",
      "Epoch 1/5, Batch 64/7596, Batch Loss: 19.7508\n",
      "Epoch 1/5, Batch 65/7596, Batch Loss: 10.9054\n",
      "Epoch 1/5, Batch 66/7596, Batch Loss: 7.5586\n",
      "Epoch 1/5, Batch 67/7596, Batch Loss: 11.3637\n",
      "Epoch 1/5, Batch 68/7596, Batch Loss: 15.8010\n",
      "Epoch 1/5, Batch 69/7596, Batch Loss: 32.2197\n",
      "Epoch 1/5, Batch 70/7596, Batch Loss: 7.1039\n",
      "Epoch 1/5, Batch 71/7596, Batch Loss: 9.6771\n",
      "Epoch 1/5, Batch 72/7596, Batch Loss: 14.2589\n",
      "Epoch 1/5, Batch 73/7596, Batch Loss: 9.8135\n",
      "Epoch 1/5, Batch 74/7596, Batch Loss: 14.1415\n",
      "Epoch 1/5, Batch 75/7596, Batch Loss: 15.4074\n",
      "Epoch 1/5, Batch 76/7596, Batch Loss: 22.9899\n",
      "Epoch 1/5, Batch 77/7596, Batch Loss: 20.8486\n",
      "Epoch 1/5, Batch 78/7596, Batch Loss: 10.5858\n",
      "Epoch 1/5, Batch 79/7596, Batch Loss: 9.3680\n",
      "Epoch 1/5, Batch 80/7596, Batch Loss: 21.3102\n",
      "Epoch 1/5, Batch 81/7596, Batch Loss: 19.3352\n",
      "Epoch 1/5, Batch 82/7596, Batch Loss: 12.2259\n",
      "Epoch 1/5, Batch 83/7596, Batch Loss: 13.8115\n",
      "Epoch 1/5, Batch 84/7596, Batch Loss: 11.5135\n",
      "Epoch 1/5, Batch 85/7596, Batch Loss: 11.6120\n",
      "Epoch 1/5, Batch 86/7596, Batch Loss: 14.5176\n",
      "Epoch 1/5, Batch 87/7596, Batch Loss: 6.4081\n",
      "Epoch 1/5, Batch 88/7596, Batch Loss: 3.7787\n",
      "Epoch 1/5, Batch 89/7596, Batch Loss: 24.0338\n",
      "Epoch 1/5, Batch 90/7596, Batch Loss: 10.5156\n",
      "Epoch 1/5, Batch 91/7596, Batch Loss: 9.3879\n",
      "Epoch 1/5, Batch 92/7596, Batch Loss: 12.5152\n",
      "Epoch 1/5, Batch 93/7596, Batch Loss: 10.0679\n",
      "Epoch 1/5, Batch 94/7596, Batch Loss: 9.9414\n",
      "Epoch 1/5, Batch 95/7596, Batch Loss: 7.2211\n",
      "Epoch 1/5, Batch 96/7596, Batch Loss: 4.0050\n",
      "Epoch 1/5, Batch 97/7596, Batch Loss: 18.0760\n",
      "Epoch 1/5, Batch 98/7596, Batch Loss: 19.1475\n",
      "Epoch 1/5, Batch 99/7596, Batch Loss: 6.8873\n",
      "Epoch 1/5, Batch 100/7596, Batch Loss: 7.7114\n",
      "Epoch 1/5, Batch 101/7596, Batch Loss: 8.7132\n",
      "Epoch 1/5, Batch 102/7596, Batch Loss: 6.7164\n",
      "Epoch 1/5, Batch 103/7596, Batch Loss: 18.6985\n",
      "Epoch 1/5, Batch 104/7596, Batch Loss: 46.4119\n",
      "Epoch 1/5, Batch 105/7596, Batch Loss: 10.3145\n",
      "Epoch 1/5, Batch 106/7596, Batch Loss: 6.5061\n",
      "Epoch 1/5, Batch 107/7596, Batch Loss: 5.8234\n",
      "Epoch 1/5, Batch 108/7596, Batch Loss: 9.2082\n",
      "Epoch 1/5, Batch 109/7596, Batch Loss: 6.1132\n",
      "Epoch 1/5, Batch 110/7596, Batch Loss: 10.0875\n",
      "Epoch 1/5, Batch 111/7596, Batch Loss: 14.4060\n",
      "Epoch 1/5, Batch 112/7596, Batch Loss: 15.3826\n",
      "Epoch 1/5, Batch 113/7596, Batch Loss: 6.1523\n",
      "Epoch 1/5, Batch 114/7596, Batch Loss: 13.9629\n",
      "Epoch 1/5, Batch 115/7596, Batch Loss: 4.2979\n",
      "Epoch 1/5, Batch 116/7596, Batch Loss: 16.9493\n",
      "Epoch 1/5, Batch 117/7596, Batch Loss: 13.8728\n",
      "Epoch 1/5, Batch 118/7596, Batch Loss: 7.7727\n",
      "Epoch 1/5, Batch 119/7596, Batch Loss: 12.5049\n",
      "Epoch 1/5, Batch 120/7596, Batch Loss: 8.5117\n",
      "Epoch 1/5, Batch 121/7596, Batch Loss: 13.6513\n",
      "Epoch 1/5, Batch 122/7596, Batch Loss: 7.5310\n",
      "Epoch 1/5, Batch 123/7596, Batch Loss: 7.4594\n",
      "Epoch 1/5, Batch 124/7596, Batch Loss: 7.2018\n",
      "Epoch 1/5, Batch 125/7596, Batch Loss: 13.4319\n",
      "Epoch 1/5, Batch 126/7596, Batch Loss: 4.3381\n",
      "Epoch 1/5, Batch 127/7596, Batch Loss: 15.2895\n",
      "Epoch 1/5, Batch 128/7596, Batch Loss: 12.1188\n",
      "Epoch 1/5, Batch 129/7596, Batch Loss: 14.4147\n",
      "Epoch 1/5, Batch 130/7596, Batch Loss: 8.2986\n",
      "Epoch 1/5, Batch 131/7596, Batch Loss: 16.3347\n",
      "Epoch 1/5, Batch 132/7596, Batch Loss: 7.1100\n",
      "Epoch 1/5, Batch 133/7596, Batch Loss: 8.7832\n",
      "Epoch 1/5, Batch 134/7596, Batch Loss: 12.9171\n",
      "Epoch 1/5, Batch 135/7596, Batch Loss: 7.7520\n",
      "Epoch 1/5, Batch 136/7596, Batch Loss: 8.3286\n",
      "Epoch 1/5, Batch 137/7596, Batch Loss: 10.5013\n",
      "Epoch 1/5, Batch 138/7596, Batch Loss: 10.4866\n",
      "Epoch 1/5, Batch 139/7596, Batch Loss: 8.0993\n",
      "Epoch 1/5, Batch 140/7596, Batch Loss: 13.5000\n",
      "Epoch 1/5, Batch 141/7596, Batch Loss: 4.1139\n",
      "Epoch 1/5, Batch 142/7596, Batch Loss: 3.1266\n",
      "Epoch 1/5, Batch 143/7596, Batch Loss: 3.1817\n",
      "Epoch 1/5, Batch 144/7596, Batch Loss: 66.4534\n",
      "Epoch 1/5, Batch 145/7596, Batch Loss: 9.0882\n",
      "Epoch 1/5, Batch 146/7596, Batch Loss: 8.6623\n",
      "Epoch 1/5, Batch 147/7596, Batch Loss: 11.4226\n",
      "Epoch 1/5, Batch 148/7596, Batch Loss: 10.8016\n",
      "Epoch 1/5, Batch 149/7596, Batch Loss: 8.6964\n",
      "Epoch 1/5, Batch 150/7596, Batch Loss: 10.0933\n",
      "Epoch 1/5, Batch 151/7596, Batch Loss: 8.3557\n",
      "Epoch 1/5, Batch 152/7596, Batch Loss: 8.6918\n",
      "Epoch 1/5, Batch 153/7596, Batch Loss: 5.0841\n",
      "Epoch 1/5, Batch 154/7596, Batch Loss: 10.2855\n",
      "Epoch 1/5, Batch 155/7596, Batch Loss: 10.4399\n",
      "Epoch 1/5, Batch 156/7596, Batch Loss: 6.5818\n",
      "Epoch 1/5, Batch 157/7596, Batch Loss: 9.4793\n",
      "Epoch 1/5, Batch 158/7596, Batch Loss: 7.8193\n",
      "Epoch 1/5, Batch 159/7596, Batch Loss: 6.4128\n",
      "Epoch 1/5, Batch 160/7596, Batch Loss: 8.4081\n",
      "Epoch 1/5, Batch 161/7596, Batch Loss: 16.3666\n",
      "Epoch 1/5, Batch 162/7596, Batch Loss: 8.0130\n",
      "Epoch 1/5, Batch 163/7596, Batch Loss: 11.1085\n",
      "Epoch 1/5, Batch 164/7596, Batch Loss: 5.3861\n",
      "Epoch 1/5, Batch 165/7596, Batch Loss: 11.8083\n",
      "Epoch 1/5, Batch 166/7596, Batch Loss: 13.8858\n",
      "Epoch 1/5, Batch 167/7596, Batch Loss: 13.0974\n",
      "Epoch 1/5, Batch 168/7596, Batch Loss: 4.1746\n",
      "Epoch 1/5, Batch 169/7596, Batch Loss: 9.7178\n",
      "Epoch 1/5, Batch 170/7596, Batch Loss: 8.9382\n",
      "Epoch 1/5, Batch 171/7596, Batch Loss: 15.1118\n",
      "Epoch 1/5, Batch 172/7596, Batch Loss: 4.1751\n",
      "Epoch 1/5, Batch 173/7596, Batch Loss: 8.7445\n",
      "Epoch 1/5, Batch 174/7596, Batch Loss: 8.1647\n",
      "Epoch 1/5, Batch 175/7596, Batch Loss: 8.9578\n",
      "Epoch 1/5, Batch 176/7596, Batch Loss: 10.4079\n",
      "Epoch 1/5, Batch 177/7596, Batch Loss: 9.2706\n",
      "Epoch 1/5, Batch 178/7596, Batch Loss: 9.4029\n",
      "Epoch 1/5, Batch 179/7596, Batch Loss: 3.8827\n",
      "Epoch 1/5, Batch 180/7596, Batch Loss: 15.5050\n",
      "Epoch 1/5, Batch 181/7596, Batch Loss: 5.9869\n",
      "Epoch 1/5, Batch 182/7596, Batch Loss: 8.2243\n",
      "Epoch 1/5, Batch 183/7596, Batch Loss: 8.4639\n",
      "Epoch 1/5, Batch 184/7596, Batch Loss: 13.4154\n",
      "Epoch 1/5, Batch 185/7596, Batch Loss: 13.5589\n",
      "Epoch 1/5, Batch 186/7596, Batch Loss: 8.8525\n",
      "Epoch 1/5, Batch 187/7596, Batch Loss: 16.2762\n",
      "Epoch 1/5, Batch 188/7596, Batch Loss: 19.8875\n",
      "Epoch 1/5, Batch 189/7596, Batch Loss: 12.9444\n",
      "Epoch 1/5, Batch 190/7596, Batch Loss: 18.9896\n",
      "Epoch 1/5, Batch 191/7596, Batch Loss: 15.2753\n",
      "Epoch 1/5, Batch 192/7596, Batch Loss: 17.1665\n",
      "Epoch 1/5, Batch 193/7596, Batch Loss: 16.2162\n",
      "Epoch 1/5, Batch 194/7596, Batch Loss: 13.3553\n",
      "Epoch 1/5, Batch 195/7596, Batch Loss: 13.4585\n",
      "Epoch 1/5, Batch 196/7596, Batch Loss: 9.3849\n",
      "Epoch 1/5, Batch 197/7596, Batch Loss: 6.9959\n",
      "Epoch 1/5, Batch 198/7596, Batch Loss: 18.5326\n",
      "Epoch 1/5, Batch 199/7596, Batch Loss: 8.2081\n",
      "Epoch 1/5, Batch 200/7596, Batch Loss: 16.9701\n",
      "Epoch 1/5, Batch 201/7596, Batch Loss: 13.7360\n",
      "Epoch 1/5, Batch 202/7596, Batch Loss: 17.9091\n",
      "Epoch 1/5, Batch 203/7596, Batch Loss: 8.2264\n",
      "Epoch 1/5, Batch 204/7596, Batch Loss: 46.3170\n",
      "Epoch 1/5, Batch 205/7596, Batch Loss: 8.1051\n",
      "Epoch 1/5, Batch 206/7596, Batch Loss: 12.5031\n",
      "Epoch 1/5, Batch 207/7596, Batch Loss: 8.6668\n",
      "Epoch 1/5, Batch 208/7596, Batch Loss: 4.5641\n",
      "Epoch 1/5, Batch 209/7596, Batch Loss: 8.2994\n",
      "Epoch 1/5, Batch 210/7596, Batch Loss: 9.4010\n",
      "Epoch 1/5, Batch 211/7596, Batch Loss: 13.3553\n",
      "Epoch 1/5, Batch 212/7596, Batch Loss: 10.6932\n",
      "Epoch 1/5, Batch 213/7596, Batch Loss: 11.1776\n",
      "Epoch 1/5, Batch 214/7596, Batch Loss: 7.4036\n",
      "Epoch 1/5, Batch 215/7596, Batch Loss: 7.6254\n",
      "Epoch 1/5, Batch 216/7596, Batch Loss: 12.0571\n",
      "Epoch 1/5, Batch 217/7596, Batch Loss: 3.8109\n",
      "Epoch 1/5, Batch 218/7596, Batch Loss: 11.5986\n",
      "Epoch 1/5, Batch 219/7596, Batch Loss: 5.7902\n",
      "Epoch 1/5, Batch 220/7596, Batch Loss: 14.5180\n",
      "Epoch 1/5, Batch 221/7596, Batch Loss: 6.4106\n",
      "Epoch 1/5, Batch 222/7596, Batch Loss: 10.0821\n",
      "Epoch 1/5, Batch 223/7596, Batch Loss: 22.2425\n",
      "Epoch 1/5, Batch 224/7596, Batch Loss: 10.0784\n",
      "Epoch 1/5, Batch 225/7596, Batch Loss: 5.8456\n",
      "Epoch 1/5, Batch 226/7596, Batch Loss: 15.0139\n",
      "Epoch 1/5, Batch 227/7596, Batch Loss: 10.5521\n",
      "Epoch 1/5, Batch 228/7596, Batch Loss: 11.1217\n",
      "Epoch 1/5, Batch 229/7596, Batch Loss: 10.9567\n",
      "Epoch 1/5, Batch 230/7596, Batch Loss: 7.8520\n",
      "Epoch 1/5, Batch 231/7596, Batch Loss: 8.9398\n",
      "Epoch 1/5, Batch 232/7596, Batch Loss: 17.8330\n",
      "Epoch 1/5, Batch 233/7596, Batch Loss: 10.9255\n",
      "Epoch 1/5, Batch 234/7596, Batch Loss: 15.6877\n",
      "Epoch 1/5, Batch 235/7596, Batch Loss: 17.2601\n",
      "Epoch 1/5, Batch 236/7596, Batch Loss: 10.5092\n",
      "Epoch 1/5, Batch 237/7596, Batch Loss: 10.7617\n",
      "Epoch 1/5, Batch 238/7596, Batch Loss: 8.9368\n",
      "Epoch 1/5, Batch 239/7596, Batch Loss: 16.5012\n",
      "Epoch 1/5, Batch 240/7596, Batch Loss: 17.5152\n",
      "Epoch 1/5, Batch 241/7596, Batch Loss: 15.8365\n",
      "Epoch 1/5, Batch 242/7596, Batch Loss: 10.3229\n",
      "Epoch 1/5, Batch 243/7596, Batch Loss: 8.1620\n",
      "Epoch 1/5, Batch 244/7596, Batch Loss: 10.1874\n",
      "Epoch 1/5, Batch 245/7596, Batch Loss: 6.0634\n",
      "Epoch 1/5, Batch 246/7596, Batch Loss: 28.1842\n",
      "Epoch 1/5, Batch 247/7596, Batch Loss: 10.9549\n",
      "Epoch 1/5, Batch 248/7596, Batch Loss: 3.3660\n",
      "Epoch 1/5, Batch 249/7596, Batch Loss: 16.8710\n",
      "Epoch 1/5, Batch 250/7596, Batch Loss: 12.3275\n",
      "Epoch 1/5, Batch 251/7596, Batch Loss: 13.2056\n",
      "Epoch 1/5, Batch 252/7596, Batch Loss: 4.6527\n",
      "Epoch 1/5, Batch 253/7596, Batch Loss: 9.1068\n",
      "Epoch 1/5, Batch 254/7596, Batch Loss: 9.3779\n",
      "Epoch 1/5, Batch 255/7596, Batch Loss: 8.2818\n",
      "Epoch 1/5, Batch 256/7596, Batch Loss: 6.1378\n",
      "Epoch 1/5, Batch 257/7596, Batch Loss: 11.6513\n",
      "Epoch 1/5, Batch 258/7596, Batch Loss: 204.0752\n",
      "Epoch 1/5, Batch 259/7596, Batch Loss: 9.4395\n",
      "Epoch 1/5, Batch 260/7596, Batch Loss: 17.3486\n",
      "Epoch 1/5, Batch 261/7596, Batch Loss: 18.2322\n",
      "Epoch 1/5, Batch 262/7596, Batch Loss: 41.5482\n",
      "Epoch 1/5, Batch 263/7596, Batch Loss: 23.8621\n",
      "Epoch 1/5, Batch 264/7596, Batch Loss: 15.4954\n",
      "Epoch 1/5, Batch 265/7596, Batch Loss: 21.7110\n",
      "Epoch 1/5, Batch 266/7596, Batch Loss: 44.6521\n",
      "Epoch 1/5, Batch 267/7596, Batch Loss: 18.8990\n",
      "Epoch 1/5, Batch 268/7596, Batch Loss: 15.5391\n",
      "Epoch 1/5, Batch 269/7596, Batch Loss: 17.2409\n",
      "Epoch 1/5, Batch 270/7596, Batch Loss: 11.2568\n",
      "Epoch 1/5, Batch 271/7596, Batch Loss: 7.9367\n",
      "Epoch 1/5, Batch 272/7596, Batch Loss: 36.9355\n",
      "Epoch 1/5, Batch 273/7596, Batch Loss: 23.1162\n",
      "Epoch 1/5, Batch 274/7596, Batch Loss: 50.9276\n",
      "Epoch 1/5, Batch 275/7596, Batch Loss: 11.1096\n",
      "Epoch 1/5, Batch 276/7596, Batch Loss: 11.9049\n",
      "Epoch 1/5, Batch 277/7596, Batch Loss: 13.8774\n",
      "Epoch 1/5, Batch 278/7596, Batch Loss: 13.3973\n",
      "Epoch 1/5, Batch 279/7596, Batch Loss: 12.6598\n",
      "Epoch 1/5, Batch 280/7596, Batch Loss: 15.7066\n",
      "Epoch 1/5, Batch 281/7596, Batch Loss: 16.4495\n",
      "Epoch 1/5, Batch 282/7596, Batch Loss: 9.3271\n",
      "Epoch 1/5, Batch 283/7596, Batch Loss: 9.7325\n",
      "Epoch 1/5, Batch 284/7596, Batch Loss: 6.9261\n",
      "Epoch 1/5, Batch 285/7596, Batch Loss: 14.1321\n",
      "Epoch 1/5, Batch 286/7596, Batch Loss: 15.1829\n",
      "Epoch 1/5, Batch 287/7596, Batch Loss: 10.8496\n",
      "Epoch 1/5, Batch 288/7596, Batch Loss: 15.6665\n",
      "Epoch 1/5, Batch 289/7596, Batch Loss: 22.9280\n",
      "Epoch 1/5, Batch 290/7596, Batch Loss: 17.2630\n",
      "Epoch 1/5, Batch 291/7596, Batch Loss: 13.0512\n",
      "Epoch 1/5, Batch 292/7596, Batch Loss: 17.6796\n",
      "Epoch 1/5, Batch 293/7596, Batch Loss: 16.8593\n",
      "Epoch 1/5, Batch 294/7596, Batch Loss: 14.7550\n",
      "Epoch 1/5, Batch 295/7596, Batch Loss: 26.0450\n",
      "Epoch 1/5, Batch 296/7596, Batch Loss: 32.9117\n",
      "Epoch 1/5, Batch 297/7596, Batch Loss: 36.3705\n",
      "Epoch 1/5, Batch 298/7596, Batch Loss: 16.4161\n",
      "Epoch 1/5, Batch 299/7596, Batch Loss: 26.5197\n",
      "Epoch 1/5, Batch 300/7596, Batch Loss: 24.3480\n",
      "Epoch 1/5, Batch 301/7596, Batch Loss: 23.2534\n",
      "Epoch 1/5, Batch 302/7596, Batch Loss: 14.5679\n",
      "Epoch 1/5, Batch 303/7596, Batch Loss: 20.1194\n",
      "Epoch 1/5, Batch 304/7596, Batch Loss: 14.3945\n",
      "Epoch 1/5, Batch 305/7596, Batch Loss: 8.9652\n",
      "Epoch 1/5, Batch 306/7596, Batch Loss: 18.8235\n",
      "Epoch 1/5, Batch 307/7596, Batch Loss: 13.4927\n",
      "Epoch 1/5, Batch 308/7596, Batch Loss: 8.7380\n",
      "Epoch 1/5, Batch 309/7596, Batch Loss: 21.3218\n",
      "Epoch 1/5, Batch 310/7596, Batch Loss: 11.0035\n",
      "Epoch 1/5, Batch 311/7596, Batch Loss: 15.9840\n",
      "Epoch 1/5, Batch 312/7596, Batch Loss: 26.5944\n",
      "Epoch 1/5, Batch 313/7596, Batch Loss: 13.7125\n",
      "Epoch 1/5, Batch 314/7596, Batch Loss: 25.1672\n",
      "Epoch 1/5, Batch 315/7596, Batch Loss: 68.8225\n",
      "Epoch 1/5, Batch 316/7596, Batch Loss: 1448.8458\n",
      "Epoch 1/5, Batch 317/7596, Batch Loss: 34.4128\n",
      "Epoch 1/5, Batch 318/7596, Batch Loss: 36.0682\n",
      "Epoch 1/5, Batch 319/7596, Batch Loss: 21.2617\n",
      "Epoch 1/5, Batch 320/7596, Batch Loss: 722.1749\n",
      "Epoch 1/5, Batch 321/7596, Batch Loss: 20.0593\n",
      "Epoch 1/5, Batch 322/7596, Batch Loss: 47.7207\n",
      "Epoch 1/5, Batch 323/7596, Batch Loss: 212.5433\n",
      "Epoch 1/5, Batch 324/7596, Batch Loss: 21.7035\n",
      "Epoch 1/5, Batch 325/7596, Batch Loss: 18.8047\n",
      "Epoch 1/5, Batch 326/7596, Batch Loss: 19.4781\n",
      "Epoch 1/5, Batch 327/7596, Batch Loss: 6017.2671\n",
      "Epoch 1/5, Batch 328/7596, Batch Loss: 17.7167\n",
      "Epoch 1/5, Batch 329/7596, Batch Loss: 630.5321\n",
      "Epoch 1/5, Batch 330/7596, Batch Loss: 756.5338\n",
      "Epoch 1/5, Batch 331/7596, Batch Loss: 227.7119\n",
      "Epoch 1/5, Batch 332/7596, Batch Loss: 22.6498\n",
      "Epoch 1/5, Batch 333/7596, Batch Loss: 24.7127\n",
      "Epoch 1/5, Batch 334/7596, Batch Loss: 38.9546\n",
      "Epoch 1/5, Batch 335/7596, Batch Loss: 47.5617\n",
      "Epoch 1/5, Batch 336/7596, Batch Loss: 29.2990\n",
      "Epoch 1/5, Batch 337/7596, Batch Loss: 136.4280\n",
      "Epoch 1/5, Batch 338/7596, Batch Loss: 22.8307\n",
      "Epoch 1/5, Batch 339/7596, Batch Loss: 18.6784\n",
      "Epoch 1/5, Batch 340/7596, Batch Loss: 22.6633\n",
      "Epoch 1/5, Batch 341/7596, Batch Loss: 20.9590\n",
      "Epoch 1/5, Batch 342/7596, Batch Loss: 24.9201\n",
      "Epoch 1/5, Batch 343/7596, Batch Loss: 39.7061\n",
      "Epoch 1/5, Batch 344/7596, Batch Loss: 28.2386\n",
      "Epoch 1/5, Batch 345/7596, Batch Loss: 60460.4414\n",
      "Epoch 1/5, Batch 346/7596, Batch Loss: 19.7773\n",
      "Epoch 1/5, Batch 347/7596, Batch Loss: 252.8726\n",
      "Epoch 1/5, Batch 348/7596, Batch Loss: 34.4940\n",
      "Epoch 1/5, Batch 349/7596, Batch Loss: 26.2535\n",
      "Epoch 1/5, Batch 350/7596, Batch Loss: 564.8430\n",
      "Epoch 1/5, Batch 351/7596, Batch Loss: 975.6075\n",
      "Epoch 1/5, Batch 352/7596, Batch Loss: 152.9279\n",
      "Epoch 1/5, Batch 353/7596, Batch Loss: 1589.9424\n",
      "Epoch 1/5, Batch 354/7596, Batch Loss: 180.7081\n",
      "Epoch 1/5, Batch 355/7596, Batch Loss: 2159.3787\n",
      "Epoch 1/5, Batch 356/7596, Batch Loss: 72.2089\n",
      "Epoch 1/5, Batch 357/7596, Batch Loss: 75.6791\n",
      "Epoch 1/5, Batch 358/7596, Batch Loss: 84.9835\n",
      "Epoch 1/5, Batch 359/7596, Batch Loss: 90.9633\n",
      "Epoch 1/5, Batch 360/7596, Batch Loss: 79.1490\n",
      "Epoch 1/5, Batch 361/7596, Batch Loss: 98.7275\n",
      "Epoch 1/5, Batch 362/7596, Batch Loss: 720.0410\n",
      "Epoch 1/5, Batch 363/7596, Batch Loss: 40.7142\n",
      "Epoch 1/5, Batch 364/7596, Batch Loss: 107.1139\n",
      "Epoch 1/5, Batch 365/7596, Batch Loss: 78.5132\n",
      "Epoch 1/5, Batch 366/7596, Batch Loss: 47.1682\n",
      "Epoch 1/5, Batch 367/7596, Batch Loss: 72.6029\n",
      "Epoch 1/5, Batch 368/7596, Batch Loss: 201.7169\n",
      "Epoch 1/5, Batch 369/7596, Batch Loss: 34.2815\n",
      "Epoch 1/5, Batch 370/7596, Batch Loss: 46.4916\n",
      "Epoch 1/5, Batch 371/7596, Batch Loss: 46.4117\n",
      "Epoch 1/5, Batch 372/7596, Batch Loss: 26.8508\n",
      "Epoch 1/5, Batch 373/7596, Batch Loss: 150.9486\n",
      "Epoch 1/5, Batch 374/7596, Batch Loss: 133.9062\n",
      "Epoch 1/5, Batch 375/7596, Batch Loss: 110.8811\n",
      "Epoch 1/5, Batch 376/7596, Batch Loss: 91.1065\n",
      "Epoch 1/5, Batch 377/7596, Batch Loss: 34.4701\n",
      "Epoch 1/5, Batch 378/7596, Batch Loss: 74.0331\n",
      "Epoch 1/5, Batch 379/7596, Batch Loss: 25.3595\n",
      "Epoch 1/5, Batch 380/7596, Batch Loss: 131.6644\n",
      "Epoch 1/5, Batch 381/7596, Batch Loss: 121.6714\n",
      "Epoch 1/5, Batch 382/7596, Batch Loss: 40.5646\n",
      "Epoch 1/5, Batch 383/7596, Batch Loss: 35.5941\n",
      "Epoch 1/5, Batch 384/7596, Batch Loss: 4435.8379\n",
      "Epoch 1/5, Batch 385/7596, Batch Loss: 52.8165\n",
      "Epoch 1/5, Batch 386/7596, Batch Loss: 110.2518\n",
      "Epoch 1/5, Batch 387/7596, Batch Loss: 747.9237\n",
      "Epoch 1/5, Batch 388/7596, Batch Loss: 43.4902\n",
      "Epoch 1/5, Batch 389/7596, Batch Loss: 205.5490\n",
      "Epoch 1/5, Batch 390/7596, Batch Loss: 57.3827\n",
      "Epoch 1/5, Batch 391/7596, Batch Loss: 70.8528\n",
      "Epoch 1/5, Batch 392/7596, Batch Loss: 52.2349\n",
      "Epoch 1/5, Batch 393/7596, Batch Loss: 39.8295\n",
      "Epoch 1/5, Batch 394/7596, Batch Loss: 33.4613\n",
      "Epoch 1/5, Batch 395/7596, Batch Loss: 31.9776\n",
      "Epoch 1/5, Batch 396/7596, Batch Loss: 33.9872\n",
      "Epoch 1/5, Batch 397/7596, Batch Loss: 62.0246\n",
      "Epoch 1/5, Batch 398/7596, Batch Loss: 9880.7764\n",
      "Epoch 1/5, Batch 399/7596, Batch Loss: 136.1853\n",
      "Epoch 1/5, Batch 400/7596, Batch Loss: 10203.8652\n",
      "Epoch 1/5, Batch 401/7596, Batch Loss: 50.6430\n",
      "Epoch 1/5, Batch 402/7596, Batch Loss: 38.8028\n",
      "Epoch 1/5, Batch 403/7596, Batch Loss: 48.5782\n",
      "Epoch 1/5, Batch 404/7596, Batch Loss: 161.8437\n",
      "Epoch 1/5, Batch 405/7596, Batch Loss: 105.3054\n",
      "Epoch 1/5, Batch 406/7596, Batch Loss: 56.0272\n",
      "Epoch 1/5, Batch 407/7596, Batch Loss: 38.6644\n",
      "Epoch 1/5, Batch 408/7596, Batch Loss: 21.2628\n",
      "Epoch 1/5, Batch 409/7596, Batch Loss: 182.3610\n",
      "Epoch 1/5, Batch 410/7596, Batch Loss: 99.6276\n",
      "Epoch 1/5, Batch 411/7596, Batch Loss: 120.6062\n",
      "Epoch 1/5, Batch 412/7596, Batch Loss: 41.6538\n",
      "Epoch 1/5, Batch 413/7596, Batch Loss: 177.3029\n",
      "Epoch 1/5, Batch 414/7596, Batch Loss: 89.6673\n",
      "Epoch 1/5, Batch 415/7596, Batch Loss: 96.3597\n",
      "Epoch 1/5, Batch 416/7596, Batch Loss: 36.3998\n",
      "Epoch 1/5, Batch 417/7596, Batch Loss: 57.3305\n",
      "Epoch 1/5, Batch 418/7596, Batch Loss: 62.1565\n",
      "Epoch 1/5, Batch 419/7596, Batch Loss: 36.2549\n",
      "Epoch 1/5, Batch 420/7596, Batch Loss: 41.9833\n",
      "Epoch 1/5, Batch 421/7596, Batch Loss: 119.6031\n",
      "Epoch 1/5, Batch 422/7596, Batch Loss: 37.3816\n",
      "Epoch 1/5, Batch 423/7596, Batch Loss: 63.7630\n",
      "Epoch 1/5, Batch 424/7596, Batch Loss: 28.8102\n",
      "Epoch 1/5, Batch 425/7596, Batch Loss: 49.8878\n",
      "Epoch 1/5, Batch 426/7596, Batch Loss: 116.9964\n",
      "Epoch 1/5, Batch 427/7596, Batch Loss: 41.7985\n",
      "Epoch 1/5, Batch 428/7596, Batch Loss: 128.5574\n",
      "Epoch 1/5, Batch 429/7596, Batch Loss: 59.4161\n",
      "Epoch 1/5, Batch 430/7596, Batch Loss: 54.0033\n",
      "Epoch 1/5, Batch 431/7596, Batch Loss: 48.3447\n",
      "Epoch 1/5, Batch 432/7596, Batch Loss: 50.4705\n",
      "Epoch 1/5, Batch 433/7596, Batch Loss: 92.6773\n",
      "Epoch 1/5, Batch 434/7596, Batch Loss: 51.9382\n",
      "Epoch 1/5, Batch 435/7596, Batch Loss: 52.1948\n",
      "Epoch 1/5, Batch 436/7596, Batch Loss: 248.2149\n",
      "Epoch 1/5, Batch 437/7596, Batch Loss: 48.0414\n",
      "Epoch 1/5, Batch 438/7596, Batch Loss: 48.7677\n",
      "Epoch 1/5, Batch 439/7596, Batch Loss: 37.4053\n",
      "Epoch 1/5, Batch 440/7596, Batch Loss: 28.2857\n",
      "Epoch 1/5, Batch 441/7596, Batch Loss: 100.1018\n",
      "Epoch 1/5, Batch 442/7596, Batch Loss: 165.5705\n",
      "Epoch 1/5, Batch 443/7596, Batch Loss: 53.8738\n",
      "Epoch 1/5, Batch 444/7596, Batch Loss: 26.3974\n",
      "Epoch 1/5, Batch 445/7596, Batch Loss: 96.7095\n",
      "Epoch 1/5, Batch 446/7596, Batch Loss: 33.8861\n",
      "Epoch 1/5, Batch 447/7596, Batch Loss: 19.9764\n",
      "Epoch 1/5, Batch 448/7596, Batch Loss: 63.1457\n",
      "Epoch 1/5, Batch 449/7596, Batch Loss: 21.1557\n",
      "Epoch 1/5, Batch 450/7596, Batch Loss: 37.0594\n",
      "Epoch 1/5, Batch 451/7596, Batch Loss: 26.6966\n",
      "Epoch 1/5, Batch 452/7596, Batch Loss: 43.6793\n",
      "Epoch 1/5, Batch 453/7596, Batch Loss: 34.5196\n",
      "Epoch 1/5, Batch 454/7596, Batch Loss: 48.0110\n",
      "Epoch 1/5, Batch 455/7596, Batch Loss: 31.5177\n",
      "Epoch 1/5, Batch 456/7596, Batch Loss: 20.1980\n",
      "Epoch 1/5, Batch 457/7596, Batch Loss: 29.3465\n",
      "Epoch 1/5, Batch 458/7596, Batch Loss: 18.7368\n",
      "Epoch 1/5, Batch 459/7596, Batch Loss: 28.7218\n",
      "Epoch 1/5, Batch 460/7596, Batch Loss: 127.1597\n",
      "Epoch 1/5, Batch 461/7596, Batch Loss: 131.1591\n",
      "Epoch 1/5, Batch 462/7596, Batch Loss: 43.2584\n",
      "Epoch 1/5, Batch 463/7596, Batch Loss: 20.6565\n",
      "Epoch 1/5, Batch 464/7596, Batch Loss: 515.7426\n",
      "Epoch 1/5, Batch 465/7596, Batch Loss: 58.0043\n",
      "Epoch 1/5, Batch 466/7596, Batch Loss: 28.1764\n",
      "Epoch 1/5, Batch 467/7596, Batch Loss: 13.8772\n",
      "Epoch 1/5, Batch 468/7596, Batch Loss: 20.0809\n",
      "Epoch 1/5, Batch 469/7596, Batch Loss: 39.0557\n",
      "Epoch 1/5, Batch 470/7596, Batch Loss: 23.8464\n",
      "Epoch 1/5, Batch 471/7596, Batch Loss: 49.0684\n",
      "Epoch 1/5, Batch 472/7596, Batch Loss: 40.1677\n",
      "Epoch 1/5, Batch 473/7596, Batch Loss: 20.4609\n",
      "Epoch 1/5, Batch 474/7596, Batch Loss: 58.4378\n",
      "Epoch 1/5, Batch 475/7596, Batch Loss: 25.4660\n",
      "Epoch 1/5, Batch 476/7596, Batch Loss: 43.6667\n",
      "Epoch 1/5, Batch 477/7596, Batch Loss: 44.6343\n",
      "Epoch 1/5, Batch 478/7596, Batch Loss: 19.2891\n",
      "Epoch 1/5, Batch 479/7596, Batch Loss: 1212.0168\n",
      "Epoch 1/5, Batch 480/7596, Batch Loss: 24.1954\n",
      "Epoch 1/5, Batch 481/7596, Batch Loss: 80.7226\n",
      "Epoch 1/5, Batch 482/7596, Batch Loss: 84.7499\n",
      "Epoch 1/5, Batch 483/7596, Batch Loss: 383.6904\n",
      "Epoch 1/5, Batch 484/7596, Batch Loss: 76.3446\n",
      "Epoch 1/5, Batch 485/7596, Batch Loss: 234.2136\n",
      "Epoch 1/5, Batch 486/7596, Batch Loss: 37.0534\n",
      "Epoch 1/5, Batch 487/7596, Batch Loss: 18.4227\n",
      "Epoch 1/5, Batch 488/7596, Batch Loss: 294.4567\n",
      "Epoch 1/5, Batch 489/7596, Batch Loss: 264.1518\n",
      "Epoch 1/5, Batch 490/7596, Batch Loss: 100.6249\n",
      "Epoch 1/5, Batch 491/7596, Batch Loss: 126.0654\n",
      "Epoch 1/5, Batch 492/7596, Batch Loss: 28.9643\n",
      "Epoch 1/5, Batch 493/7596, Batch Loss: 465.8286\n",
      "Epoch 1/5, Batch 494/7596, Batch Loss: 393.4023\n",
      "Epoch 1/5, Batch 495/7596, Batch Loss: 77.5631\n",
      "Epoch 1/5, Batch 496/7596, Batch Loss: 44.3710\n",
      "Epoch 1/5, Batch 497/7596, Batch Loss: 153.7227\n",
      "Epoch 1/5, Batch 498/7596, Batch Loss: 21.9375\n",
      "Epoch 1/5, Batch 499/7596, Batch Loss: 246.6693\n",
      "Epoch 1/5, Batch 500/7596, Batch Loss: 702.8448\n",
      "Epoch 1/5, Batch 501/7596, Batch Loss: 92.3745\n",
      "Epoch 1/5, Batch 502/7596, Batch Loss: 201.4249\n",
      "Epoch 1/5, Batch 503/7596, Batch Loss: 267.3570\n",
      "Epoch 1/5, Batch 504/7596, Batch Loss: 177.4229\n",
      "Epoch 1/5, Batch 505/7596, Batch Loss: 594.8921\n",
      "Epoch 1/5, Batch 506/7596, Batch Loss: 93834.6094\n",
      "Epoch 1/5, Batch 507/7596, Batch Loss: 24.2185\n",
      "Epoch 1/5, Batch 508/7596, Batch Loss: 193.4495\n",
      "Epoch 1/5, Batch 509/7596, Batch Loss: 200.8905\n",
      "Epoch 1/5, Batch 510/7596, Batch Loss: 226.5385\n",
      "Epoch 1/5, Batch 511/7596, Batch Loss: 662.9646\n",
      "Epoch 1/5, Batch 512/7596, Batch Loss: 352.3434\n",
      "Epoch 1/5, Batch 513/7596, Batch Loss: 26.2274\n",
      "Epoch 1/5, Batch 514/7596, Batch Loss: 25.5922\n",
      "Epoch 1/5, Batch 515/7596, Batch Loss: 519.2629\n",
      "Epoch 1/5, Batch 516/7596, Batch Loss: 864.7465\n",
      "Epoch 1/5, Batch 517/7596, Batch Loss: 190.5446\n",
      "Epoch 1/5, Batch 518/7596, Batch Loss: 80.3556\n",
      "Epoch 1/5, Batch 519/7596, Batch Loss: 93.7988\n",
      "Epoch 1/5, Batch 520/7596, Batch Loss: 394.7612\n",
      "Epoch 1/5, Batch 521/7596, Batch Loss: 186.9621\n",
      "Epoch 1/5, Batch 522/7596, Batch Loss: 188.2474\n",
      "Epoch 1/5, Batch 523/7596, Batch Loss: 157.0443\n",
      "Epoch 1/5, Batch 524/7596, Batch Loss: 1451.6110\n",
      "Epoch 1/5, Batch 525/7596, Batch Loss: 128.1156\n",
      "Epoch 1/5, Batch 526/7596, Batch Loss: 139.8378\n",
      "Epoch 1/5, Batch 527/7596, Batch Loss: 547.4633\n",
      "Epoch 1/5, Batch 528/7596, Batch Loss: 252.1997\n",
      "Epoch 1/5, Batch 529/7596, Batch Loss: 314.8124\n",
      "Epoch 1/5, Batch 530/7596, Batch Loss: 36.8769\n",
      "Epoch 1/5, Batch 531/7596, Batch Loss: 571.2450\n",
      "Epoch 1/5, Batch 532/7596, Batch Loss: 21.2102\n",
      "Epoch 1/5, Batch 533/7596, Batch Loss: 104.4622\n",
      "Epoch 1/5, Batch 534/7596, Batch Loss: 122.7227\n",
      "Epoch 1/5, Batch 535/7596, Batch Loss: 70.0124\n",
      "Epoch 1/5, Batch 536/7596, Batch Loss: 747.9731\n",
      "Epoch 1/5, Batch 537/7596, Batch Loss: 118.9328\n",
      "Epoch 1/5, Batch 538/7596, Batch Loss: 32.8129\n",
      "Epoch 1/5, Batch 539/7596, Batch Loss: 654.0916\n",
      "Epoch 1/5, Batch 540/7596, Batch Loss: 153.8953\n",
      "Epoch 1/5, Batch 541/7596, Batch Loss: 117.6353\n",
      "Epoch 1/5, Batch 542/7596, Batch Loss: 188.7552\n",
      "Epoch 1/5, Batch 543/7596, Batch Loss: 39.6609\n",
      "Epoch 1/5, Batch 544/7596, Batch Loss: 39.6327\n",
      "Epoch 1/5, Batch 545/7596, Batch Loss: 284.6055\n",
      "Epoch 1/5, Batch 546/7596, Batch Loss: 105.7779\n",
      "Epoch 1/5, Batch 547/7596, Batch Loss: 108.9841\n",
      "Epoch 1/5, Batch 548/7596, Batch Loss: 80.5465\n",
      "Epoch 1/5, Batch 549/7596, Batch Loss: 94.0975\n",
      "Epoch 1/5, Batch 550/7596, Batch Loss: 49.6799\n",
      "Epoch 1/5, Batch 551/7596, Batch Loss: 44.4078\n",
      "Epoch 1/5, Batch 552/7596, Batch Loss: 51.7644\n",
      "Epoch 1/5, Batch 553/7596, Batch Loss: 27.0750\n",
      "Epoch 1/5, Batch 554/7596, Batch Loss: 147.7267\n",
      "Epoch 1/5, Batch 555/7596, Batch Loss: 2370.9065\n",
      "Epoch 1/5, Batch 556/7596, Batch Loss: 76.2032\n",
      "Epoch 1/5, Batch 557/7596, Batch Loss: 23.9411\n",
      "Epoch 1/5, Batch 558/7596, Batch Loss: 151.4173\n",
      "Epoch 1/5, Batch 559/7596, Batch Loss: 174.9260\n",
      "Epoch 1/5, Batch 560/7596, Batch Loss: 15.2230\n",
      "Epoch 1/5, Batch 561/7596, Batch Loss: 24.5575\n",
      "Epoch 1/5, Batch 562/7596, Batch Loss: 66.4367\n",
      "Epoch 1/5, Batch 563/7596, Batch Loss: 66.6173\n",
      "Epoch 1/5, Batch 564/7596, Batch Loss: 36.8737\n",
      "Epoch 1/5, Batch 565/7596, Batch Loss: 24.0358\n",
      "Epoch 1/5, Batch 566/7596, Batch Loss: 55.1339\n",
      "Epoch 1/5, Batch 567/7596, Batch Loss: 101.4394\n",
      "Epoch 1/5, Batch 568/7596, Batch Loss: 24.5880\n",
      "Epoch 1/5, Batch 569/7596, Batch Loss: 13.9653\n",
      "Epoch 1/5, Batch 570/7596, Batch Loss: 65.8957\n",
      "Epoch 1/5, Batch 571/7596, Batch Loss: 11.5662\n",
      "Epoch 1/5, Batch 572/7596, Batch Loss: 34.0337\n",
      "Epoch 1/5, Batch 573/7596, Batch Loss: 21.1620\n",
      "Epoch 1/5, Batch 574/7596, Batch Loss: 49.6252\n",
      "Epoch 1/5, Batch 575/7596, Batch Loss: 22.0446\n",
      "Epoch 1/5, Batch 576/7596, Batch Loss: 22.3281\n",
      "Epoch 1/5, Batch 577/7596, Batch Loss: 28.8913\n",
      "Epoch 1/5, Batch 578/7596, Batch Loss: 36.0509\n",
      "Epoch 1/5, Batch 579/7596, Batch Loss: 49.2726\n",
      "Epoch 1/5, Batch 580/7596, Batch Loss: 61.8186\n",
      "Epoch 1/5, Batch 581/7596, Batch Loss: 29.2325\n",
      "Epoch 1/5, Batch 582/7596, Batch Loss: 36.3946\n",
      "Epoch 1/5, Batch 583/7596, Batch Loss: 45.8803\n",
      "Epoch 1/5, Batch 584/7596, Batch Loss: 30.5669\n",
      "Epoch 1/5, Batch 585/7596, Batch Loss: 37.8113\n",
      "Epoch 1/5, Batch 586/7596, Batch Loss: 24.0687\n",
      "Epoch 1/5, Batch 587/7596, Batch Loss: 27.9460\n",
      "Epoch 1/5, Batch 588/7596, Batch Loss: 34.7421\n",
      "Epoch 1/5, Batch 589/7596, Batch Loss: 20.8286\n",
      "Epoch 1/5, Batch 590/7596, Batch Loss: 289.4415\n",
      "Epoch 1/5, Batch 591/7596, Batch Loss: 35.0029\n",
      "Epoch 1/5, Batch 592/7596, Batch Loss: 44.6405\n",
      "Epoch 1/5, Batch 593/7596, Batch Loss: 41.5922\n",
      "Epoch 1/5, Batch 594/7596, Batch Loss: 44.1438\n",
      "Epoch 1/5, Batch 595/7596, Batch Loss: 87.8720\n",
      "Epoch 1/5, Batch 596/7596, Batch Loss: 37.7215\n",
      "Epoch 1/5, Batch 597/7596, Batch Loss: 160.4401\n",
      "Epoch 1/5, Batch 598/7596, Batch Loss: 67.8813\n",
      "Epoch 1/5, Batch 599/7596, Batch Loss: 34.6624\n",
      "Epoch 1/5, Batch 600/7596, Batch Loss: 57.0871\n",
      "Epoch 1/5, Batch 601/7596, Batch Loss: 38.9786\n",
      "Epoch 1/5, Batch 602/7596, Batch Loss: 26.8507\n",
      "Epoch 1/5, Batch 603/7596, Batch Loss: 21.6872\n",
      "Epoch 1/5, Batch 604/7596, Batch Loss: 24.9935\n",
      "Epoch 1/5, Batch 605/7596, Batch Loss: 178.2677\n",
      "Epoch 1/5, Batch 606/7596, Batch Loss: 57.2646\n",
      "Epoch 1/5, Batch 607/7596, Batch Loss: 58.2655\n",
      "Epoch 1/5, Batch 608/7596, Batch Loss: 11.7152\n",
      "Epoch 1/5, Batch 609/7596, Batch Loss: 50.5427\n",
      "Epoch 1/5, Batch 610/7596, Batch Loss: 38.8168\n",
      "Epoch 1/5, Batch 611/7596, Batch Loss: 76.2911\n",
      "Epoch 1/5, Batch 612/7596, Batch Loss: 62.2578\n",
      "Epoch 1/5, Batch 613/7596, Batch Loss: 32.9202\n",
      "Epoch 1/5, Batch 614/7596, Batch Loss: 55.3308\n",
      "Epoch 1/5, Batch 615/7596, Batch Loss: 41.9588\n",
      "Epoch 1/5, Batch 616/7596, Batch Loss: 49.7113\n",
      "Epoch 1/5, Batch 617/7596, Batch Loss: 78.7633\n",
      "Epoch 1/5, Batch 618/7596, Batch Loss: 39.5623\n",
      "Epoch 1/5, Batch 619/7596, Batch Loss: 33.6480\n",
      "Epoch 1/5, Batch 620/7596, Batch Loss: 48.6765\n",
      "Epoch 1/5, Batch 621/7596, Batch Loss: 56.1559\n",
      "Epoch 1/5, Batch 622/7596, Batch Loss: 46.1356\n",
      "Epoch 1/5, Batch 623/7596, Batch Loss: 77.0879\n",
      "Epoch 1/5, Batch 624/7596, Batch Loss: 36.2697\n",
      "Epoch 1/5, Batch 625/7596, Batch Loss: 54.9962\n",
      "Epoch 1/5, Batch 626/7596, Batch Loss: 36.7243\n",
      "Epoch 1/5, Batch 627/7596, Batch Loss: 28.0818\n",
      "Epoch 1/5, Batch 628/7596, Batch Loss: 60.1576\n",
      "Epoch 1/5, Batch 629/7596, Batch Loss: 38.9531\n",
      "Epoch 1/5, Batch 630/7596, Batch Loss: 34.7894\n",
      "Epoch 1/5, Batch 631/7596, Batch Loss: 23.3166\n",
      "Epoch 1/5, Batch 632/7596, Batch Loss: 41.4037\n",
      "Epoch 1/5, Batch 633/7596, Batch Loss: 39.4964\n",
      "Epoch 1/5, Batch 634/7596, Batch Loss: 376.1805\n",
      "Epoch 1/5, Batch 635/7596, Batch Loss: 75.7734\n",
      "Epoch 1/5, Batch 636/7596, Batch Loss: 53.3672\n",
      "Epoch 1/5, Batch 637/7596, Batch Loss: 38.9337\n",
      "Epoch 1/5, Batch 638/7596, Batch Loss: 39.9842\n",
      "Epoch 1/5, Batch 639/7596, Batch Loss: 39.2326\n",
      "Epoch 1/5, Batch 640/7596, Batch Loss: 55.3497\n",
      "Epoch 1/5, Batch 641/7596, Batch Loss: 41.6249\n",
      "Epoch 1/5, Batch 642/7596, Batch Loss: 42.6584\n",
      "Epoch 1/5, Batch 643/7596, Batch Loss: 24.5939\n",
      "Epoch 1/5, Batch 644/7596, Batch Loss: 210.8569\n",
      "Epoch 1/5, Batch 645/7596, Batch Loss: 36.3279\n",
      "Epoch 1/5, Batch 646/7596, Batch Loss: 75.8780\n",
      "Epoch 1/5, Batch 647/7596, Batch Loss: 53.4570\n",
      "Epoch 1/5, Batch 648/7596, Batch Loss: 65.2291\n",
      "Epoch 1/5, Batch 649/7596, Batch Loss: 98.5878\n",
      "Epoch 1/5, Batch 650/7596, Batch Loss: 127.2811\n",
      "Epoch 1/5, Batch 651/7596, Batch Loss: 21.0359\n",
      "Epoch 1/5, Batch 652/7596, Batch Loss: 47.8605\n",
      "Epoch 1/5, Batch 653/7596, Batch Loss: 121.1944\n",
      "Epoch 1/5, Batch 654/7596, Batch Loss: 36.2954\n",
      "Epoch 1/5, Batch 655/7596, Batch Loss: 38.6892\n",
      "Epoch 1/5, Batch 656/7596, Batch Loss: 69.4733\n",
      "Epoch 1/5, Batch 657/7596, Batch Loss: 38.9879\n",
      "Epoch 1/5, Batch 658/7596, Batch Loss: 34.1934\n",
      "Epoch 1/5, Batch 659/7596, Batch Loss: 38.1472\n",
      "Epoch 1/5, Batch 660/7596, Batch Loss: 180.5140\n",
      "Epoch 1/5, Batch 661/7596, Batch Loss: 67.6968\n",
      "Epoch 1/5, Batch 662/7596, Batch Loss: 72.3033\n",
      "Epoch 1/5, Batch 663/7596, Batch Loss: 49.9764\n",
      "Epoch 1/5, Batch 664/7596, Batch Loss: 42.6416\n",
      "Epoch 1/5, Batch 665/7596, Batch Loss: 59.9351\n",
      "Epoch 1/5, Batch 666/7596, Batch Loss: 32.3243\n",
      "Epoch 1/5, Batch 667/7596, Batch Loss: 46.2198\n",
      "Epoch 1/5, Batch 668/7596, Batch Loss: 38.5335\n",
      "Epoch 1/5, Batch 669/7596, Batch Loss: 36.5089\n",
      "Epoch 1/5, Batch 670/7596, Batch Loss: 45.4102\n",
      "Epoch 1/5, Batch 671/7596, Batch Loss: 40.5341\n",
      "Epoch 1/5, Batch 672/7596, Batch Loss: 44.1688\n",
      "Epoch 1/5, Batch 673/7596, Batch Loss: 39.0432\n",
      "Epoch 1/5, Batch 674/7596, Batch Loss: 35.5695\n",
      "Epoch 1/5, Batch 675/7596, Batch Loss: 20.6248\n",
      "Epoch 1/5, Batch 676/7596, Batch Loss: 29.9664\n",
      "Epoch 1/5, Batch 677/7596, Batch Loss: 30.6010\n",
      "Epoch 1/5, Batch 678/7596, Batch Loss: 38.9906\n",
      "Epoch 1/5, Batch 679/7596, Batch Loss: 97.0650\n",
      "Epoch 1/5, Batch 680/7596, Batch Loss: 35.7582\n",
      "Epoch 1/5, Batch 681/7596, Batch Loss: 67.2359\n",
      "Epoch 1/5, Batch 682/7596, Batch Loss: 23.7521\n",
      "Epoch 1/5, Batch 683/7596, Batch Loss: 41.7627\n",
      "Epoch 1/5, Batch 684/7596, Batch Loss: 336.8018\n",
      "Epoch 1/5, Batch 685/7596, Batch Loss: 41.7667\n",
      "Epoch 1/5, Batch 686/7596, Batch Loss: 85.6483\n",
      "Epoch 1/5, Batch 687/7596, Batch Loss: 265.3784\n",
      "Epoch 1/5, Batch 688/7596, Batch Loss: 137.8271\n",
      "Epoch 1/5, Batch 689/7596, Batch Loss: 52.8432\n",
      "Epoch 1/5, Batch 690/7596, Batch Loss: 134.4863\n",
      "Epoch 1/5, Batch 691/7596, Batch Loss: 172.8472\n",
      "Epoch 1/5, Batch 692/7596, Batch Loss: 38.2722\n",
      "Epoch 1/5, Batch 693/7596, Batch Loss: 35.7330\n",
      "Epoch 1/5, Batch 694/7596, Batch Loss: 86.9836\n",
      "Epoch 1/5, Batch 695/7596, Batch Loss: 101.5336\n",
      "Epoch 1/5, Batch 696/7596, Batch Loss: 22.6501\n",
      "Epoch 1/5, Batch 697/7596, Batch Loss: 219.1407\n",
      "Epoch 1/5, Batch 698/7596, Batch Loss: 41.5000\n",
      "Epoch 1/5, Batch 699/7596, Batch Loss: 367.3802\n",
      "Epoch 1/5, Batch 700/7596, Batch Loss: 199.6350\n",
      "Epoch 1/5, Batch 701/7596, Batch Loss: 42.0675\n",
      "Epoch 1/5, Batch 702/7596, Batch Loss: 6103.0537\n",
      "Epoch 1/5, Batch 703/7596, Batch Loss: 42.5730\n",
      "Epoch 1/5, Batch 704/7596, Batch Loss: 23.3657\n",
      "Epoch 1/5, Batch 705/7596, Batch Loss: 134.8307\n",
      "Epoch 1/5, Batch 706/7596, Batch Loss: 317.6124\n",
      "Epoch 1/5, Batch 707/7596, Batch Loss: 410.1886\n",
      "Epoch 1/5, Batch 708/7596, Batch Loss: 36.9142\n",
      "Epoch 1/5, Batch 709/7596, Batch Loss: 256.6828\n",
      "Epoch 1/5, Batch 710/7596, Batch Loss: 137.1069\n",
      "Epoch 1/5, Batch 711/7596, Batch Loss: 48.0382\n",
      "Epoch 1/5, Batch 712/7596, Batch Loss: 147.7458\n",
      "Epoch 1/5, Batch 713/7596, Batch Loss: 312.1575\n",
      "Epoch 1/5, Batch 714/7596, Batch Loss: 56.8435\n",
      "Epoch 1/5, Batch 715/7596, Batch Loss: 188.2295\n",
      "Epoch 1/5, Batch 716/7596, Batch Loss: 90.5581\n",
      "Epoch 1/5, Batch 717/7596, Batch Loss: 441.8684\n",
      "Epoch 1/5, Batch 718/7596, Batch Loss: 952.5554\n",
      "Epoch 1/5, Batch 719/7596, Batch Loss: 104.4967\n",
      "Epoch 1/5, Batch 720/7596, Batch Loss: 258.5393\n",
      "Epoch 1/5, Batch 721/7596, Batch Loss: 112.9453\n",
      "Epoch 1/5, Batch 722/7596, Batch Loss: 34.9220\n",
      "Epoch 1/5, Batch 723/7596, Batch Loss: 92.1544\n",
      "Epoch 1/5, Batch 724/7596, Batch Loss: 111.9953\n",
      "Epoch 1/5, Batch 725/7596, Batch Loss: 79.2438\n",
      "Epoch 1/5, Batch 726/7596, Batch Loss: 123.5057\n",
      "Epoch 1/5, Batch 727/7596, Batch Loss: 71.0916\n",
      "Epoch 1/5, Batch 728/7596, Batch Loss: 235.9535\n",
      "Epoch 1/5, Batch 729/7596, Batch Loss: 39.4055\n",
      "Epoch 1/5, Batch 730/7596, Batch Loss: 32.7548\n",
      "Epoch 1/5, Batch 731/7596, Batch Loss: 34.9096\n",
      "Epoch 1/5, Batch 732/7596, Batch Loss: 34.4957\n",
      "Epoch 1/5, Batch 733/7596, Batch Loss: 331.4561\n",
      "Epoch 1/5, Batch 734/7596, Batch Loss: 24.7855\n",
      "Epoch 1/5, Batch 735/7596, Batch Loss: 35.9931\n",
      "Epoch 1/5, Batch 736/7596, Batch Loss: 38.5121\n",
      "Epoch 1/5, Batch 737/7596, Batch Loss: 163.9356\n",
      "Epoch 1/5, Batch 738/7596, Batch Loss: 6548.9551\n",
      "Epoch 1/5, Batch 739/7596, Batch Loss: 286.8009\n",
      "Epoch 1/5, Batch 740/7596, Batch Loss: 601.7112\n",
      "Epoch 1/5, Batch 741/7596, Batch Loss: 814.4555\n",
      "Epoch 1/5, Batch 742/7596, Batch Loss: 455.4500\n",
      "Epoch 1/5, Batch 743/7596, Batch Loss: 812.7386\n",
      "Epoch 1/5, Batch 744/7596, Batch Loss: 205.2744\n",
      "Epoch 1/5, Batch 745/7596, Batch Loss: 201.1641\n",
      "Epoch 1/5, Batch 746/7596, Batch Loss: 323.1565\n",
      "Epoch 1/5, Batch 747/7596, Batch Loss: 531.5991\n",
      "Epoch 1/5, Batch 748/7596, Batch Loss: 292.9524\n",
      "Epoch 1/5, Batch 749/7596, Batch Loss: 404.7486\n",
      "Epoch 1/5, Batch 750/7596, Batch Loss: 36.3828\n",
      "Epoch 1/5, Batch 751/7596, Batch Loss: 29.2934\n",
      "Epoch 1/5, Batch 752/7596, Batch Loss: 35.0624\n",
      "Epoch 1/5, Batch 753/7596, Batch Loss: 19.4356\n",
      "Epoch 1/5, Batch 754/7596, Batch Loss: 807.9568\n",
      "Epoch 1/5, Batch 755/7596, Batch Loss: 316.4859\n",
      "Epoch 1/5, Batch 756/7596, Batch Loss: 21.9162\n",
      "Epoch 1/5, Batch 757/7596, Batch Loss: 20.1014\n",
      "Epoch 1/5, Batch 758/7596, Batch Loss: 408.0518\n",
      "Epoch 1/5, Batch 759/7596, Batch Loss: 21.1353\n",
      "Epoch 1/5, Batch 760/7596, Batch Loss: 275.3594\n",
      "Epoch 1/5, Batch 761/7596, Batch Loss: 10342.6260\n",
      "Epoch 1/5, Batch 762/7596, Batch Loss: 1547.1226\n",
      "Epoch 1/5, Batch 763/7596, Batch Loss: 875.7796\n",
      "Epoch 1/5, Batch 764/7596, Batch Loss: 20658.8711\n",
      "Epoch 1/5, Batch 765/7596, Batch Loss: 20.5046\n",
      "Epoch 1/5, Batch 766/7596, Batch Loss: 454.2831\n",
      "Epoch 1/5, Batch 767/7596, Batch Loss: 488.4519\n",
      "Epoch 1/5, Batch 768/7596, Batch Loss: 36.4378\n",
      "Epoch 1/5, Batch 769/7596, Batch Loss: 26.9257\n",
      "Epoch 1/5, Batch 770/7596, Batch Loss: 31.5792\n",
      "Epoch 1/5, Batch 771/7596, Batch Loss: 643.9089\n",
      "Epoch 1/5, Batch 772/7596, Batch Loss: 1277.7820\n",
      "Epoch 1/5, Batch 773/7596, Batch Loss: 82.5617\n",
      "Epoch 1/5, Batch 774/7596, Batch Loss: 14.6559\n",
      "Epoch 1/5, Batch 775/7596, Batch Loss: 384.8975\n",
      "Epoch 1/5, Batch 776/7596, Batch Loss: 50398.4727\n",
      "Epoch 1/5, Batch 777/7596, Batch Loss: 26.1343\n",
      "Epoch 1/5, Batch 778/7596, Batch Loss: 602.5363\n",
      "Epoch 1/5, Batch 779/7596, Batch Loss: 413.4324\n",
      "Epoch 1/5, Batch 780/7596, Batch Loss: 373.2129\n",
      "Epoch 1/5, Batch 781/7596, Batch Loss: 296.5400\n",
      "Epoch 1/5, Batch 782/7596, Batch Loss: 537.9107\n",
      "Epoch 1/5, Batch 783/7596, Batch Loss: 2580.7400\n",
      "Epoch 1/5, Batch 784/7596, Batch Loss: 171.9467\n",
      "Epoch 1/5, Batch 785/7596, Batch Loss: 26.5738\n",
      "Epoch 1/5, Batch 786/7596, Batch Loss: 25.5278\n",
      "Epoch 1/5, Batch 787/7596, Batch Loss: 13.1481\n",
      "Epoch 1/5, Batch 788/7596, Batch Loss: 21.7296\n",
      "Epoch 1/5, Batch 789/7596, Batch Loss: 2129.4211\n",
      "Epoch 1/5, Batch 790/7596, Batch Loss: 330.7158\n",
      "Epoch 1/5, Batch 791/7596, Batch Loss: 4507.2505\n",
      "Epoch 1/5, Batch 792/7596, Batch Loss: 3139.1250\n",
      "Epoch 1/5, Batch 793/7596, Batch Loss: 1151.8370\n",
      "Epoch 1/5, Batch 794/7596, Batch Loss: 36.6536\n",
      "Epoch 1/5, Batch 795/7596, Batch Loss: 36.4814\n",
      "Epoch 1/5, Batch 796/7596, Batch Loss: 677.4186\n",
      "Epoch 1/5, Batch 797/7596, Batch Loss: 611.0432\n",
      "Epoch 1/5, Batch 798/7596, Batch Loss: 584.0698\n",
      "Epoch 1/5, Batch 799/7596, Batch Loss: 20.6112\n",
      "Epoch 1/5, Batch 800/7596, Batch Loss: 22.5933\n",
      "Epoch 1/5, Batch 801/7596, Batch Loss: 371.7385\n",
      "Epoch 1/5, Batch 802/7596, Batch Loss: 494.3565\n",
      "Epoch 1/5, Batch 803/7596, Batch Loss: 27.7223\n",
      "Epoch 1/5, Batch 804/7596, Batch Loss: 49.2502\n",
      "Epoch 1/5, Batch 805/7596, Batch Loss: 463.9211\n",
      "Epoch 1/5, Batch 806/7596, Batch Loss: 538.3793\n",
      "Epoch 1/5, Batch 807/7596, Batch Loss: 21.1794\n",
      "Epoch 1/5, Batch 808/7596, Batch Loss: 213.3594\n",
      "Epoch 1/5, Batch 809/7596, Batch Loss: 253.4542\n",
      "Epoch 1/5, Batch 810/7596, Batch Loss: 3050.4290\n",
      "Epoch 1/5, Batch 811/7596, Batch Loss: 664.0818\n",
      "Epoch 1/5, Batch 812/7596, Batch Loss: 279.6472\n",
      "Epoch 1/5, Batch 813/7596, Batch Loss: 524.3717\n",
      "Epoch 1/5, Batch 814/7596, Batch Loss: 36.5295\n",
      "Epoch 1/5, Batch 815/7596, Batch Loss: 488.8243\n",
      "Epoch 1/5, Batch 816/7596, Batch Loss: 146.1955\n",
      "Epoch 1/5, Batch 817/7596, Batch Loss: 609.3686\n",
      "Epoch 1/5, Batch 818/7596, Batch Loss: 17757.2734\n",
      "Epoch 1/5, Batch 819/7596, Batch Loss: 40.4914\n",
      "Epoch 1/5, Batch 820/7596, Batch Loss: 8163.5576\n",
      "Epoch 1/5, Batch 821/7596, Batch Loss: 990.4618\n",
      "Epoch 1/5, Batch 822/7596, Batch Loss: 445.6048\n",
      "Epoch 1/5, Batch 823/7596, Batch Loss: 555.5058\n",
      "Epoch 1/5, Batch 824/7596, Batch Loss: 35.8411\n",
      "Epoch 1/5, Batch 825/7596, Batch Loss: 1421.6958\n",
      "Epoch 1/5, Batch 826/7596, Batch Loss: 41.1658\n",
      "Epoch 1/5, Batch 827/7596, Batch Loss: 1229.0129\n",
      "Epoch 1/5, Batch 828/7596, Batch Loss: 691.4338\n",
      "Epoch 1/5, Batch 829/7596, Batch Loss: 642.2774\n",
      "Epoch 1/5, Batch 830/7596, Batch Loss: 44.4824\n",
      "Epoch 1/5, Batch 831/7596, Batch Loss: 676.7726\n",
      "Epoch 1/5, Batch 832/7596, Batch Loss: 159.9310\n",
      "Epoch 1/5, Batch 833/7596, Batch Loss: 149.2100\n",
      "Epoch 1/5, Batch 834/7596, Batch Loss: 1150.5347\n",
      "Epoch 1/5, Batch 835/7596, Batch Loss: 79.9254\n",
      "Epoch 1/5, Batch 836/7596, Batch Loss: 89747.6484\n",
      "Epoch 1/5, Batch 837/7596, Batch Loss: 160.8709\n",
      "Epoch 1/5, Batch 838/7596, Batch Loss: 33.9231\n",
      "Epoch 1/5, Batch 839/7596, Batch Loss: 855.3239\n",
      "Epoch 1/5, Batch 840/7596, Batch Loss: 61.8387\n",
      "Epoch 1/5, Batch 841/7596, Batch Loss: 669.6065\n",
      "Epoch 1/5, Batch 842/7596, Batch Loss: 305.6272\n",
      "Epoch 1/5, Batch 843/7596, Batch Loss: 98.9323\n",
      "Epoch 1/5, Batch 844/7596, Batch Loss: 254.4095\n",
      "Epoch 1/5, Batch 845/7596, Batch Loss: 618.0479\n",
      "Epoch 1/5, Batch 846/7596, Batch Loss: 114.8886\n",
      "Epoch 1/5, Batch 847/7596, Batch Loss: 229.6468\n",
      "Epoch 1/5, Batch 848/7596, Batch Loss: 410.2657\n",
      "Epoch 1/5, Batch 849/7596, Batch Loss: 37.9186\n",
      "Epoch 1/5, Batch 850/7596, Batch Loss: 159.8373\n",
      "Epoch 1/5, Batch 851/7596, Batch Loss: 994.3760\n",
      "Epoch 1/5, Batch 852/7596, Batch Loss: 188.4459\n",
      "Epoch 1/5, Batch 853/7596, Batch Loss: 46.9711\n",
      "Epoch 1/5, Batch 854/7596, Batch Loss: 274.8961\n",
      "Epoch 1/5, Batch 855/7596, Batch Loss: 398.0243\n",
      "Epoch 1/5, Batch 856/7596, Batch Loss: 182.1951\n",
      "Epoch 1/5, Batch 857/7596, Batch Loss: 191.9139\n",
      "Epoch 1/5, Batch 858/7596, Batch Loss: 168.8057\n",
      "Epoch 1/5, Batch 859/7596, Batch Loss: 141.3407\n",
      "Epoch 1/5, Batch 860/7596, Batch Loss: 98.6226\n",
      "Epoch 1/5, Batch 861/7596, Batch Loss: 1216.6182\n",
      "Epoch 1/5, Batch 862/7596, Batch Loss: 207.7182\n",
      "Epoch 1/5, Batch 863/7596, Batch Loss: 561.5894\n",
      "Epoch 1/5, Batch 864/7596, Batch Loss: 238.8841\n",
      "Epoch 1/5, Batch 865/7596, Batch Loss: 559.3296\n",
      "Epoch 1/5, Batch 866/7596, Batch Loss: 75393.3906\n",
      "Epoch 1/5, Batch 867/7596, Batch Loss: 37.3056\n",
      "Epoch 1/5, Batch 868/7596, Batch Loss: 7161.1519\n",
      "Epoch 1/5, Batch 869/7596, Batch Loss: 1863.0944\n",
      "Epoch 1/5, Batch 870/7596, Batch Loss: 28.4140\n",
      "Epoch 1/5, Batch 871/7596, Batch Loss: 1014.7410\n",
      "Epoch 1/5, Batch 872/7596, Batch Loss: 2724.6685\n",
      "Epoch 1/5, Batch 873/7596, Batch Loss: 6466.8838\n",
      "Epoch 1/5, Batch 874/7596, Batch Loss: 1887.8378\n",
      "Epoch 1/5, Batch 875/7596, Batch Loss: 3574.2654\n",
      "Epoch 1/5, Batch 876/7596, Batch Loss: 2291.6614\n",
      "Epoch 1/5, Batch 877/7596, Batch Loss: 814.0948\n",
      "Epoch 1/5, Batch 878/7596, Batch Loss: 3174.6387\n",
      "Epoch 1/5, Batch 879/7596, Batch Loss: 693.3644\n",
      "Epoch 1/5, Batch 880/7596, Batch Loss: 42.0295\n",
      "Epoch 1/5, Batch 881/7596, Batch Loss: 1759.2943\n",
      "Epoch 1/5, Batch 882/7596, Batch Loss: 242.2906\n",
      "Epoch 1/5, Batch 883/7596, Batch Loss: 1699.1947\n",
      "Epoch 1/5, Batch 884/7596, Batch Loss: 806.5818\n",
      "Epoch 1/5, Batch 885/7596, Batch Loss: 118.7310\n",
      "Epoch 1/5, Batch 886/7596, Batch Loss: 3606.4766\n",
      "Epoch 1/5, Batch 887/7596, Batch Loss: 131703.6250\n",
      "Epoch 1/5, Batch 888/7596, Batch Loss: 22.5578\n",
      "Epoch 1/5, Batch 889/7596, Batch Loss: 3075.3577\n",
      "Epoch 1/5, Batch 890/7596, Batch Loss: 7550.2798\n",
      "Epoch 1/5, Batch 891/7596, Batch Loss: 40.7451\n",
      "Epoch 1/5, Batch 892/7596, Batch Loss: 1139.8572\n",
      "Epoch 1/5, Batch 893/7596, Batch Loss: 1384.3092\n",
      "Epoch 1/5, Batch 894/7596, Batch Loss: 264.1826\n",
      "Epoch 1/5, Batch 895/7596, Batch Loss: 2584.6147\n",
      "Epoch 1/5, Batch 896/7596, Batch Loss: 2270.5391\n",
      "Epoch 1/5, Batch 897/7596, Batch Loss: 658.7928\n",
      "Epoch 1/5, Batch 898/7596, Batch Loss: 4042.0625\n",
      "Epoch 1/5, Batch 899/7596, Batch Loss: 1077.5427\n",
      "Epoch 1/5, Batch 900/7596, Batch Loss: 206.2105\n",
      "Epoch 1/5, Batch 901/7596, Batch Loss: 36.6629\n",
      "Epoch 1/5, Batch 902/7596, Batch Loss: 1036.1427\n",
      "Epoch 1/5, Batch 903/7596, Batch Loss: 1862.9608\n",
      "Epoch 1/5, Batch 904/7596, Batch Loss: 966.3645\n",
      "Epoch 1/5, Batch 905/7596, Batch Loss: 1811.2874\n",
      "Epoch 1/5, Batch 906/7596, Batch Loss: 886.8350\n",
      "Epoch 1/5, Batch 907/7596, Batch Loss: 737.1593\n",
      "Epoch 1/5, Batch 908/7596, Batch Loss: 72.5585\n",
      "Epoch 1/5, Batch 909/7596, Batch Loss: 267.2896\n",
      "Epoch 1/5, Batch 910/7596, Batch Loss: 207.8941\n",
      "Epoch 1/5, Batch 911/7596, Batch Loss: 39.5951\n",
      "Epoch 1/5, Batch 912/7596, Batch Loss: 25.4138\n",
      "Epoch 1/5, Batch 913/7596, Batch Loss: 1951.5604\n",
      "Epoch 1/5, Batch 914/7596, Batch Loss: 110.3681\n",
      "Epoch 1/5, Batch 915/7596, Batch Loss: 42.2677\n",
      "Epoch 1/5, Batch 916/7596, Batch Loss: 631.7851\n",
      "Epoch 1/5, Batch 917/7596, Batch Loss: 1550.1414\n",
      "Epoch 1/5, Batch 918/7596, Batch Loss: 349.3874\n",
      "Epoch 1/5, Batch 919/7596, Batch Loss: 777.4236\n",
      "Epoch 1/5, Batch 920/7596, Batch Loss: 24837.9746\n",
      "Epoch 1/5, Batch 921/7596, Batch Loss: 1096.8511\n",
      "Epoch 1/5, Batch 922/7596, Batch Loss: 966.4533\n",
      "Epoch 1/5, Batch 923/7596, Batch Loss: 1079.6696\n",
      "Epoch 1/5, Batch 924/7596, Batch Loss: 1111.7775\n",
      "Epoch 1/5, Batch 925/7596, Batch Loss: 191.4334\n",
      "Epoch 1/5, Batch 926/7596, Batch Loss: 1397.9164\n",
      "Epoch 1/5, Batch 927/7596, Batch Loss: 2210.0537\n",
      "Epoch 1/5, Batch 928/7596, Batch Loss: 42.0390\n",
      "Epoch 1/5, Batch 929/7596, Batch Loss: 719.2634\n",
      "Epoch 1/5, Batch 930/7596, Batch Loss: 1191.0363\n",
      "Epoch 1/5, Batch 931/7596, Batch Loss: 592.3871\n",
      "Epoch 1/5, Batch 932/7596, Batch Loss: 2493.6150\n",
      "Epoch 1/5, Batch 933/7596, Batch Loss: 42.9782\n",
      "Epoch 1/5, Batch 934/7596, Batch Loss: 403.3917\n",
      "Epoch 1/5, Batch 935/7596, Batch Loss: 1832.7462\n",
      "Epoch 1/5, Batch 936/7596, Batch Loss: 1673.4230\n",
      "Epoch 1/5, Batch 937/7596, Batch Loss: 661.4913\n",
      "Epoch 1/5, Batch 938/7596, Batch Loss: 34.8538\n",
      "Epoch 1/5, Batch 939/7596, Batch Loss: 289.3067\n",
      "Epoch 1/5, Batch 940/7596, Batch Loss: 38.6192\n",
      "Epoch 1/5, Batch 941/7596, Batch Loss: 126.8196\n",
      "Epoch 1/5, Batch 942/7596, Batch Loss: 254.0904\n",
      "Epoch 1/5, Batch 943/7596, Batch Loss: 176.7479\n",
      "Epoch 1/5, Batch 944/7596, Batch Loss: 36898.2930\n",
      "Epoch 1/5, Batch 945/7596, Batch Loss: 655.4205\n",
      "Epoch 1/5, Batch 946/7596, Batch Loss: 650.5776\n",
      "Epoch 1/5, Batch 947/7596, Batch Loss: 1715.2524\n",
      "Epoch 1/5, Batch 948/7596, Batch Loss: 33.9630\n",
      "Epoch 1/5, Batch 949/7596, Batch Loss: 262.3331\n",
      "Epoch 1/5, Batch 950/7596, Batch Loss: 101.7355\n",
      "Epoch 1/5, Batch 951/7596, Batch Loss: 66504.5703\n",
      "Epoch 1/5, Batch 952/7596, Batch Loss: 35306.8398\n",
      "Epoch 1/5, Batch 953/7596, Batch Loss: 1893.1826\n",
      "Epoch 1/5, Batch 954/7596, Batch Loss: 38.7806\n",
      "Epoch 1/5, Batch 955/7596, Batch Loss: 41.9510\n",
      "Epoch 1/5, Batch 956/7596, Batch Loss: 45873.6758\n",
      "Epoch 1/5, Batch 957/7596, Batch Loss: 10101.8701\n",
      "Epoch 1/5, Batch 958/7596, Batch Loss: 13684.1260\n",
      "Epoch 1/5, Batch 959/7596, Batch Loss: 309.5088\n",
      "Epoch 1/5, Batch 960/7596, Batch Loss: 5683.6851\n",
      "Epoch 1/5, Batch 961/7596, Batch Loss: 17239.9414\n",
      "Epoch 1/5, Batch 962/7596, Batch Loss: 1223.3904\n",
      "Epoch 1/5, Batch 963/7596, Batch Loss: 1380.5856\n",
      "Epoch 1/5, Batch 964/7596, Batch Loss: 20394.7305\n",
      "Epoch 1/5, Batch 965/7596, Batch Loss: 759.6381\n",
      "Epoch 1/5, Batch 966/7596, Batch Loss: 193.9040\n",
      "Epoch 1/5, Batch 967/7596, Batch Loss: 2018.1102\n",
      "Epoch 1/5, Batch 968/7596, Batch Loss: 39.7640\n",
      "Epoch 1/5, Batch 969/7596, Batch Loss: 3411.1797\n",
      "Epoch 1/5, Batch 970/7596, Batch Loss: 14406.7139\n",
      "Epoch 1/5, Batch 971/7596, Batch Loss: 45.2287\n",
      "Epoch 1/5, Batch 972/7596, Batch Loss: 41.1161\n",
      "Epoch 1/5, Batch 973/7596, Batch Loss: 116.9505\n",
      "Epoch 1/5, Batch 974/7596, Batch Loss: 2155.9785\n",
      "Epoch 1/5, Batch 975/7596, Batch Loss: 27.7845\n",
      "Epoch 1/5, Batch 976/7596, Batch Loss: 1796.5475\n",
      "Epoch 1/5, Batch 977/7596, Batch Loss: 25.4448\n",
      "Epoch 1/5, Batch 978/7596, Batch Loss: 9444.9443\n",
      "Epoch 1/5, Batch 979/7596, Batch Loss: 12925350.0000\n",
      "Epoch 1/5, Batch 980/7596, Batch Loss: 548.8629\n",
      "Epoch 1/5, Batch 981/7596, Batch Loss: 1169.3308\n",
      "Epoch 1/5, Batch 982/7596, Batch Loss: 24.1264\n",
      "Epoch 1/5, Batch 983/7596, Batch Loss: 8543.4258\n",
      "Epoch 1/5, Batch 984/7596, Batch Loss: 284.9997\n",
      "Epoch 1/5, Batch 985/7596, Batch Loss: 2824.2219\n",
      "Epoch 1/5, Batch 986/7596, Batch Loss: 2386.8401\n",
      "Epoch 1/5, Batch 987/7596, Batch Loss: 1251.2045\n",
      "Epoch 1/5, Batch 988/7596, Batch Loss: 120380.6250\n",
      "Epoch 1/5, Batch 989/7596, Batch Loss: 1153.7029\n",
      "Epoch 1/5, Batch 990/7596, Batch Loss: 755.2672\n",
      "Epoch 1/5, Batch 991/7596, Batch Loss: 3419.7285\n",
      "Epoch 1/5, Batch 992/7596, Batch Loss: 1852.1003\n",
      "Epoch 1/5, Batch 993/7596, Batch Loss: 1490.0247\n",
      "Epoch 1/5, Batch 994/7596, Batch Loss: 761.5764\n",
      "Epoch 1/5, Batch 995/7596, Batch Loss: 4413.8442\n",
      "Epoch 1/5, Batch 996/7596, Batch Loss: 4051.0750\n",
      "Epoch 1/5, Batch 997/7596, Batch Loss: 3570.3374\n",
      "Epoch 1/5, Batch 998/7596, Batch Loss: 3935.4385\n",
      "Epoch 1/5, Batch 999/7596, Batch Loss: 1629198.7500\n",
      "Epoch 1/5, Batch 1000/7596, Batch Loss: 6306.2231\n",
      "Epoch 1/5, Batch 1001/7596, Batch Loss: 2631.4673\n",
      "Epoch 1/5, Batch 1002/7596, Batch Loss: 12185.1455\n",
      "Epoch 1/5, Batch 1003/7596, Batch Loss: 34999.9961\n",
      "Epoch 1/5, Batch 1004/7596, Batch Loss: 2477.6575\n",
      "Epoch 1/5, Batch 1005/7596, Batch Loss: 4538.3667\n",
      "Epoch 1/5, Batch 1006/7596, Batch Loss: 51.1282\n",
      "Epoch 1/5, Batch 1007/7596, Batch Loss: 43.1358\n",
      "Epoch 1/5, Batch 1008/7596, Batch Loss: 30446.0156\n",
      "Epoch 1/5, Batch 1009/7596, Batch Loss: 6770.4858\n",
      "Epoch 1/5, Batch 1010/7596, Batch Loss: 4735.7041\n",
      "Epoch 1/5, Batch 1011/7596, Batch Loss: 21200.5371\n",
      "Epoch 1/5, Batch 1012/7596, Batch Loss: 3872.9858\n",
      "Epoch 1/5, Batch 1013/7596, Batch Loss: 4819.6719\n",
      "Epoch 1/5, Batch 1014/7596, Batch Loss: 1317.5646\n",
      "Epoch 1/5, Batch 1015/7596, Batch Loss: 73.7196\n",
      "Epoch 1/5, Batch 1016/7596, Batch Loss: 1238.3051\n",
      "Epoch 1/5, Batch 1017/7596, Batch Loss: 1326.6426\n",
      "Epoch 1/5, Batch 1018/7596, Batch Loss: 1344.4811\n",
      "Epoch 1/5, Batch 1019/7596, Batch Loss: 1871.9569\n",
      "Epoch 1/5, Batch 1020/7596, Batch Loss: 4252.2729\n",
      "Epoch 1/5, Batch 1021/7596, Batch Loss: 4966.3931\n",
      "Epoch 1/5, Batch 1022/7596, Batch Loss: 1909.3074\n",
      "Epoch 1/5, Batch 1023/7596, Batch Loss: 885.4877\n",
      "Epoch 1/5, Batch 1024/7596, Batch Loss: 203444.4844\n",
      "Epoch 1/5, Batch 1025/7596, Batch Loss: 2940.0691\n",
      "Epoch 1/5, Batch 1026/7596, Batch Loss: 1521.1516\n",
      "Epoch 1/5, Batch 1027/7596, Batch Loss: 727.0714\n",
      "Epoch 1/5, Batch 1028/7596, Batch Loss: 3575.5833\n",
      "Epoch 1/5, Batch 1029/7596, Batch Loss: 36.1835\n",
      "Epoch 1/5, Batch 1030/7596, Batch Loss: 255.4064\n",
      "Epoch 1/5, Batch 1031/7596, Batch Loss: 37463.3906\n",
      "Epoch 1/5, Batch 1032/7596, Batch Loss: 6988.1543\n",
      "Epoch 1/5, Batch 1033/7596, Batch Loss: 7380.3931\n",
      "Epoch 1/5, Batch 1034/7596, Batch Loss: 9059.7686\n",
      "Epoch 1/5, Batch 1035/7596, Batch Loss: 36.7791\n",
      "Epoch 1/5, Batch 1036/7596, Batch Loss: 1284.6200\n",
      "Epoch 1/5, Batch 1037/7596, Batch Loss: 18901.7500\n",
      "Epoch 1/5, Batch 1038/7596, Batch Loss: 917.3956\n",
      "Epoch 1/5, Batch 1039/7596, Batch Loss: 698.5166\n",
      "Epoch 1/5, Batch 1040/7596, Batch Loss: 370304.8125\n",
      "Epoch 1/5, Batch 1041/7596, Batch Loss: 54394.9922\n",
      "Epoch 1/5, Batch 1042/7596, Batch Loss: 12046.2305\n",
      "Epoch 1/5, Batch 1043/7596, Batch Loss: 39.8500\n",
      "Epoch 1/5, Batch 1044/7596, Batch Loss: 41.3363\n",
      "Epoch 1/5, Batch 1045/7596, Batch Loss: 40.1083\n",
      "Epoch 1/5, Batch 1046/7596, Batch Loss: 420002.0938\n",
      "Epoch 1/5, Batch 1047/7596, Batch Loss: 38.7476\n",
      "Epoch 1/5, Batch 1048/7596, Batch Loss: 32.6536\n",
      "Epoch 1/5, Batch 1049/7596, Batch Loss: 1403.9816\n",
      "Epoch 1/5, Batch 1050/7596, Batch Loss: 8023.6646\n",
      "Epoch 1/5, Batch 1051/7596, Batch Loss: 2007.2832\n",
      "Epoch 1/5, Batch 1052/7596, Batch Loss: 4127.2354\n",
      "Epoch 1/5, Batch 1053/7596, Batch Loss: 7474.2334\n",
      "Epoch 1/5, Batch 1054/7596, Batch Loss: 3231.8887\n",
      "Epoch 1/5, Batch 1055/7596, Batch Loss: 6239.6182\n",
      "Epoch 1/5, Batch 1056/7596, Batch Loss: 27.8769\n",
      "Epoch 1/5, Batch 1057/7596, Batch Loss: 67914.4375\n",
      "Epoch 1/5, Batch 1058/7596, Batch Loss: 1379.6107\n",
      "Epoch 1/5, Batch 1059/7596, Batch Loss: 1057.5085\n",
      "Epoch 1/5, Batch 1060/7596, Batch Loss: 20694.1328\n",
      "Epoch 1/5, Batch 1061/7596, Batch Loss: 34.9918\n",
      "Epoch 1/5, Batch 1062/7596, Batch Loss: 444.4058\n",
      "Epoch 1/5, Batch 1063/7596, Batch Loss: 5415.7969\n",
      "Epoch 1/5, Batch 1064/7596, Batch Loss: 26.7672\n",
      "Epoch 1/5, Batch 1065/7596, Batch Loss: 467574.9688\n",
      "Epoch 1/5, Batch 1066/7596, Batch Loss: 9544.3486\n",
      "Epoch 1/5, Batch 1067/7596, Batch Loss: 14.4304\n",
      "Epoch 1/5, Batch 1068/7596, Batch Loss: 32.2882\n",
      "Epoch 1/5, Batch 1069/7596, Batch Loss: 34.2749\n",
      "Epoch 1/5, Batch 1070/7596, Batch Loss: 33.3914\n",
      "Epoch 1/5, Batch 1071/7596, Batch Loss: 21629.9102\n",
      "Epoch 1/5, Batch 1072/7596, Batch Loss: 824.0936\n",
      "Epoch 1/5, Batch 1073/7596, Batch Loss: 495.3156\n",
      "Epoch 1/5, Batch 1074/7596, Batch Loss: 302.2227\n",
      "Epoch 1/5, Batch 1075/7596, Batch Loss: 773156.5625\n",
      "Epoch 1/5, Batch 1076/7596, Batch Loss: 38.4412\n",
      "Epoch 1/5, Batch 1077/7596, Batch Loss: 3716.3164\n",
      "Epoch 1/5, Batch 1078/7596, Batch Loss: 35.9637\n",
      "Epoch 1/5, Batch 1079/7596, Batch Loss: 71157.7578\n",
      "Epoch 1/5, Batch 1080/7596, Batch Loss: 35.1764\n",
      "Epoch 1/5, Batch 1081/7596, Batch Loss: 21.7527\n",
      "Epoch 1/5, Batch 1082/7596, Batch Loss: 12208.1572\n",
      "Epoch 1/5, Batch 1083/7596, Batch Loss: 5244.9102\n",
      "Epoch 1/5, Batch 1084/7596, Batch Loss: 3043.0369\n",
      "Epoch 1/5, Batch 1085/7596, Batch Loss: 63540.6562\n",
      "Epoch 1/5, Batch 1086/7596, Batch Loss: 172228.1719\n",
      "Epoch 1/5, Batch 1087/7596, Batch Loss: 37.2722\n",
      "Epoch 1/5, Batch 1088/7596, Batch Loss: 36.5866\n",
      "Epoch 1/5, Batch 1089/7596, Batch Loss: 40.3238\n",
      "Epoch 1/5, Batch 1090/7596, Batch Loss: 21774.8398\n",
      "Epoch 1/5, Batch 1091/7596, Batch Loss: 40.5394\n",
      "Epoch 1/5, Batch 1092/7596, Batch Loss: 42.0220\n",
      "Epoch 1/5, Batch 1093/7596, Batch Loss: 32.5565\n",
      "Epoch 1/5, Batch 1094/7596, Batch Loss: 100413.6250\n",
      "Epoch 1/5, Batch 1095/7596, Batch Loss: 1619.7440\n",
      "Epoch 1/5, Batch 1096/7596, Batch Loss: 26.4738\n",
      "Epoch 1/5, Batch 1097/7596, Batch Loss: 1220.6549\n",
      "Epoch 1/5, Batch 1098/7596, Batch Loss: 83.6009\n",
      "Epoch 1/5, Batch 1099/7596, Batch Loss: 82327.9766\n",
      "Epoch 1/5, Batch 1100/7596, Batch Loss: 3049.1917\n",
      "Epoch 1/5, Batch 1101/7596, Batch Loss: 880.0228\n",
      "Epoch 1/5, Batch 1102/7596, Batch Loss: 3801.4041\n",
      "Epoch 1/5, Batch 1103/7596, Batch Loss: 30.3533\n",
      "Epoch 1/5, Batch 1104/7596, Batch Loss: 24.3376\n",
      "Epoch 1/5, Batch 1105/7596, Batch Loss: 2180.4326\n",
      "Epoch 1/5, Batch 1106/7596, Batch Loss: 2807.5981\n",
      "Epoch 1/5, Batch 1107/7596, Batch Loss: 9945.0977\n",
      "Epoch 1/5, Batch 1108/7596, Batch Loss: 82783.2891\n",
      "Epoch 1/5, Batch 1109/7596, Batch Loss: 39.1899\n",
      "Epoch 1/5, Batch 1110/7596, Batch Loss: 1478.2555\n",
      "Epoch 1/5, Batch 1111/7596, Batch Loss: 170827.4688\n",
      "Epoch 1/5, Batch 1112/7596, Batch Loss: 322296.6875\n",
      "Epoch 1/5, Batch 1113/7596, Batch Loss: 37.2053\n",
      "Epoch 1/5, Batch 1114/7596, Batch Loss: 35.1904\n",
      "Epoch 1/5, Batch 1115/7596, Batch Loss: 1314103.3750\n",
      "Epoch 1/5, Batch 1116/7596, Batch Loss: 302075.3438\n",
      "Epoch 1/5, Batch 1117/7596, Batch Loss: 26545.5977\n",
      "Epoch 1/5, Batch 1118/7596, Batch Loss: 3025.1055\n",
      "Epoch 1/5, Batch 1119/7596, Batch Loss: 17885.7461\n",
      "Epoch 1/5, Batch 1120/7596, Batch Loss: 1670.0819\n",
      "Epoch 1/5, Batch 1121/7596, Batch Loss: 4076.4658\n",
      "Epoch 1/5, Batch 1122/7596, Batch Loss: 15847.6436\n",
      "Epoch 1/5, Batch 1123/7596, Batch Loss: 6919.1709\n",
      "Epoch 1/5, Batch 1124/7596, Batch Loss: 203185.8438\n",
      "Epoch 1/5, Batch 1125/7596, Batch Loss: 9398.9160\n",
      "Epoch 1/5, Batch 1126/7596, Batch Loss: 1935.9343\n",
      "Epoch 1/5, Batch 1127/7596, Batch Loss: 2771.2998\n",
      "Epoch 1/5, Batch 1128/7596, Batch Loss: 769.7411\n",
      "Epoch 1/5, Batch 1129/7596, Batch Loss: 18229.6523\n",
      "Epoch 1/5, Batch 1130/7596, Batch Loss: 4822.2773\n",
      "Epoch 1/5, Batch 1131/7596, Batch Loss: 62.9681\n",
      "Epoch 1/5, Batch 1132/7596, Batch Loss: 42228.2930\n",
      "Epoch 1/5, Batch 1133/7596, Batch Loss: 9867.2500\n",
      "Epoch 1/5, Batch 1134/7596, Batch Loss: 19070.8477\n",
      "Epoch 1/5, Batch 1135/7596, Batch Loss: 40.4246\n",
      "Epoch 1/5, Batch 1136/7596, Batch Loss: 2687.1814\n",
      "Epoch 1/5, Batch 1137/7596, Batch Loss: 1391.8591\n",
      "Epoch 1/5, Batch 1138/7596, Batch Loss: 47824.5664\n",
      "Epoch 1/5, Batch 1139/7596, Batch Loss: 15338.5908\n",
      "Epoch 1/5, Batch 1140/7596, Batch Loss: 44.0581\n",
      "Epoch 1/5, Batch 1141/7596, Batch Loss: 42.1892\n",
      "Epoch 1/5, Batch 1142/7596, Batch Loss: 12433.1338\n",
      "Epoch 1/5, Batch 1143/7596, Batch Loss: 56665.7070\n",
      "Epoch 1/5, Batch 1144/7596, Batch Loss: 450649.2500\n",
      "Epoch 1/5, Batch 1145/7596, Batch Loss: 4477.0562\n",
      "Epoch 1/5, Batch 1146/7596, Batch Loss: 1337.5132\n",
      "Epoch 1/5, Batch 1147/7596, Batch Loss: 51.5836\n",
      "Epoch 1/5, Batch 1148/7596, Batch Loss: 849.7472\n",
      "Epoch 1/5, Batch 1149/7596, Batch Loss: 3769.7778\n",
      "Epoch 1/5, Batch 1150/7596, Batch Loss: 396.4166\n",
      "Epoch 1/5, Batch 1151/7596, Batch Loss: 459.1582\n",
      "Epoch 1/5, Batch 1152/7596, Batch Loss: 389.7362\n",
      "Epoch 1/5, Batch 1153/7596, Batch Loss: 3365.7324\n",
      "Epoch 1/5, Batch 1154/7596, Batch Loss: 350.4691\n",
      "Epoch 1/5, Batch 1155/7596, Batch Loss: 246.5360\n",
      "Epoch 1/5, Batch 1156/7596, Batch Loss: 4960.7520\n",
      "Epoch 1/5, Batch 1157/7596, Batch Loss: 120.1416\n",
      "Epoch 1/5, Batch 1158/7596, Batch Loss: 70.8178\n",
      "Epoch 1/5, Batch 1159/7596, Batch Loss: 744.8352\n",
      "Epoch 1/5, Batch 1160/7596, Batch Loss: 1441.7335\n",
      "Epoch 1/5, Batch 1161/7596, Batch Loss: 113.6144\n",
      "Epoch 1/5, Batch 1162/7596, Batch Loss: 982.8550\n",
      "Epoch 1/5, Batch 1163/7596, Batch Loss: 4635.3828\n",
      "Epoch 1/5, Batch 1164/7596, Batch Loss: 730.3417\n",
      "Epoch 1/5, Batch 1165/7596, Batch Loss: 2971.5476\n",
      "Epoch 1/5, Batch 1166/7596, Batch Loss: 3518.2622\n",
      "Epoch 1/5, Batch 1167/7596, Batch Loss: 77.2219\n",
      "Epoch 1/5, Batch 1168/7596, Batch Loss: 3658.2002\n",
      "Epoch 1/5, Batch 1169/7596, Batch Loss: 1298.8359\n",
      "Epoch 1/5, Batch 1170/7596, Batch Loss: 1100.8838\n",
      "Epoch 1/5, Batch 1171/7596, Batch Loss: 316.8259\n",
      "Epoch 1/5, Batch 1172/7596, Batch Loss: 4444.6831\n",
      "Epoch 1/5, Batch 1173/7596, Batch Loss: 12540.1318\n",
      "Epoch 1/5, Batch 1174/7596, Batch Loss: 136.4987\n",
      "Epoch 1/5, Batch 1175/7596, Batch Loss: 740.4188\n",
      "Epoch 1/5, Batch 1176/7596, Batch Loss: 39.5417\n",
      "Epoch 1/5, Batch 1177/7596, Batch Loss: 597.7867\n",
      "Epoch 1/5, Batch 1178/7596, Batch Loss: 13151.5986\n",
      "Epoch 1/5, Batch 1179/7596, Batch Loss: 40195.3555\n",
      "Epoch 1/5, Batch 1180/7596, Batch Loss: 1444.1718\n",
      "Epoch 1/5, Batch 1181/7596, Batch Loss: 2034.5940\n",
      "Epoch 1/5, Batch 1182/7596, Batch Loss: 37.4906\n",
      "Epoch 1/5, Batch 1183/7596, Batch Loss: 35.1827\n",
      "Epoch 1/5, Batch 1184/7596, Batch Loss: 43.3648\n",
      "Epoch 1/5, Batch 1185/7596, Batch Loss: 671.4032\n",
      "Epoch 1/5, Batch 1186/7596, Batch Loss: 825.4080\n",
      "Epoch 1/5, Batch 1187/7596, Batch Loss: 42.9434\n",
      "Epoch 1/5, Batch 1188/7596, Batch Loss: 2378.4705\n",
      "Epoch 1/5, Batch 1189/7596, Batch Loss: 455.5963\n",
      "Epoch 1/5, Batch 1190/7596, Batch Loss: 48.3958\n",
      "Epoch 1/5, Batch 1191/7596, Batch Loss: 2976258.0000\n",
      "Epoch 1/5, Batch 1192/7596, Batch Loss: 180.5629\n",
      "Epoch 1/5, Batch 1193/7596, Batch Loss: 58.3939\n",
      "Epoch 1/5, Batch 1194/7596, Batch Loss: 60.1892\n",
      "Epoch 1/5, Batch 1195/7596, Batch Loss: 79.9034\n",
      "Epoch 1/5, Batch 1196/7596, Batch Loss: 42.1144\n",
      "Epoch 1/5, Batch 1197/7596, Batch Loss: 1121.2175\n",
      "Epoch 1/5, Batch 1198/7596, Batch Loss: 9247.7070\n",
      "Epoch 1/5, Batch 1199/7596, Batch Loss: 46.8368\n",
      "Epoch 1/5, Batch 1200/7596, Batch Loss: 1041.9796\n",
      "Epoch 1/5, Batch 1201/7596, Batch Loss: 53.9490\n",
      "Epoch 1/5, Batch 1202/7596, Batch Loss: 903.1831\n",
      "Epoch 1/5, Batch 1203/7596, Batch Loss: 23844.2734\n",
      "Epoch 1/5, Batch 1204/7596, Batch Loss: 340.7217\n",
      "Epoch 1/5, Batch 1205/7596, Batch Loss: 28828.3633\n",
      "Epoch 1/5, Batch 1206/7596, Batch Loss: 512152.6250\n",
      "Epoch 1/5, Batch 1207/7596, Batch Loss: 15991.7002\n",
      "Epoch 1/5, Batch 1208/7596, Batch Loss: 6146.6138\n",
      "Epoch 1/5, Batch 1209/7596, Batch Loss: 4859.1860\n",
      "Epoch 1/5, Batch 1210/7596, Batch Loss: 10282.4170\n",
      "Epoch 1/5, Batch 1211/7596, Batch Loss: 41.4197\n",
      "Epoch 1/5, Batch 1212/7596, Batch Loss: 12706.6953\n",
      "Epoch 1/5, Batch 1213/7596, Batch Loss: 37.1674\n",
      "Epoch 1/5, Batch 1214/7596, Batch Loss: 24.3335\n",
      "Epoch 1/5, Batch 1215/7596, Batch Loss: 93681.3047\n",
      "Epoch 1/5, Batch 1216/7596, Batch Loss: 41955.6250\n",
      "Epoch 1/5, Batch 1217/7596, Batch Loss: 19891.1855\n",
      "Epoch 1/5, Batch 1218/7596, Batch Loss: 104308.3828\n",
      "Epoch 1/5, Batch 1219/7596, Batch Loss: 3935.1501\n",
      "Epoch 1/5, Batch 1220/7596, Batch Loss: 28763.2148\n",
      "Epoch 1/5, Batch 1221/7596, Batch Loss: 3434.4878\n",
      "Epoch 1/5, Batch 1222/7596, Batch Loss: 23.4120\n",
      "Epoch 1/5, Batch 1223/7596, Batch Loss: 15282.0400\n",
      "Epoch 1/5, Batch 1224/7596, Batch Loss: 27368.0938\n",
      "Epoch 1/5, Batch 1225/7596, Batch Loss: 25024.7344\n",
      "Epoch 1/5, Batch 1226/7596, Batch Loss: 11680.5928\n",
      "Epoch 1/5, Batch 1227/7596, Batch Loss: 20494.2500\n",
      "Epoch 1/5, Batch 1228/7596, Batch Loss: 61.1010\n",
      "Epoch 1/5, Batch 1229/7596, Batch Loss: 1948.7051\n",
      "Epoch 1/5, Batch 1230/7596, Batch Loss: 61.0418\n",
      "Epoch 1/5, Batch 1231/7596, Batch Loss: 28.2153\n",
      "Epoch 1/5, Batch 1232/7596, Batch Loss: 1951.8082\n",
      "Epoch 1/5, Batch 1233/7596, Batch Loss: 23.9536\n",
      "Epoch 1/5, Batch 1234/7596, Batch Loss: 2967.9290\n",
      "Epoch 1/5, Batch 1235/7596, Batch Loss: 759517.1875\n",
      "Epoch 1/5, Batch 1236/7596, Batch Loss: 4037.6118\n",
      "Epoch 1/5, Batch 1237/7596, Batch Loss: 39.5536\n",
      "Epoch 1/5, Batch 1238/7596, Batch Loss: 13692.4482\n",
      "Epoch 1/5, Batch 1239/7596, Batch Loss: 3112.2417\n",
      "Epoch 1/5, Batch 1240/7596, Batch Loss: 40.1370\n",
      "Epoch 1/5, Batch 1241/7596, Batch Loss: 18073.4473\n",
      "Epoch 1/5, Batch 1242/7596, Batch Loss: 14570.9854\n",
      "Epoch 1/5, Batch 1243/7596, Batch Loss: 3677.5190\n",
      "Epoch 1/5, Batch 1244/7596, Batch Loss: 12872.6396\n",
      "Epoch 1/5, Batch 1245/7596, Batch Loss: 21150.7344\n",
      "Epoch 1/5, Batch 1246/7596, Batch Loss: 6428.9526\n",
      "Epoch 1/5, Batch 1247/7596, Batch Loss: 5924.1855\n",
      "Epoch 1/5, Batch 1248/7596, Batch Loss: 69079.3203\n",
      "Epoch 1/5, Batch 1249/7596, Batch Loss: 20.4985\n",
      "Epoch 1/5, Batch 1250/7596, Batch Loss: 55875.6914\n",
      "Epoch 1/5, Batch 1251/7596, Batch Loss: 36.4693\n",
      "Epoch 1/5, Batch 1252/7596, Batch Loss: 5989.9805\n",
      "Epoch 1/5, Batch 1253/7596, Batch Loss: 23.7304\n",
      "Epoch 1/5, Batch 1254/7596, Batch Loss: 39.4435\n",
      "Epoch 1/5, Batch 1255/7596, Batch Loss: 4679.9370\n",
      "Epoch 1/5, Batch 1256/7596, Batch Loss: 41.8893\n",
      "Epoch 1/5, Batch 1257/7596, Batch Loss: 55328.9453\n",
      "Epoch 1/5, Batch 1258/7596, Batch Loss: 1506887.7500\n",
      "Epoch 1/5, Batch 1259/7596, Batch Loss: 17631.8906\n",
      "Epoch 1/5, Batch 1260/7596, Batch Loss: 37.5212\n",
      "Epoch 1/5, Batch 1261/7596, Batch Loss: 5224.5020\n",
      "Epoch 1/5, Batch 1262/7596, Batch Loss: 3553.5266\n",
      "Epoch 1/5, Batch 1263/7596, Batch Loss: 29499.8359\n",
      "Epoch 1/5, Batch 1264/7596, Batch Loss: 653.6542\n",
      "Epoch 1/5, Batch 1265/7596, Batch Loss: 594.6885\n",
      "Epoch 1/5, Batch 1266/7596, Batch Loss: 2495.0312\n",
      "Epoch 1/5, Batch 1267/7596, Batch Loss: 8327.0879\n",
      "Epoch 1/5, Batch 1268/7596, Batch Loss: 1380.3616\n",
      "Epoch 1/5, Batch 1269/7596, Batch Loss: 2593.5527\n",
      "Epoch 1/5, Batch 1270/7596, Batch Loss: 31.9087\n",
      "Epoch 1/5, Batch 1271/7596, Batch Loss: 57.0607\n",
      "Epoch 1/5, Batch 1272/7596, Batch Loss: 1265.0898\n",
      "Epoch 1/5, Batch 1273/7596, Batch Loss: 1207.4702\n",
      "Epoch 1/5, Batch 1274/7596, Batch Loss: 1388.0535\n",
      "Epoch 1/5, Batch 1275/7596, Batch Loss: 13637.1074\n",
      "Epoch 1/5, Batch 1276/7596, Batch Loss: 33305.0156\n",
      "Epoch 1/5, Batch 1277/7596, Batch Loss: 38.1562\n",
      "Epoch 1/5, Batch 1278/7596, Batch Loss: 18723.6562\n",
      "Epoch 1/5, Batch 1279/7596, Batch Loss: 2914.2437\n",
      "Epoch 1/5, Batch 1280/7596, Batch Loss: 1130.7618\n",
      "Epoch 1/5, Batch 1281/7596, Batch Loss: 278.8714\n",
      "Epoch 1/5, Batch 1282/7596, Batch Loss: 15850.5625\n",
      "Epoch 1/5, Batch 1283/7596, Batch Loss: 9058.9736\n",
      "Epoch 1/5, Batch 1284/7596, Batch Loss: 1237.2159\n",
      "Epoch 1/5, Batch 1285/7596, Batch Loss: 319.5037\n",
      "Epoch 1/5, Batch 1286/7596, Batch Loss: 51.2865\n",
      "Epoch 1/5, Batch 1287/7596, Batch Loss: 22885.8184\n",
      "Epoch 1/5, Batch 1288/7596, Batch Loss: 1688.2549\n",
      "Epoch 1/5, Batch 1289/7596, Batch Loss: 15140.2236\n",
      "Epoch 1/5, Batch 1290/7596, Batch Loss: 177.9804\n",
      "Epoch 1/5, Batch 1291/7596, Batch Loss: 41.7147\n",
      "Epoch 1/5, Batch 1292/7596, Batch Loss: 17634.3496\n",
      "Epoch 1/5, Batch 1293/7596, Batch Loss: 2627.0723\n",
      "Epoch 1/5, Batch 1294/7596, Batch Loss: 2150.1250\n",
      "Epoch 1/5, Batch 1295/7596, Batch Loss: 36.5759\n",
      "Epoch 1/5, Batch 1296/7596, Batch Loss: 1072.2799\n",
      "Epoch 1/5, Batch 1297/7596, Batch Loss: 558.0500\n",
      "Epoch 1/5, Batch 1298/7596, Batch Loss: 646.9196\n",
      "Epoch 1/5, Batch 1299/7596, Batch Loss: 550.6990\n",
      "Epoch 1/5, Batch 1300/7596, Batch Loss: 10805.7607\n",
      "Epoch 1/5, Batch 1301/7596, Batch Loss: 38.5868\n",
      "Epoch 1/5, Batch 1302/7596, Batch Loss: 38.1556\n",
      "Epoch 1/5, Batch 1303/7596, Batch Loss: 37.9028\n",
      "Epoch 1/5, Batch 1304/7596, Batch Loss: 44.8708\n",
      "Epoch 1/5, Batch 1305/7596, Batch Loss: 84088.1797\n",
      "Epoch 1/5, Batch 1306/7596, Batch Loss: 32809.4727\n",
      "Epoch 1/5, Batch 1307/7596, Batch Loss: 3298.8374\n",
      "Epoch 1/5, Batch 1308/7596, Batch Loss: 33000.4648\n",
      "Epoch 1/5, Batch 1309/7596, Batch Loss: 38.7449\n",
      "Epoch 1/5, Batch 1310/7596, Batch Loss: 22.6899\n",
      "Epoch 1/5, Batch 1311/7596, Batch Loss: 23.9264\n",
      "Epoch 1/5, Batch 1312/7596, Batch Loss: 2576.0066\n",
      "Epoch 1/5, Batch 1313/7596, Batch Loss: 38.3528\n",
      "Epoch 1/5, Batch 1314/7596, Batch Loss: 34.5129\n",
      "Epoch 1/5, Batch 1315/7596, Batch Loss: 3933.1614\n",
      "Epoch 1/5, Batch 1316/7596, Batch Loss: 23.2000\n",
      "Epoch 1/5, Batch 1317/7596, Batch Loss: 6048.6133\n",
      "Epoch 1/5, Batch 1318/7596, Batch Loss: 2322.9983\n",
      "Epoch 1/5, Batch 1319/7596, Batch Loss: 3236.8074\n",
      "Epoch 1/5, Batch 1320/7596, Batch Loss: 2822.0117\n",
      "Epoch 1/5, Batch 1321/7596, Batch Loss: 2764.2612\n",
      "Epoch 1/5, Batch 1322/7596, Batch Loss: 1523.5409\n",
      "Epoch 1/5, Batch 1323/7596, Batch Loss: 42.8595\n",
      "Epoch 1/5, Batch 1324/7596, Batch Loss: 1518.4357\n",
      "Epoch 1/5, Batch 1325/7596, Batch Loss: 1919.0847\n",
      "Epoch 1/5, Batch 1326/7596, Batch Loss: 1493.6583\n",
      "Epoch 1/5, Batch 1327/7596, Batch Loss: 1278.7036\n",
      "Epoch 1/5, Batch 1328/7596, Batch Loss: 5865.5630\n",
      "Epoch 1/5, Batch 1329/7596, Batch Loss: 3736.5569\n",
      "Epoch 1/5, Batch 1330/7596, Batch Loss: 36.8582\n",
      "Epoch 1/5, Batch 1331/7596, Batch Loss: 680.4026\n",
      "Epoch 1/5, Batch 1332/7596, Batch Loss: 426.0124\n",
      "Epoch 1/5, Batch 1333/7596, Batch Loss: 38.5348\n",
      "Epoch 1/5, Batch 1334/7596, Batch Loss: 1315.2698\n",
      "Epoch 1/5, Batch 1335/7596, Batch Loss: 1746.5916\n",
      "Epoch 1/5, Batch 1336/7596, Batch Loss: 512.8917\n",
      "Epoch 1/5, Batch 1337/7596, Batch Loss: 641.7639\n",
      "Epoch 1/5, Batch 1338/7596, Batch Loss: 1119.0966\n",
      "Epoch 1/5, Batch 1339/7596, Batch Loss: 33.6804\n",
      "Epoch 1/5, Batch 1340/7596, Batch Loss: 3934.2942\n",
      "Epoch 1/5, Batch 1341/7596, Batch Loss: 36.6415\n",
      "Epoch 1/5, Batch 1342/7596, Batch Loss: 879.8591\n",
      "Epoch 1/5, Batch 1343/7596, Batch Loss: 704.8275\n",
      "Epoch 1/5, Batch 1344/7596, Batch Loss: 516.2048\n",
      "Epoch 1/5, Batch 1345/7596, Batch Loss: 42.1451\n",
      "Epoch 1/5, Batch 1346/7596, Batch Loss: 41.4840\n",
      "Epoch 1/5, Batch 1347/7596, Batch Loss: 1549.9364\n",
      "Epoch 1/5, Batch 1348/7596, Batch Loss: 31.3468\n",
      "Epoch 1/5, Batch 1349/7596, Batch Loss: 1758.6987\n",
      "Epoch 1/5, Batch 1350/7596, Batch Loss: 732.3686\n",
      "Epoch 1/5, Batch 1351/7596, Batch Loss: 630.3140\n",
      "Epoch 1/5, Batch 1352/7596, Batch Loss: 289.7644\n",
      "Epoch 1/5, Batch 1353/7596, Batch Loss: 39.3546\n",
      "Epoch 1/5, Batch 1354/7596, Batch Loss: 653.9461\n",
      "Epoch 1/5, Batch 1355/7596, Batch Loss: 503.6485\n",
      "Epoch 1/5, Batch 1356/7596, Batch Loss: 500.0416\n",
      "Epoch 1/5, Batch 1357/7596, Batch Loss: 2229.6082\n",
      "Epoch 1/5, Batch 1358/7596, Batch Loss: 36.5945\n",
      "Epoch 1/5, Batch 1359/7596, Batch Loss: 1017.2214\n",
      "Epoch 1/5, Batch 1360/7596, Batch Loss: 39.0401\n",
      "Epoch 1/5, Batch 1361/7596, Batch Loss: 20.9503\n",
      "Epoch 1/5, Batch 1362/7596, Batch Loss: 604.0794\n",
      "Epoch 1/5, Batch 1363/7596, Batch Loss: 559.2482\n",
      "Epoch 1/5, Batch 1364/7596, Batch Loss: 4409.8857\n",
      "Epoch 1/5, Batch 1365/7596, Batch Loss: 1459.1530\n",
      "Epoch 1/5, Batch 1366/7596, Batch Loss: 986.8680\n",
      "Epoch 1/5, Batch 1367/7596, Batch Loss: 40.5009\n",
      "Epoch 1/5, Batch 1368/7596, Batch Loss: 38.6339\n",
      "Epoch 1/5, Batch 1369/7596, Batch Loss: 1713.1322\n",
      "Epoch 1/5, Batch 1370/7596, Batch Loss: 888.6340\n",
      "Epoch 1/5, Batch 1371/7596, Batch Loss: 830.2075\n",
      "Epoch 1/5, Batch 1372/7596, Batch Loss: 22.1806\n",
      "Epoch 1/5, Batch 1373/7596, Batch Loss: 22.3965\n",
      "Epoch 1/5, Batch 1374/7596, Batch Loss: 1108.2979\n",
      "Epoch 1/5, Batch 1375/7596, Batch Loss: 36.2597\n",
      "Epoch 1/5, Batch 1376/7596, Batch Loss: 35.9411\n",
      "Epoch 1/5, Batch 1377/7596, Batch Loss: 23.3180\n",
      "Epoch 1/5, Batch 1378/7596, Batch Loss: 39.8750\n",
      "Epoch 1/5, Batch 1379/7596, Batch Loss: 959.0732\n",
      "Epoch 1/5, Batch 1380/7596, Batch Loss: 762.2479\n",
      "Epoch 1/5, Batch 1381/7596, Batch Loss: 250.1294\n",
      "Epoch 1/5, Batch 1382/7596, Batch Loss: 1231.2494\n",
      "Epoch 1/5, Batch 1383/7596, Batch Loss: 947.8351\n",
      "Epoch 1/5, Batch 1384/7596, Batch Loss: 2385.4963\n",
      "Epoch 1/5, Batch 1385/7596, Batch Loss: 34.4447\n",
      "Epoch 1/5, Batch 1386/7596, Batch Loss: 298.1169\n",
      "Epoch 1/5, Batch 1387/7596, Batch Loss: 37.6000\n",
      "Epoch 1/5, Batch 1388/7596, Batch Loss: 135.2310\n",
      "Epoch 1/5, Batch 1389/7596, Batch Loss: 346.7823\n",
      "Epoch 1/5, Batch 1390/7596, Batch Loss: 367.3034\n",
      "Epoch 1/5, Batch 1391/7596, Batch Loss: 136.2261\n",
      "Epoch 1/5, Batch 1392/7596, Batch Loss: 2259.5430\n",
      "Epoch 1/5, Batch 1393/7596, Batch Loss: 238.4096\n",
      "Epoch 1/5, Batch 1394/7596, Batch Loss: 39.7402\n",
      "Epoch 1/5, Batch 1395/7596, Batch Loss: 433.2653\n",
      "Epoch 1/5, Batch 1396/7596, Batch Loss: 233.6111\n",
      "Epoch 1/5, Batch 1397/7596, Batch Loss: 239.6393\n",
      "Epoch 1/5, Batch 1398/7596, Batch Loss: 351.5485\n",
      "Epoch 1/5, Batch 1399/7596, Batch Loss: 235.8959\n",
      "Epoch 1/5, Batch 1400/7596, Batch Loss: 338.1501\n",
      "Epoch 1/5, Batch 1401/7596, Batch Loss: 128.8678\n",
      "Epoch 1/5, Batch 1402/7596, Batch Loss: 3270.0537\n",
      "Epoch 1/5, Batch 1403/7596, Batch Loss: 35.4500\n",
      "Epoch 1/5, Batch 1404/7596, Batch Loss: 2068.3899\n",
      "Epoch 1/5, Batch 1405/7596, Batch Loss: 37.5195\n",
      "Epoch 1/5, Batch 1406/7596, Batch Loss: 2109.8835\n",
      "Epoch 1/5, Batch 1407/7596, Batch Loss: 4918.7520\n",
      "Epoch 1/5, Batch 1408/7596, Batch Loss: 257.3943\n",
      "Epoch 1/5, Batch 1409/7596, Batch Loss: 342.6707\n",
      "Epoch 1/5, Batch 1410/7596, Batch Loss: 7172.5063\n",
      "Epoch 1/5, Batch 1411/7596, Batch Loss: 1299.7231\n",
      "Epoch 1/5, Batch 1412/7596, Batch Loss: 41.6543\n",
      "Epoch 1/5, Batch 1413/7596, Batch Loss: 975.2852\n",
      "Epoch 1/5, Batch 1414/7596, Batch Loss: 4441.4873\n",
      "Epoch 1/5, Batch 1415/7596, Batch Loss: 1954.4102\n",
      "Epoch 1/5, Batch 1416/7596, Batch Loss: 38.1535\n",
      "Epoch 1/5, Batch 1417/7596, Batch Loss: 39.8368\n",
      "Epoch 1/5, Batch 1418/7596, Batch Loss: 234.3346\n",
      "Epoch 1/5, Batch 1419/7596, Batch Loss: 35.9325\n",
      "Epoch 1/5, Batch 1420/7596, Batch Loss: 1232.7245\n",
      "Epoch 1/5, Batch 1421/7596, Batch Loss: 278.1080\n",
      "Epoch 1/5, Batch 1422/7596, Batch Loss: 36.7201\n",
      "Epoch 1/5, Batch 1423/7596, Batch Loss: 790.9289\n",
      "Epoch 1/5, Batch 1424/7596, Batch Loss: 611.7745\n",
      "Epoch 1/5, Batch 1425/7596, Batch Loss: 36.3348\n",
      "Epoch 1/5, Batch 1426/7596, Batch Loss: 341.3266\n",
      "Epoch 1/5, Batch 1427/7596, Batch Loss: 50.8621\n",
      "Epoch 1/5, Batch 1428/7596, Batch Loss: 51.1840\n",
      "Epoch 1/5, Batch 1429/7596, Batch Loss: 526.3271\n",
      "Epoch 1/5, Batch 1430/7596, Batch Loss: 1222.5037\n",
      "Epoch 1/5, Batch 1431/7596, Batch Loss: 648.1292\n",
      "Epoch 1/5, Batch 1432/7596, Batch Loss: 296.4404\n",
      "Epoch 1/5, Batch 1433/7596, Batch Loss: 22.7700\n",
      "Epoch 1/5, Batch 1434/7596, Batch Loss: 531.8696\n",
      "Epoch 1/5, Batch 1435/7596, Batch Loss: 824.9863\n",
      "Epoch 1/5, Batch 1436/7596, Batch Loss: 35.4605\n",
      "Epoch 1/5, Batch 1437/7596, Batch Loss: 397.7224\n",
      "Epoch 1/5, Batch 1438/7596, Batch Loss: 626.7096\n",
      "Epoch 1/5, Batch 1439/7596, Batch Loss: 331.7855\n",
      "Epoch 1/5, Batch 1440/7596, Batch Loss: 212.1837\n",
      "Epoch 1/5, Batch 1441/7596, Batch Loss: 138.2127\n",
      "Epoch 1/5, Batch 1442/7596, Batch Loss: 138.0431\n",
      "Epoch 1/5, Batch 1443/7596, Batch Loss: 14573.2393\n",
      "Epoch 1/5, Batch 1444/7596, Batch Loss: 37.2695\n",
      "Epoch 1/5, Batch 1445/7596, Batch Loss: 4542.6104\n",
      "Epoch 1/5, Batch 1446/7596, Batch Loss: 8696.2715\n",
      "Epoch 1/5, Batch 1447/7596, Batch Loss: 1938.8613\n",
      "Epoch 1/5, Batch 1448/7596, Batch Loss: 1226.1171\n",
      "Epoch 1/5, Batch 1449/7596, Batch Loss: 24.1995\n",
      "Epoch 1/5, Batch 1450/7596, Batch Loss: 1628.2771\n",
      "Epoch 1/5, Batch 1451/7596, Batch Loss: 21.7118\n",
      "Epoch 1/5, Batch 1452/7596, Batch Loss: 36.3699\n",
      "Epoch 1/5, Batch 1453/7596, Batch Loss: 10904.4170\n",
      "Epoch 1/5, Batch 1454/7596, Batch Loss: 69.9049\n",
      "Epoch 1/5, Batch 1455/7596, Batch Loss: 3212.4287\n",
      "Epoch 1/5, Batch 1456/7596, Batch Loss: 3132.5505\n",
      "Epoch 1/5, Batch 1457/7596, Batch Loss: 3618.7915\n",
      "Epoch 1/5, Batch 1458/7596, Batch Loss: 1385.5742\n",
      "Epoch 1/5, Batch 1459/7596, Batch Loss: 2295.5483\n",
      "Epoch 1/5, Batch 1460/7596, Batch Loss: 2705.1958\n",
      "Epoch 1/5, Batch 1461/7596, Batch Loss: 1155.0240\n",
      "Epoch 1/5, Batch 1462/7596, Batch Loss: 508.2969\n",
      "Epoch 1/5, Batch 1463/7596, Batch Loss: 4696.7427\n",
      "Epoch 1/5, Batch 1464/7596, Batch Loss: 1698.0770\n",
      "Epoch 1/5, Batch 1465/7596, Batch Loss: 287.9975\n",
      "Epoch 1/5, Batch 1466/7596, Batch Loss: 44.1849\n",
      "Epoch 1/5, Batch 1467/7596, Batch Loss: 3928.1814\n",
      "Epoch 1/5, Batch 1468/7596, Batch Loss: 5883.1045\n",
      "Epoch 1/5, Batch 1469/7596, Batch Loss: 33.8149\n",
      "Epoch 1/5, Batch 1470/7596, Batch Loss: 9550.9053\n",
      "Epoch 1/5, Batch 1471/7596, Batch Loss: 2206.2188\n",
      "Epoch 1/5, Batch 1472/7596, Batch Loss: 22.7821\n",
      "Epoch 1/5, Batch 1473/7596, Batch Loss: 1815.0406\n",
      "Epoch 1/5, Batch 1474/7596, Batch Loss: 1472.2178\n",
      "Epoch 1/5, Batch 1475/7596, Batch Loss: 32.5187\n",
      "Epoch 1/5, Batch 1476/7596, Batch Loss: 1480.0851\n",
      "Epoch 1/5, Batch 1477/7596, Batch Loss: 33.2562\n",
      "Epoch 1/5, Batch 1478/7596, Batch Loss: 3579.5361\n",
      "Epoch 1/5, Batch 1479/7596, Batch Loss: 123.5042\n",
      "Epoch 1/5, Batch 1480/7596, Batch Loss: 49.7313\n",
      "Epoch 1/5, Batch 1481/7596, Batch Loss: 11781.8564\n",
      "Epoch 1/5, Batch 1482/7596, Batch Loss: 2355.5134\n",
      "Epoch 1/5, Batch 1483/7596, Batch Loss: 1916.9111\n",
      "Epoch 1/5, Batch 1484/7596, Batch Loss: 5319.3467\n",
      "Epoch 1/5, Batch 1485/7596, Batch Loss: 148.3486\n",
      "Epoch 1/5, Batch 1486/7596, Batch Loss: 2515.5464\n",
      "Epoch 1/5, Batch 1487/7596, Batch Loss: 3230.8440\n",
      "Epoch 1/5, Batch 1488/7596, Batch Loss: 37.3543\n",
      "Epoch 1/5, Batch 1489/7596, Batch Loss: 715.4199\n",
      "Epoch 1/5, Batch 1490/7596, Batch Loss: 38.3516\n",
      "Epoch 1/5, Batch 1491/7596, Batch Loss: 311.2682\n",
      "Epoch 1/5, Batch 1492/7596, Batch Loss: 826.0391\n",
      "Epoch 1/5, Batch 1493/7596, Batch Loss: 40.1155\n",
      "Epoch 1/5, Batch 1494/7596, Batch Loss: 182.7771\n",
      "Epoch 1/5, Batch 1495/7596, Batch Loss: 59.7579\n",
      "Epoch 1/5, Batch 1496/7596, Batch Loss: 34.8056\n",
      "Epoch 1/5, Batch 1497/7596, Batch Loss: 41.9683\n",
      "Epoch 1/5, Batch 1498/7596, Batch Loss: 532.0106\n",
      "Epoch 1/5, Batch 1499/7596, Batch Loss: 552.2172\n",
      "Epoch 1/5, Batch 1500/7596, Batch Loss: 914.2645\n",
      "Epoch 1/5, Batch 1501/7596, Batch Loss: 38.6333\n",
      "Epoch 1/5, Batch 1502/7596, Batch Loss: 368.4376\n",
      "Epoch 1/5, Batch 1503/7596, Batch Loss: 268.8064\n",
      "Epoch 1/5, Batch 1504/7596, Batch Loss: 2648.8171\n",
      "Epoch 1/5, Batch 1505/7596, Batch Loss: 393.6929\n",
      "Epoch 1/5, Batch 1506/7596, Batch Loss: 303.9949\n",
      "Epoch 1/5, Batch 1507/7596, Batch Loss: 672.1364\n",
      "Epoch 1/5, Batch 1508/7596, Batch Loss: 39.7127\n",
      "Epoch 1/5, Batch 1509/7596, Batch Loss: 19.4643\n",
      "Epoch 1/5, Batch 1510/7596, Batch Loss: 198.1102\n",
      "Epoch 1/5, Batch 1511/7596, Batch Loss: 162.9030\n",
      "Epoch 1/5, Batch 1512/7596, Batch Loss: 62.6848\n",
      "Epoch 1/5, Batch 1513/7596, Batch Loss: 37.8503\n",
      "Epoch 1/5, Batch 1514/7596, Batch Loss: 32.8291\n",
      "Epoch 1/5, Batch 1515/7596, Batch Loss: 37.3557\n",
      "Epoch 1/5, Batch 1516/7596, Batch Loss: 199.6933\n",
      "Epoch 1/5, Batch 1517/7596, Batch Loss: 219.8640\n",
      "Epoch 1/5, Batch 1518/7596, Batch Loss: 41.9622\n",
      "Epoch 1/5, Batch 1519/7596, Batch Loss: 29.0880\n",
      "Epoch 1/5, Batch 1520/7596, Batch Loss: 171.1287\n",
      "Epoch 1/5, Batch 1521/7596, Batch Loss: 158.1221\n",
      "Epoch 1/5, Batch 1522/7596, Batch Loss: 38.0371\n",
      "Epoch 1/5, Batch 1523/7596, Batch Loss: 128.4756\n",
      "Epoch 1/5, Batch 1524/7596, Batch Loss: 56.2672\n",
      "Epoch 1/5, Batch 1525/7596, Batch Loss: 8637.5654\n",
      "Epoch 1/5, Batch 1526/7596, Batch Loss: 155.1402\n",
      "Epoch 1/5, Batch 1527/7596, Batch Loss: 4719.7349\n",
      "Epoch 1/5, Batch 1528/7596, Batch Loss: 36.7918\n",
      "Epoch 1/5, Batch 1529/7596, Batch Loss: 91.6018\n",
      "Epoch 1/5, Batch 1530/7596, Batch Loss: 2683.5093\n",
      "Epoch 1/5, Batch 1531/7596, Batch Loss: 42.0367\n",
      "Epoch 1/5, Batch 1532/7596, Batch Loss: 19995.8242\n",
      "Epoch 1/5, Batch 1533/7596, Batch Loss: 1515.2826\n",
      "Epoch 1/5, Batch 1534/7596, Batch Loss: 958.9261\n",
      "Epoch 1/5, Batch 1535/7596, Batch Loss: 1087.4838\n",
      "Epoch 1/5, Batch 1536/7596, Batch Loss: 36.9066\n",
      "Epoch 1/5, Batch 1537/7596, Batch Loss: 1012.8684\n",
      "Epoch 1/5, Batch 1538/7596, Batch Loss: 907.0546\n",
      "Epoch 1/5, Batch 1539/7596, Batch Loss: 879.3242\n",
      "Epoch 1/5, Batch 1540/7596, Batch Loss: 923.5507\n",
      "Epoch 1/5, Batch 1541/7596, Batch Loss: 230.2799\n",
      "Epoch 1/5, Batch 1542/7596, Batch Loss: 12216.3506\n",
      "Epoch 1/5, Batch 1543/7596, Batch Loss: 1666.9690\n",
      "Epoch 1/5, Batch 1544/7596, Batch Loss: 1300.8992\n",
      "Epoch 1/5, Batch 1545/7596, Batch Loss: 407.1980\n",
      "Epoch 1/5, Batch 1546/7596, Batch Loss: 1011.8946\n",
      "Epoch 1/5, Batch 1547/7596, Batch Loss: 37.7148\n",
      "Epoch 1/5, Batch 1548/7596, Batch Loss: 22.6613\n",
      "Epoch 1/5, Batch 1549/7596, Batch Loss: 34.8742\n",
      "Epoch 1/5, Batch 1550/7596, Batch Loss: 30315.0781\n",
      "Epoch 1/5, Batch 1551/7596, Batch Loss: 17156.7168\n",
      "Epoch 1/5, Batch 1552/7596, Batch Loss: 41.2486\n",
      "Epoch 1/5, Batch 1553/7596, Batch Loss: 10378.9160\n",
      "Epoch 1/5, Batch 1554/7596, Batch Loss: 5162.4004\n",
      "Epoch 1/5, Batch 1555/7596, Batch Loss: 32015.3867\n",
      "Epoch 1/5, Batch 1556/7596, Batch Loss: 135.7128\n",
      "Epoch 1/5, Batch 1557/7596, Batch Loss: 193.7971\n",
      "Epoch 1/5, Batch 1558/7596, Batch Loss: 81794.3438\n",
      "Epoch 1/5, Batch 1559/7596, Batch Loss: 219.9275\n",
      "Epoch 1/5, Batch 1560/7596, Batch Loss: 5092.0190\n",
      "Epoch 1/5, Batch 1561/7596, Batch Loss: 96942.5312\n",
      "Epoch 1/5, Batch 1562/7596, Batch Loss: 41.3439\n",
      "Epoch 1/5, Batch 1563/7596, Batch Loss: 5861.3242\n",
      "Epoch 1/5, Batch 1564/7596, Batch Loss: 727.0476\n",
      "Epoch 1/5, Batch 1565/7596, Batch Loss: 36.9770\n",
      "Epoch 1/5, Batch 1566/7596, Batch Loss: 919.5432\n",
      "Epoch 1/5, Batch 1567/7596, Batch Loss: 960.2738\n",
      "Epoch 1/5, Batch 1568/7596, Batch Loss: 39.5424\n",
      "Epoch 1/5, Batch 1569/7596, Batch Loss: 22.4345\n",
      "Epoch 1/5, Batch 1570/7596, Batch Loss: 1219.3666\n",
      "Epoch 1/5, Batch 1571/7596, Batch Loss: 3121.6492\n",
      "Epoch 1/5, Batch 1572/7596, Batch Loss: 796.3496\n",
      "Epoch 1/5, Batch 1573/7596, Batch Loss: 194.6122\n",
      "Epoch 1/5, Batch 1574/7596, Batch Loss: 628.4182\n",
      "Epoch 1/5, Batch 1575/7596, Batch Loss: 1627.7095\n",
      "Epoch 1/5, Batch 1576/7596, Batch Loss: 44.4522\n",
      "Epoch 1/5, Batch 1577/7596, Batch Loss: 19.2788\n",
      "Epoch 1/5, Batch 1578/7596, Batch Loss: 959.9564\n",
      "Epoch 1/5, Batch 1579/7596, Batch Loss: 13130.7666\n",
      "Epoch 1/5, Batch 1580/7596, Batch Loss: 738.3611\n",
      "Epoch 1/5, Batch 1581/7596, Batch Loss: 36.9959\n",
      "Epoch 1/5, Batch 1582/7596, Batch Loss: 1185.4434\n",
      "Epoch 1/5, Batch 1583/7596, Batch Loss: 420.6700\n",
      "Epoch 1/5, Batch 1584/7596, Batch Loss: 143328.5625\n",
      "Epoch 1/5, Batch 1585/7596, Batch Loss: 25.4901\n",
      "Epoch 1/5, Batch 1586/7596, Batch Loss: 7408.2017\n",
      "Epoch 1/5, Batch 1587/7596, Batch Loss: 20864.0723\n",
      "Epoch 1/5, Batch 1588/7596, Batch Loss: 95680.7031\n",
      "Epoch 1/5, Batch 1589/7596, Batch Loss: 11714.8994\n",
      "Epoch 1/5, Batch 1590/7596, Batch Loss: 1671.3107\n",
      "Epoch 1/5, Batch 1591/7596, Batch Loss: 139.0612\n",
      "Epoch 1/5, Batch 1592/7596, Batch Loss: 28739.6953\n",
      "Epoch 1/5, Batch 1593/7596, Batch Loss: 133269.9844\n",
      "Epoch 1/5, Batch 1594/7596, Batch Loss: 27571.7852\n",
      "Epoch 1/5, Batch 1595/7596, Batch Loss: 4155.6553\n",
      "Epoch 1/5, Batch 1596/7596, Batch Loss: 19.9802\n",
      "Epoch 1/5, Batch 1597/7596, Batch Loss: 49828.8359\n",
      "Epoch 1/5, Batch 1598/7596, Batch Loss: 127666.5234\n",
      "Epoch 1/5, Batch 1599/7596, Batch Loss: 37.8948\n",
      "Epoch 1/5, Batch 1600/7596, Batch Loss: 41620.7227\n",
      "Epoch 1/5, Batch 1601/7596, Batch Loss: 40285.5312\n",
      "Epoch 1/5, Batch 1602/7596, Batch Loss: 25339.3652\n",
      "Epoch 1/5, Batch 1603/7596, Batch Loss: 23538.9980\n",
      "Epoch 1/5, Batch 1604/7596, Batch Loss: 76828.6328\n",
      "Epoch 1/5, Batch 1605/7596, Batch Loss: 40734.0820\n",
      "Epoch 1/5, Batch 1606/7596, Batch Loss: 2144.1929\n",
      "Epoch 1/5, Batch 1607/7596, Batch Loss: 2154.7783\n",
      "Epoch 1/5, Batch 1608/7596, Batch Loss: 2307.4097\n",
      "Epoch 1/5, Batch 1609/7596, Batch Loss: 1242.6260\n",
      "Epoch 1/5, Batch 1610/7596, Batch Loss: 60.5196\n",
      "Epoch 1/5, Batch 1611/7596, Batch Loss: 2316.2410\n",
      "Epoch 1/5, Batch 1612/7596, Batch Loss: 42.7936\n",
      "Epoch 1/5, Batch 1613/7596, Batch Loss: 46.7043\n",
      "Epoch 1/5, Batch 1614/7596, Batch Loss: 37.5829\n",
      "Epoch 1/5, Batch 1615/7596, Batch Loss: 35.4418\n",
      "Epoch 1/5, Batch 1616/7596, Batch Loss: 3430.1392\n",
      "Epoch 1/5, Batch 1617/7596, Batch Loss: 574.4141\n",
      "Epoch 1/5, Batch 1618/7596, Batch Loss: 415.0787\n",
      "Epoch 1/5, Batch 1619/7596, Batch Loss: 2410.1650\n",
      "Epoch 1/5, Batch 1620/7596, Batch Loss: 35.2225\n",
      "Epoch 1/5, Batch 1621/7596, Batch Loss: 3283.3870\n",
      "Epoch 1/5, Batch 1622/7596, Batch Loss: 1218.3098\n",
      "Epoch 1/5, Batch 1623/7596, Batch Loss: 36.4507\n",
      "Epoch 1/5, Batch 1624/7596, Batch Loss: 1600.5961\n",
      "Epoch 1/5, Batch 1625/7596, Batch Loss: 34.8546\n",
      "Epoch 1/5, Batch 1626/7596, Batch Loss: 36.7196\n",
      "Epoch 1/5, Batch 1627/7596, Batch Loss: 2025.7302\n",
      "Epoch 1/5, Batch 1628/7596, Batch Loss: 22.5458\n",
      "Epoch 1/5, Batch 1629/7596, Batch Loss: 2739.1499\n",
      "Epoch 1/5, Batch 1630/7596, Batch Loss: 45.2067\n",
      "Epoch 1/5, Batch 1631/7596, Batch Loss: 574.7234\n",
      "Epoch 1/5, Batch 1632/7596, Batch Loss: 3139.0120\n",
      "Epoch 1/5, Batch 1633/7596, Batch Loss: 32.5608\n",
      "Epoch 1/5, Batch 1634/7596, Batch Loss: 4915821.5000\n",
      "Epoch 1/5, Batch 1635/7596, Batch Loss: 127.2546\n",
      "Epoch 1/5, Batch 1636/7596, Batch Loss: 37893.6523\n",
      "Epoch 1/5, Batch 1637/7596, Batch Loss: 1168152.0000\n",
      "Epoch 1/5, Batch 1638/7596, Batch Loss: 804456.3125\n",
      "Epoch 1/5, Batch 1639/7596, Batch Loss: 124881.4219\n",
      "Epoch 1/5, Batch 1640/7596, Batch Loss: 83009.8984\n",
      "Epoch 1/5, Batch 1641/7596, Batch Loss: 1675.0299\n",
      "Epoch 1/5, Batch 1642/7596, Batch Loss: 24365.5664\n",
      "Epoch 1/5, Batch 1643/7596, Batch Loss: 4493.3340\n",
      "Epoch 1/5, Batch 1644/7596, Batch Loss: 4636.7544\n",
      "Epoch 1/5, Batch 1645/7596, Batch Loss: 2043.3813\n",
      "Epoch 1/5, Batch 1646/7596, Batch Loss: 1741.0890\n",
      "Epoch 1/5, Batch 1647/7596, Batch Loss: 4316.4570\n",
      "Epoch 1/5, Batch 1648/7596, Batch Loss: 9626.9922\n",
      "Epoch 1/5, Batch 1649/7596, Batch Loss: 1183922.6250\n",
      "Epoch 1/5, Batch 1650/7596, Batch Loss: 27212.7891\n",
      "Epoch 1/5, Batch 1651/7596, Batch Loss: 63073004.0000\n",
      "Epoch 1/5, Batch 1652/7596, Batch Loss: 105034.1641\n",
      "Epoch 1/5, Batch 1653/7596, Batch Loss: 31238.0684\n",
      "Epoch 1/5, Batch 1654/7596, Batch Loss: 30112.6777\n",
      "Epoch 1/5, Batch 1655/7596, Batch Loss: 23031.9766\n",
      "Epoch 1/5, Batch 1656/7596, Batch Loss: 2501.6482\n",
      "Epoch 1/5, Batch 1657/7596, Batch Loss: 14234.9268\n",
      "Epoch 1/5, Batch 1658/7596, Batch Loss: 15364.6299\n",
      "Epoch 1/5, Batch 1659/7596, Batch Loss: 950.8309\n",
      "Epoch 1/5, Batch 1660/7596, Batch Loss: 68886.3047\n",
      "Epoch 1/5, Batch 1661/7596, Batch Loss: 49823.2227\n",
      "Epoch 1/5, Batch 1662/7596, Batch Loss: 1633487.8750\n",
      "Epoch 1/5, Batch 1663/7596, Batch Loss: 13555.5996\n",
      "Epoch 1/5, Batch 1664/7596, Batch Loss: 8592.9160\n",
      "Epoch 1/5, Batch 1665/7596, Batch Loss: 172.8237\n",
      "Epoch 1/5, Batch 1666/7596, Batch Loss: 71689.9297\n",
      "Epoch 1/5, Batch 1667/7596, Batch Loss: 28897.1426\n",
      "Epoch 1/5, Batch 1668/7596, Batch Loss: 447259.9688\n",
      "Epoch 1/5, Batch 1669/7596, Batch Loss: 10146.5361\n",
      "Epoch 1/5, Batch 1670/7596, Batch Loss: 110.2365\n",
      "Epoch 1/5, Batch 1671/7596, Batch Loss: 42.4435\n",
      "Epoch 1/5, Batch 1672/7596, Batch Loss: 229307.5625\n",
      "Epoch 1/5, Batch 1673/7596, Batch Loss: 26027.0410\n",
      "Epoch 1/5, Batch 1674/7596, Batch Loss: 689808.1250\n",
      "Epoch 1/5, Batch 1675/7596, Batch Loss: 167586.4531\n",
      "Epoch 1/5, Batch 1676/7596, Batch Loss: 13548192.0000\n",
      "Epoch 1/5, Batch 1677/7596, Batch Loss: 37376.8633\n",
      "Epoch 1/5, Batch 1678/7596, Batch Loss: 221754.7969\n",
      "Epoch 1/5, Batch 1679/7596, Batch Loss: 111381.8906\n",
      "Epoch 1/5, Batch 1680/7596, Batch Loss: 34268.9023\n",
      "Epoch 1/5, Batch 1681/7596, Batch Loss: 27388.0000\n",
      "Epoch 1/5, Batch 1682/7596, Batch Loss: 11402.7285\n",
      "Epoch 1/5, Batch 1683/7596, Batch Loss: 57.5622\n",
      "Epoch 1/5, Batch 1684/7596, Batch Loss: 28943.9375\n",
      "Epoch 1/5, Batch 1685/7596, Batch Loss: 36181.8633\n",
      "Epoch 1/5, Batch 1686/7596, Batch Loss: 43307.4688\n",
      "Epoch 1/5, Batch 1687/7596, Batch Loss: 43440.7773\n",
      "Epoch 1/5, Batch 1688/7596, Batch Loss: 40.4761\n",
      "Epoch 1/5, Batch 1689/7596, Batch Loss: 43.3713\n",
      "Epoch 1/5, Batch 1690/7596, Batch Loss: 282244.7500\n",
      "Epoch 1/5, Batch 1691/7596, Batch Loss: 20646.0781\n",
      "Epoch 1/5, Batch 1692/7596, Batch Loss: 7919.8784\n",
      "Epoch 1/5, Batch 1693/7596, Batch Loss: 14317.0361\n",
      "Epoch 1/5, Batch 1694/7596, Batch Loss: 86.5070\n",
      "Epoch 1/5, Batch 1695/7596, Batch Loss: 16793.5723\n",
      "Epoch 1/5, Batch 1696/7596, Batch Loss: 5465.6860\n",
      "Epoch 1/5, Batch 1697/7596, Batch Loss: 15385.5879\n",
      "Epoch 1/5, Batch 1698/7596, Batch Loss: 18393.4219\n",
      "Epoch 1/5, Batch 1699/7596, Batch Loss: 324188.4062\n",
      "Epoch 1/5, Batch 1700/7596, Batch Loss: 27211.6973\n",
      "Epoch 1/5, Batch 1701/7596, Batch Loss: 55281.5000\n",
      "Epoch 1/5, Batch 1702/7596, Batch Loss: 31528.1230\n",
      "Epoch 1/5, Batch 1703/7596, Batch Loss: 251874.5000\n",
      "Epoch 1/5, Batch 1704/7596, Batch Loss: 716180.5625\n",
      "Epoch 1/5, Batch 1705/7596, Batch Loss: 82.5173\n",
      "Epoch 1/5, Batch 1706/7596, Batch Loss: 11312872.0000\n",
      "Epoch 1/5, Batch 1707/7596, Batch Loss: 786345.5625\n",
      "Epoch 1/5, Batch 1708/7596, Batch Loss: 21204.8340\n",
      "Epoch 1/5, Batch 1709/7596, Batch Loss: 1462893.7500\n",
      "Epoch 1/5, Batch 1710/7596, Batch Loss: 451416.1250\n",
      "Epoch 1/5, Batch 1711/7596, Batch Loss: 3443883.0000\n",
      "Epoch 1/5, Batch 1712/7596, Batch Loss: 10503.0176\n",
      "Epoch 1/5, Batch 1713/7596, Batch Loss: 164560.0469\n",
      "Epoch 1/5, Batch 1714/7596, Batch Loss: 145.0932\n",
      "Epoch 1/5, Batch 1715/7596, Batch Loss: 8721514.0000\n",
      "Epoch 1/5, Batch 1716/7596, Batch Loss: 1642.3448\n",
      "Epoch 1/5, Batch 1717/7596, Batch Loss: 8565.5371\n",
      "Epoch 1/5, Batch 1718/7596, Batch Loss: 12862.9062\n",
      "Epoch 1/5, Batch 1719/7596, Batch Loss: 919789.6250\n",
      "Epoch 1/5, Batch 1720/7596, Batch Loss: 424982176.0000\n",
      "Epoch 1/5, Batch 1721/7596, Batch Loss: 33759.7109\n",
      "Epoch 1/5, Batch 1722/7596, Batch Loss: 93606.3906\n",
      "Epoch 1/5, Batch 1723/7596, Batch Loss: 79083.5703\n",
      "Epoch 1/5, Batch 1724/7596, Batch Loss: 291949.6875\n",
      "Epoch 1/5, Batch 1725/7596, Batch Loss: 117134.8438\n",
      "Epoch 1/5, Batch 1726/7596, Batch Loss: 324402.2500\n",
      "Epoch 1/5, Batch 1727/7596, Batch Loss: 158560.4531\n",
      "Epoch 1/5, Batch 1728/7596, Batch Loss: 30808.1309\n",
      "Epoch 1/5, Batch 1729/7596, Batch Loss: 12055.0781\n",
      "Epoch 1/5, Batch 1730/7596, Batch Loss: 43929.3945\n",
      "Epoch 1/5, Batch 1731/7596, Batch Loss: 19232.9160\n",
      "Epoch 1/5, Batch 1732/7596, Batch Loss: 4981.2236\n",
      "Epoch 1/5, Batch 1733/7596, Batch Loss: 4424.6816\n",
      "Epoch 1/5, Batch 1734/7596, Batch Loss: 11508.4375\n",
      "Epoch 1/5, Batch 1735/7596, Batch Loss: 180115.2969\n",
      "Epoch 1/5, Batch 1736/7596, Batch Loss: 5987.5498\n",
      "Epoch 1/5, Batch 1737/7596, Batch Loss: 17700.8613\n",
      "Epoch 1/5, Batch 1738/7596, Batch Loss: 16721.7461\n",
      "Epoch 1/5, Batch 1739/7596, Batch Loss: 95612.3750\n",
      "Epoch 1/5, Batch 1740/7596, Batch Loss: 10685.0459\n",
      "Epoch 1/5, Batch 1741/7596, Batch Loss: 1967.2659\n",
      "Epoch 1/5, Batch 1742/7596, Batch Loss: 127150.9062\n",
      "Epoch 1/5, Batch 1743/7596, Batch Loss: 3523.7109\n",
      "Epoch 1/5, Batch 1744/7596, Batch Loss: 24409.5176\n",
      "Epoch 1/5, Batch 1745/7596, Batch Loss: 13144.8154\n",
      "Epoch 1/5, Batch 1746/7596, Batch Loss: 24966.7656\n",
      "Epoch 1/5, Batch 1747/7596, Batch Loss: 6416.4341\n",
      "Epoch 1/5, Batch 1748/7596, Batch Loss: 21154.1406\n",
      "Epoch 1/5, Batch 1749/7596, Batch Loss: 5669.3188\n",
      "Epoch 1/5, Batch 1750/7596, Batch Loss: 6813.9551\n",
      "Epoch 1/5, Batch 1751/7596, Batch Loss: 7227.8213\n",
      "Epoch 1/5, Batch 1752/7596, Batch Loss: 4271.1943\n",
      "Epoch 1/5, Batch 1753/7596, Batch Loss: 3993982.5000\n",
      "Epoch 1/5, Batch 1754/7596, Batch Loss: 68.4947\n",
      "Epoch 1/5, Batch 1755/7596, Batch Loss: 4406.5337\n",
      "Epoch 1/5, Batch 1756/7596, Batch Loss: 20944.9473\n",
      "Epoch 1/5, Batch 1757/7596, Batch Loss: 431.2461\n",
      "Epoch 1/5, Batch 1758/7596, Batch Loss: 2462.3293\n",
      "Epoch 1/5, Batch 1759/7596, Batch Loss: 11685.2246\n",
      "Epoch 1/5, Batch 1760/7596, Batch Loss: 16353.1465\n",
      "Epoch 1/5, Batch 1761/7596, Batch Loss: 44335.1055\n",
      "Epoch 1/5, Batch 1762/7596, Batch Loss: 70746.3516\n",
      "Epoch 1/5, Batch 1763/7596, Batch Loss: 21929.0996\n",
      "Epoch 1/5, Batch 1764/7596, Batch Loss: 28123.9062\n",
      "Epoch 1/5, Batch 1765/7596, Batch Loss: 2682.4077\n",
      "Epoch 1/5, Batch 1766/7596, Batch Loss: 3931.0647\n",
      "Epoch 1/5, Batch 1767/7596, Batch Loss: 36469.5781\n",
      "Epoch 1/5, Batch 1768/7596, Batch Loss: 61.9691\n",
      "Epoch 1/5, Batch 1769/7596, Batch Loss: 10656.9541\n",
      "Epoch 1/5, Batch 1770/7596, Batch Loss: 19398.9609\n",
      "Epoch 1/5, Batch 1771/7596, Batch Loss: 6875.1138\n",
      "Epoch 1/5, Batch 1772/7596, Batch Loss: 4807.3521\n",
      "Epoch 1/5, Batch 1773/7596, Batch Loss: 50.0661\n",
      "Epoch 1/5, Batch 1774/7596, Batch Loss: 15594.7070\n",
      "Epoch 1/5, Batch 1775/7596, Batch Loss: 42.8855\n",
      "Epoch 1/5, Batch 1776/7596, Batch Loss: 45.1031\n",
      "Epoch 1/5, Batch 1777/7596, Batch Loss: 24699.2148\n",
      "Epoch 1/5, Batch 1778/7596, Batch Loss: 3098.0208\n",
      "Epoch 1/5, Batch 1779/7596, Batch Loss: 4335.0649\n",
      "Epoch 1/5, Batch 1780/7596, Batch Loss: 41.5436\n",
      "Epoch 1/5, Batch 1781/7596, Batch Loss: 32843.1367\n",
      "Epoch 1/5, Batch 1782/7596, Batch Loss: 5236.2407\n",
      "Epoch 1/5, Batch 1783/7596, Batch Loss: 303.4914\n",
      "Epoch 1/5, Batch 1784/7596, Batch Loss: 1577.5339\n",
      "Epoch 1/5, Batch 1785/7596, Batch Loss: 898.8353\n",
      "Epoch 1/5, Batch 1786/7596, Batch Loss: 40.7997\n",
      "Epoch 1/5, Batch 1787/7596, Batch Loss: 855.1596\n",
      "Epoch 1/5, Batch 1788/7596, Batch Loss: 884.9403\n",
      "Epoch 1/5, Batch 1789/7596, Batch Loss: 20483.1270\n",
      "Epoch 1/5, Batch 1790/7596, Batch Loss: 1569.9082\n",
      "Epoch 1/5, Batch 1791/7596, Batch Loss: 1239.1046\n",
      "Epoch 1/5, Batch 1792/7596, Batch Loss: 2640.4695\n",
      "Epoch 1/5, Batch 1793/7596, Batch Loss: 4717.9312\n",
      "Epoch 1/5, Batch 1794/7596, Batch Loss: 56.9192\n",
      "Epoch 1/5, Batch 1795/7596, Batch Loss: 9156.9258\n",
      "Epoch 1/5, Batch 1796/7596, Batch Loss: 2504.7722\n",
      "Epoch 1/5, Batch 1797/7596, Batch Loss: 6255.3521\n",
      "Epoch 1/5, Batch 1798/7596, Batch Loss: 4261.3960\n",
      "Epoch 1/5, Batch 1799/7596, Batch Loss: 3110.5378\n",
      "Epoch 1/5, Batch 1800/7596, Batch Loss: 2635.7896\n",
      "Epoch 1/5, Batch 1801/7596, Batch Loss: 1568.9202\n",
      "Epoch 1/5, Batch 1802/7596, Batch Loss: 1875.0052\n",
      "Epoch 1/5, Batch 1803/7596, Batch Loss: 3704.1304\n",
      "Epoch 1/5, Batch 1804/7596, Batch Loss: 864.0132\n",
      "Epoch 1/5, Batch 1805/7596, Batch Loss: 449.8055\n",
      "Epoch 1/5, Batch 1806/7596, Batch Loss: 1474.1149\n",
      "Epoch 1/5, Batch 1807/7596, Batch Loss: 7037.3916\n",
      "Epoch 1/5, Batch 1808/7596, Batch Loss: 41520.9219\n",
      "Epoch 1/5, Batch 1809/7596, Batch Loss: 40.0685\n",
      "Epoch 1/5, Batch 1810/7596, Batch Loss: 6565.1230\n",
      "Epoch 1/5, Batch 1811/7596, Batch Loss: 3228.8262\n",
      "Epoch 1/5, Batch 1812/7596, Batch Loss: 183.8318\n",
      "Epoch 1/5, Batch 1813/7596, Batch Loss: 78.9174\n",
      "Epoch 1/5, Batch 1814/7596, Batch Loss: 3510.4937\n",
      "Epoch 1/5, Batch 1815/7596, Batch Loss: 79.0068\n",
      "Epoch 1/5, Batch 1816/7596, Batch Loss: 883.6578\n",
      "Epoch 1/5, Batch 1817/7596, Batch Loss: 1664.1370\n",
      "Epoch 1/5, Batch 1818/7596, Batch Loss: 9410.4648\n",
      "Epoch 1/5, Batch 1819/7596, Batch Loss: 35.0685\n",
      "Epoch 1/5, Batch 1820/7596, Batch Loss: 375061.5938\n",
      "Epoch 1/5, Batch 1821/7596, Batch Loss: 20253.7012\n",
      "Epoch 1/5, Batch 1822/7596, Batch Loss: 5490.0088\n",
      "Epoch 1/5, Batch 1823/7596, Batch Loss: 34.2301\n",
      "Epoch 1/5, Batch 1824/7596, Batch Loss: 6196.0557\n",
      "Epoch 1/5, Batch 1825/7596, Batch Loss: 2627.9294\n",
      "Epoch 1/5, Batch 1826/7596, Batch Loss: 37.8902\n",
      "Epoch 1/5, Batch 1827/7596, Batch Loss: 12902.4404\n",
      "Epoch 1/5, Batch 1828/7596, Batch Loss: 48.9783\n",
      "Epoch 1/5, Batch 1829/7596, Batch Loss: 1261207.6250\n",
      "Epoch 1/5, Batch 1830/7596, Batch Loss: 3284.3584\n",
      "Epoch 1/5, Batch 1831/7596, Batch Loss: 11602.7314\n",
      "Epoch 1/5, Batch 1832/7596, Batch Loss: 2730.0439\n",
      "Epoch 1/5, Batch 1833/7596, Batch Loss: 62.9326\n",
      "Epoch 1/5, Batch 1834/7596, Batch Loss: 23.3633\n",
      "Epoch 1/5, Batch 1835/7596, Batch Loss: 65807.1953\n",
      "Epoch 1/5, Batch 1836/7596, Batch Loss: 16863.9004\n",
      "Epoch 1/5, Batch 1837/7596, Batch Loss: 2759862.2500\n",
      "Epoch 1/5, Batch 1838/7596, Batch Loss: 19584.0391\n",
      "Epoch 1/5, Batch 1839/7596, Batch Loss: 8939.1445\n",
      "Epoch 1/5, Batch 1840/7596, Batch Loss: 449589.0625\n",
      "Epoch 1/5, Batch 1841/7596, Batch Loss: 15168.2207\n",
      "Epoch 1/5, Batch 1842/7596, Batch Loss: 147721.1094\n",
      "Epoch 1/5, Batch 1843/7596, Batch Loss: 199.0842\n",
      "Epoch 1/5, Batch 1844/7596, Batch Loss: 14977.0273\n",
      "Epoch 1/5, Batch 1845/7596, Batch Loss: 3374.3867\n",
      "Epoch 1/5, Batch 1846/7596, Batch Loss: 3502.9473\n",
      "Epoch 1/5, Batch 1847/7596, Batch Loss: 8128.3657\n",
      "Epoch 1/5, Batch 1848/7596, Batch Loss: 4955.7183\n",
      "Epoch 1/5, Batch 1849/7596, Batch Loss: 3426.9722\n",
      "Epoch 1/5, Batch 1850/7596, Batch Loss: 293.9984\n",
      "Epoch 1/5, Batch 1851/7596, Batch Loss: 505.9690\n",
      "Epoch 1/5, Batch 1852/7596, Batch Loss: 8316.4717\n",
      "Epoch 1/5, Batch 1853/7596, Batch Loss: 3263.1211\n",
      "Epoch 1/5, Batch 1854/7596, Batch Loss: 1218.2094\n",
      "Epoch 1/5, Batch 1855/7596, Batch Loss: 6670.4990\n",
      "Epoch 1/5, Batch 1856/7596, Batch Loss: 7830.7222\n",
      "Epoch 1/5, Batch 1857/7596, Batch Loss: 447.9659\n",
      "Epoch 1/5, Batch 1858/7596, Batch Loss: 7635.9580\n",
      "Epoch 1/5, Batch 1859/7596, Batch Loss: 4663.3271\n",
      "Epoch 1/5, Batch 1860/7596, Batch Loss: 333.5894\n",
      "Epoch 1/5, Batch 1861/7596, Batch Loss: 7390.0571\n",
      "Epoch 1/5, Batch 1862/7596, Batch Loss: 37369.5078\n",
      "Epoch 1/5, Batch 1863/7596, Batch Loss: 94987.8984\n",
      "Epoch 1/5, Batch 1864/7596, Batch Loss: 2774.2998\n",
      "Epoch 1/5, Batch 1865/7596, Batch Loss: 2255.5239\n",
      "Epoch 1/5, Batch 1866/7596, Batch Loss: 3589.7568\n",
      "Epoch 1/5, Batch 1867/7596, Batch Loss: 2423.8440\n",
      "Epoch 1/5, Batch 1868/7596, Batch Loss: 1421.3589\n",
      "Epoch 1/5, Batch 1869/7596, Batch Loss: 366.0327\n",
      "Epoch 1/5, Batch 1870/7596, Batch Loss: 2798.7188\n",
      "Epoch 1/5, Batch 1871/7596, Batch Loss: 2306.1963\n",
      "Epoch 1/5, Batch 1872/7596, Batch Loss: 18851.2188\n",
      "Epoch 1/5, Batch 1873/7596, Batch Loss: 16468.7520\n",
      "Epoch 1/5, Batch 1874/7596, Batch Loss: 56457.3789\n",
      "Epoch 1/5, Batch 1875/7596, Batch Loss: 3131.2832\n",
      "Epoch 1/5, Batch 1876/7596, Batch Loss: 57921.1289\n",
      "Epoch 1/5, Batch 1877/7596, Batch Loss: 8592.5137\n",
      "Epoch 1/5, Batch 1878/7596, Batch Loss: 52295.0703\n",
      "Epoch 1/5, Batch 1879/7596, Batch Loss: 41223.8594\n",
      "Epoch 1/5, Batch 1880/7596, Batch Loss: 5312.9531\n",
      "Epoch 1/5, Batch 1881/7596, Batch Loss: 4763.0376\n",
      "Epoch 1/5, Batch 1882/7596, Batch Loss: 16803.2402\n",
      "Epoch 1/5, Batch 1883/7596, Batch Loss: 41616.1992\n",
      "Epoch 1/5, Batch 1884/7596, Batch Loss: 293.3825\n",
      "Epoch 1/5, Batch 1885/7596, Batch Loss: 7121.0908\n",
      "Epoch 1/5, Batch 1886/7596, Batch Loss: 226.9318\n",
      "Epoch 1/5, Batch 1887/7596, Batch Loss: 8639.8066\n",
      "Epoch 1/5, Batch 1888/7596, Batch Loss: 36192.6133\n",
      "Epoch 1/5, Batch 1889/7596, Batch Loss: 545.7814\n",
      "Epoch 1/5, Batch 1890/7596, Batch Loss: 293.2314\n",
      "Epoch 1/5, Batch 1891/7596, Batch Loss: 193.7028\n",
      "Epoch 1/5, Batch 1892/7596, Batch Loss: 55.2410\n",
      "Epoch 1/5, Batch 1893/7596, Batch Loss: 20153.4766\n",
      "Epoch 1/5, Batch 1894/7596, Batch Loss: 10510.0596\n",
      "Epoch 1/5, Batch 1895/7596, Batch Loss: 529009.7500\n",
      "Epoch 1/5, Batch 1896/7596, Batch Loss: 6270.1499\n",
      "Epoch 1/5, Batch 1897/7596, Batch Loss: 850236.4375\n",
      "Epoch 1/5, Batch 1898/7596, Batch Loss: 20653.6973\n",
      "Epoch 1/5, Batch 1899/7596, Batch Loss: 1449523.1250\n",
      "Epoch 1/5, Batch 1900/7596, Batch Loss: 119.6273\n",
      "Epoch 1/5, Batch 1901/7596, Batch Loss: 304053.0625\n",
      "Epoch 1/5, Batch 1902/7596, Batch Loss: 31080.5391\n",
      "Epoch 1/5, Batch 1903/7596, Batch Loss: 143731.6719\n",
      "Epoch 1/5, Batch 1904/7596, Batch Loss: 1259449.2500\n",
      "Epoch 1/5, Batch 1905/7596, Batch Loss: 797346.4375\n",
      "Epoch 1/5, Batch 1906/7596, Batch Loss: 412751.3750\n",
      "Epoch 1/5, Batch 1907/7596, Batch Loss: 27844.4863\n",
      "Epoch 1/5, Batch 1908/7596, Batch Loss: 22166.0684\n",
      "Epoch 1/5, Batch 1909/7596, Batch Loss: 19228.1504\n",
      "Epoch 1/5, Batch 1910/7596, Batch Loss: 2554.5293\n",
      "Epoch 1/5, Batch 1911/7596, Batch Loss: 23503.9961\n",
      "Epoch 1/5, Batch 1912/7596, Batch Loss: 115.9826\n",
      "Epoch 1/5, Batch 1913/7596, Batch Loss: 167.2467\n",
      "Epoch 1/5, Batch 1914/7596, Batch Loss: 299.9278\n",
      "Epoch 1/5, Batch 1915/7596, Batch Loss: 124485.8516\n",
      "Epoch 1/5, Batch 1916/7596, Batch Loss: 23525.5625\n",
      "Epoch 1/5, Batch 1917/7596, Batch Loss: 2818.7148\n",
      "Epoch 1/5, Batch 1918/7596, Batch Loss: 179551.3125\n",
      "Epoch 1/5, Batch 1919/7596, Batch Loss: 2113794.5000\n",
      "Epoch 1/5, Batch 1920/7596, Batch Loss: 21.7511\n",
      "Epoch 1/5, Batch 1921/7596, Batch Loss: 244305.7812\n",
      "Epoch 1/5, Batch 1922/7596, Batch Loss: 40.7770\n",
      "Epoch 1/5, Batch 1923/7596, Batch Loss: 45.2367\n",
      "Epoch 1/5, Batch 1924/7596, Batch Loss: 1368514.3750\n",
      "Epoch 1/5, Batch 1925/7596, Batch Loss: 189041.0469\n",
      "Epoch 1/5, Batch 1926/7596, Batch Loss: 153327.6094\n",
      "Epoch 1/5, Batch 1927/7596, Batch Loss: 254242.0625\n",
      "Epoch 1/5, Batch 1928/7596, Batch Loss: 140278.4688\n",
      "Epoch 1/5, Batch 1929/7596, Batch Loss: 38.5733\n",
      "Epoch 1/5, Batch 1930/7596, Batch Loss: 20540.3145\n",
      "Epoch 1/5, Batch 1931/7596, Batch Loss: 13839.7910\n",
      "Epoch 1/5, Batch 1932/7596, Batch Loss: 29560.2832\n",
      "Epoch 1/5, Batch 1933/7596, Batch Loss: 32842.1875\n",
      "Epoch 1/5, Batch 1934/7596, Batch Loss: 243.8510\n",
      "Epoch 1/5, Batch 1935/7596, Batch Loss: 47589.9492\n",
      "Epoch 1/5, Batch 1936/7596, Batch Loss: 10061.5742\n",
      "Epoch 1/5, Batch 1937/7596, Batch Loss: 37.3723\n",
      "Epoch 1/5, Batch 1938/7596, Batch Loss: 99.4150\n",
      "Epoch 1/5, Batch 1939/7596, Batch Loss: 1624.2285\n",
      "Epoch 1/5, Batch 1940/7596, Batch Loss: 2820.2373\n",
      "Epoch 1/5, Batch 1941/7596, Batch Loss: 29.0849\n",
      "Epoch 1/5, Batch 1942/7596, Batch Loss: 123042.9609\n",
      "Epoch 1/5, Batch 1943/7596, Batch Loss: 420328.0000\n",
      "Epoch 1/5, Batch 1944/7596, Batch Loss: 99653.6172\n",
      "Epoch 1/5, Batch 1945/7596, Batch Loss: 13628.9375\n",
      "Epoch 1/5, Batch 1946/7596, Batch Loss: 70700.5234\n",
      "Epoch 1/5, Batch 1947/7596, Batch Loss: 43490.8984\n",
      "Epoch 1/5, Batch 1948/7596, Batch Loss: 111729.1328\n",
      "Epoch 1/5, Batch 1949/7596, Batch Loss: 82734.5625\n",
      "Epoch 1/5, Batch 1950/7596, Batch Loss: 24009.0879\n",
      "Epoch 1/5, Batch 1951/7596, Batch Loss: 30158.0977\n",
      "Epoch 1/5, Batch 1952/7596, Batch Loss: 15456.4844\n",
      "Epoch 1/5, Batch 1953/7596, Batch Loss: 12505.1309\n",
      "Epoch 1/5, Batch 1954/7596, Batch Loss: 8926.8184\n",
      "Epoch 1/5, Batch 1955/7596, Batch Loss: 12805.7822\n",
      "Epoch 1/5, Batch 1956/7596, Batch Loss: 1426.8846\n",
      "Epoch 1/5, Batch 1957/7596, Batch Loss: 36.0698\n",
      "Epoch 1/5, Batch 1958/7596, Batch Loss: 11332.7314\n",
      "Epoch 1/5, Batch 1959/7596, Batch Loss: 11.3215\n",
      "Epoch 1/5, Batch 1960/7596, Batch Loss: 16656.4668\n",
      "Epoch 1/5, Batch 1961/7596, Batch Loss: 5510.5054\n",
      "Epoch 1/5, Batch 1962/7596, Batch Loss: 3983.5718\n",
      "Epoch 1/5, Batch 1963/7596, Batch Loss: 2155.3188\n",
      "Epoch 1/5, Batch 1964/7596, Batch Loss: 835789.0625\n",
      "Epoch 1/5, Batch 1965/7596, Batch Loss: 9434.7920\n",
      "Epoch 1/5, Batch 1966/7596, Batch Loss: 23355.6328\n",
      "Epoch 1/5, Batch 1967/7596, Batch Loss: 5512.4302\n",
      "Epoch 1/5, Batch 1968/7596, Batch Loss: 63794.5117\n",
      "Epoch 1/5, Batch 1969/7596, Batch Loss: 62945.9961\n",
      "Epoch 1/5, Batch 1970/7596, Batch Loss: 78183.3516\n",
      "Epoch 1/5, Batch 1971/7596, Batch Loss: 36.6957\n",
      "Epoch 1/5, Batch 1972/7596, Batch Loss: 620714.0625\n",
      "Epoch 1/5, Batch 1973/7596, Batch Loss: 242710.0781\n",
      "Epoch 1/5, Batch 1974/7596, Batch Loss: 40.0581\n",
      "Epoch 1/5, Batch 1975/7596, Batch Loss: 38.1373\n",
      "Epoch 1/5, Batch 1976/7596, Batch Loss: 3505698.5000\n",
      "Epoch 1/5, Batch 1977/7596, Batch Loss: 32033028.0000\n",
      "Epoch 1/5, Batch 1978/7596, Batch Loss: 40.1981\n",
      "Epoch 1/5, Batch 1979/7596, Batch Loss: 4375351.0000\n",
      "Epoch 1/5, Batch 1980/7596, Batch Loss: 405367.6562\n",
      "Epoch 1/5, Batch 1981/7596, Batch Loss: 52393448.0000\n",
      "Epoch 1/5, Batch 1982/7596, Batch Loss: 8418819.0000\n",
      "Epoch 1/5, Batch 1983/7596, Batch Loss: 774568.8750\n",
      "Epoch 1/5, Batch 1984/7596, Batch Loss: 40.9205\n",
      "Epoch 1/5, Batch 1985/7596, Batch Loss: 22.8235\n",
      "Epoch 1/5, Batch 1986/7596, Batch Loss: 24.8202\n",
      "Epoch 1/5, Batch 1987/7596, Batch Loss: 5939950592.0000\n",
      "Epoch 1/5, Batch 1988/7596, Batch Loss: 17665.2852\n",
      "Epoch 1/5, Batch 1989/7596, Batch Loss: 66311.1172\n",
      "Epoch 1/5, Batch 1990/7596, Batch Loss: 62514.8594\n",
      "Epoch 1/5, Batch 1991/7596, Batch Loss: 37.5783\n",
      "Epoch 1/5, Batch 1992/7596, Batch Loss: 101766.2188\n",
      "Epoch 1/5, Batch 1993/7596, Batch Loss: 298739.0938\n",
      "Epoch 1/5, Batch 1994/7596, Batch Loss: 56935.1523\n",
      "Epoch 1/5, Batch 1995/7596, Batch Loss: 81.2242\n",
      "Epoch 1/5, Batch 1996/7596, Batch Loss: 54.2281\n",
      "Epoch 1/5, Batch 1997/7596, Batch Loss: 100045808.0000\n",
      "Epoch 1/5, Batch 1998/7596, Batch Loss: 123435.8516\n",
      "Epoch 1/5, Batch 1999/7596, Batch Loss: 176664.6719\n",
      "Epoch 1/5, Batch 2000/7596, Batch Loss: 208121.4531\n",
      "Epoch 1/5, Batch 2001/7596, Batch Loss: 226277.5938\n",
      "Epoch 1/5, Batch 2002/7596, Batch Loss: 160777.4375\n",
      "Epoch 1/5, Batch 2003/7596, Batch Loss: 972278.1875\n",
      "Epoch 1/5, Batch 2004/7596, Batch Loss: 1509195.8750\n",
      "Epoch 1/5, Batch 2005/7596, Batch Loss: 591820.6250\n",
      "Epoch 1/5, Batch 2006/7596, Batch Loss: 26188440.0000\n",
      "Epoch 1/5, Batch 2007/7596, Batch Loss: 1441096.2500\n",
      "Epoch 1/5, Batch 2008/7596, Batch Loss: 32.4275\n",
      "Epoch 1/5, Batch 2009/7596, Batch Loss: 31.1181\n",
      "Epoch 1/5, Batch 2010/7596, Batch Loss: 443946.8750\n",
      "Epoch 1/5, Batch 2011/7596, Batch Loss: 29.3093\n",
      "Epoch 1/5, Batch 2012/7596, Batch Loss: 1673597.8750\n",
      "Epoch 1/5, Batch 2013/7596, Batch Loss: 877815.3750\n",
      "Epoch 1/5, Batch 2014/7596, Batch Loss: 1023651.7500\n",
      "Epoch 1/5, Batch 2015/7596, Batch Loss: 1818711.3750\n",
      "Epoch 1/5, Batch 2016/7596, Batch Loss: 14979687.0000\n",
      "Epoch 1/5, Batch 2017/7596, Batch Loss: 1484861.5000\n",
      "Epoch 1/5, Batch 2018/7596, Batch Loss: 1698343.1250\n",
      "Epoch 1/5, Batch 2019/7596, Batch Loss: 3074792.2500\n",
      "Epoch 1/5, Batch 2020/7596, Batch Loss: 4717.6650\n",
      "Epoch 1/5, Batch 2021/7596, Batch Loss: 4347.9321\n",
      "Epoch 1/5, Batch 2022/7596, Batch Loss: 21.1446\n",
      "Epoch 1/5, Batch 2023/7596, Batch Loss: 446772.3750\n",
      "Epoch 1/5, Batch 2024/7596, Batch Loss: 342148.9375\n",
      "Epoch 1/5, Batch 2025/7596, Batch Loss: 216589.1719\n",
      "Epoch 1/5, Batch 2026/7596, Batch Loss: 882762.5000\n",
      "Epoch 1/5, Batch 2027/7596, Batch Loss: 726481.5000\n",
      "Epoch 1/5, Batch 2028/7596, Batch Loss: 114379.2109\n",
      "Epoch 1/5, Batch 2029/7596, Batch Loss: 41.1633\n",
      "Epoch 1/5, Batch 2030/7596, Batch Loss: 39.9091\n",
      "Epoch 1/5, Batch 2031/7596, Batch Loss: 152986208.0000\n",
      "Epoch 1/5, Batch 2032/7596, Batch Loss: 3917005.7500\n",
      "Epoch 1/5, Batch 2033/7596, Batch Loss: 33.2474\n",
      "Epoch 1/5, Batch 2034/7596, Batch Loss: 24.8819\n",
      "Epoch 1/5, Batch 2035/7596, Batch Loss: 150839.0625\n",
      "Epoch 1/5, Batch 2036/7596, Batch Loss: 20.0555\n",
      "Epoch 1/5, Batch 2037/7596, Batch Loss: 174146192.0000\n",
      "Epoch 1/5, Batch 2038/7596, Batch Loss: 552688064.0000\n",
      "Epoch 1/5, Batch 2039/7596, Batch Loss: 1934616.0000\n",
      "Epoch 1/5, Batch 2040/7596, Batch Loss: 2847249.0000\n",
      "Epoch 1/5, Batch 2041/7596, Batch Loss: 376209.1562\n",
      "Epoch 1/5, Batch 2042/7596, Batch Loss: 2340191.0000\n",
      "Epoch 1/5, Batch 2043/7596, Batch Loss: 392432.3438\n",
      "Epoch 1/5, Batch 2044/7596, Batch Loss: 279116.6875\n",
      "Epoch 1/5, Batch 2045/7596, Batch Loss: 77641120.0000\n",
      "Epoch 1/5, Batch 2046/7596, Batch Loss: 360.1174\n",
      "Epoch 1/5, Batch 2047/7596, Batch Loss: 262278.6562\n",
      "Epoch 1/5, Batch 2048/7596, Batch Loss: 1430722.6250\n",
      "Epoch 1/5, Batch 2049/7596, Batch Loss: 1511204.0000\n",
      "Epoch 1/5, Batch 2050/7596, Batch Loss: 94.5335\n",
      "Epoch 1/5, Batch 2051/7596, Batch Loss: 67824.3906\n",
      "Epoch 1/5, Batch 2052/7596, Batch Loss: 576304.8125\n",
      "Epoch 1/5, Batch 2053/7596, Batch Loss: 7411.5688\n",
      "Epoch 1/5, Batch 2054/7596, Batch Loss: 92015.0781\n",
      "Epoch 1/5, Batch 2055/7596, Batch Loss: 21733.8691\n",
      "Epoch 1/5, Batch 2056/7596, Batch Loss: 759.8878\n",
      "Epoch 1/5, Batch 2057/7596, Batch Loss: 198085.5000\n",
      "Epoch 1/5, Batch 2058/7596, Batch Loss: 154193.6719\n",
      "Epoch 1/5, Batch 2059/7596, Batch Loss: 84101.8906\n",
      "Epoch 1/5, Batch 2060/7596, Batch Loss: 114524.5703\n",
      "Epoch 1/5, Batch 2061/7596, Batch Loss: 42844.7891\n",
      "Epoch 1/5, Batch 2062/7596, Batch Loss: 170226.1719\n",
      "Epoch 1/5, Batch 2063/7596, Batch Loss: 32273.3574\n",
      "Epoch 1/5, Batch 2064/7596, Batch Loss: 74865.4844\n",
      "Epoch 1/5, Batch 2065/7596, Batch Loss: 454403.3125\n",
      "Epoch 1/5, Batch 2066/7596, Batch Loss: 360127.0000\n",
      "Epoch 1/5, Batch 2067/7596, Batch Loss: 918.9431\n",
      "Epoch 1/5, Batch 2068/7596, Batch Loss: 18254.1855\n",
      "Epoch 1/5, Batch 2069/7596, Batch Loss: 478732.0938\n",
      "Epoch 1/5, Batch 2070/7596, Batch Loss: 325372.1562\n",
      "Epoch 1/5, Batch 2071/7596, Batch Loss: 46094.5938\n",
      "Epoch 1/5, Batch 2072/7596, Batch Loss: 15461.9043\n",
      "Epoch 1/5, Batch 2073/7596, Batch Loss: 73666.3828\n",
      "Epoch 1/5, Batch 2074/7596, Batch Loss: 1172921.7500\n",
      "Epoch 1/5, Batch 2075/7596, Batch Loss: 661356.8750\n",
      "Epoch 1/5, Batch 2076/7596, Batch Loss: 125079.9922\n",
      "Epoch 1/5, Batch 2077/7596, Batch Loss: 453043.9375\n",
      "Epoch 1/5, Batch 2078/7596, Batch Loss: 221460.9531\n",
      "Epoch 1/5, Batch 2079/7596, Batch Loss: 973.5796\n",
      "Epoch 1/5, Batch 2080/7596, Batch Loss: 1506.8823\n",
      "Epoch 1/5, Batch 2081/7596, Batch Loss: 76528.5156\n",
      "Epoch 1/5, Batch 2082/7596, Batch Loss: 10764.2822\n",
      "Epoch 1/5, Batch 2083/7596, Batch Loss: 42383.3125\n",
      "Epoch 1/5, Batch 2084/7596, Batch Loss: 1644.4781\n",
      "Epoch 1/5, Batch 2085/7596, Batch Loss: 2999.9644\n",
      "Epoch 1/5, Batch 2086/7596, Batch Loss: 1159.2974\n",
      "Epoch 1/5, Batch 2087/7596, Batch Loss: 135963.7188\n",
      "Epoch 1/5, Batch 2088/7596, Batch Loss: 1562.4878\n",
      "Epoch 1/5, Batch 2089/7596, Batch Loss: 1596.7062\n",
      "Epoch 1/5, Batch 2090/7596, Batch Loss: 65.7747\n",
      "Epoch 1/5, Batch 2091/7596, Batch Loss: 79062.0234\n",
      "Epoch 1/5, Batch 2092/7596, Batch Loss: 6520.4082\n",
      "Epoch 1/5, Batch 2093/7596, Batch Loss: 750.2006\n",
      "Epoch 1/5, Batch 2094/7596, Batch Loss: 1084389.1250\n",
      "Epoch 1/5, Batch 2095/7596, Batch Loss: 243749.7812\n",
      "Epoch 1/5, Batch 2096/7596, Batch Loss: 13971.0869\n",
      "Epoch 1/5, Batch 2097/7596, Batch Loss: 94943.3750\n",
      "Epoch 1/5, Batch 2098/7596, Batch Loss: 1336.1874\n",
      "Epoch 1/5, Batch 2099/7596, Batch Loss: 93919.4844\n",
      "Epoch 1/5, Batch 2100/7596, Batch Loss: 88207.1328\n",
      "Epoch 1/5, Batch 2101/7596, Batch Loss: 23971.2227\n",
      "Epoch 1/5, Batch 2102/7596, Batch Loss: 39357.7031\n",
      "Epoch 1/5, Batch 2103/7596, Batch Loss: 40222.0312\n",
      "Epoch 1/5, Batch 2104/7596, Batch Loss: 15262918.0000\n",
      "Epoch 1/5, Batch 2105/7596, Batch Loss: 21345862.0000\n",
      "Epoch 1/5, Batch 2106/7596, Batch Loss: 460747.2188\n",
      "Epoch 1/5, Batch 2107/7596, Batch Loss: 69.9732\n",
      "Epoch 1/5, Batch 2108/7596, Batch Loss: 463.5455\n",
      "Epoch 1/5, Batch 2109/7596, Batch Loss: 44183.0781\n",
      "Epoch 1/5, Batch 2110/7596, Batch Loss: 3468091.0000\n",
      "Epoch 1/5, Batch 2111/7596, Batch Loss: 301904.6250\n",
      "Epoch 1/5, Batch 2112/7596, Batch Loss: 137314.2344\n",
      "Epoch 1/5, Batch 2113/7596, Batch Loss: 1485157.1250\n",
      "Epoch 1/5, Batch 2114/7596, Batch Loss: 34143244.0000\n",
      "Epoch 1/5, Batch 2115/7596, Batch Loss: 679203.7500\n",
      "Epoch 1/5, Batch 2116/7596, Batch Loss: 122584584.0000\n",
      "Epoch 1/5, Batch 2117/7596, Batch Loss: 559818.5625\n",
      "Epoch 1/5, Batch 2118/7596, Batch Loss: 671844.5625\n",
      "Epoch 1/5, Batch 2119/7596, Batch Loss: 30747970.0000\n",
      "Epoch 1/5, Batch 2120/7596, Batch Loss: 9800449.0000\n",
      "Epoch 1/5, Batch 2121/7596, Batch Loss: 748385.5000\n",
      "Epoch 1/5, Batch 2122/7596, Batch Loss: 1102549.5000\n",
      "Epoch 1/5, Batch 2123/7596, Batch Loss: 127.3565\n",
      "Epoch 1/5, Batch 2124/7596, Batch Loss: 621984.8750\n",
      "Epoch 1/5, Batch 2125/7596, Batch Loss: 170479.6875\n",
      "Epoch 1/5, Batch 2126/7596, Batch Loss: 77199.8438\n",
      "Epoch 1/5, Batch 2127/7596, Batch Loss: 4327.0420\n",
      "Epoch 1/5, Batch 2128/7596, Batch Loss: 238836.0781\n",
      "Epoch 1/5, Batch 2129/7596, Batch Loss: 81.1906\n",
      "Epoch 1/5, Batch 2130/7596, Batch Loss: 38372.3359\n",
      "Epoch 1/5, Batch 2131/7596, Batch Loss: 2741.5330\n",
      "Epoch 1/5, Batch 2132/7596, Batch Loss: 35198.0781\n",
      "Epoch 1/5, Batch 2133/7596, Batch Loss: 83733.0156\n",
      "Epoch 1/5, Batch 2134/7596, Batch Loss: 46658.7500\n",
      "Epoch 1/5, Batch 2135/7596, Batch Loss: 10889.4287\n",
      "Epoch 1/5, Batch 2136/7596, Batch Loss: 36.6503\n",
      "Epoch 1/5, Batch 2137/7596, Batch Loss: 35668.3086\n",
      "Epoch 1/5, Batch 2138/7596, Batch Loss: 77540.1953\n",
      "Epoch 1/5, Batch 2139/7596, Batch Loss: 107584.9141\n",
      "Epoch 1/5, Batch 2140/7596, Batch Loss: 12222060.0000\n",
      "Epoch 1/5, Batch 2141/7596, Batch Loss: 6082684.5000\n",
      "Epoch 1/5, Batch 2142/7596, Batch Loss: 12254152.0000\n",
      "Epoch 1/5, Batch 2143/7596, Batch Loss: 207541616.0000\n",
      "Epoch 1/5, Batch 2144/7596, Batch Loss: 162116848.0000\n",
      "Epoch 1/5, Batch 2145/7596, Batch Loss: 7355635.5000\n",
      "Epoch 1/5, Batch 2146/7596, Batch Loss: 5007067.0000\n",
      "Epoch 1/5, Batch 2147/7596, Batch Loss: 3697833.0000\n",
      "Epoch 1/5, Batch 2148/7596, Batch Loss: 1394567.2500\n",
      "Epoch 1/5, Batch 2149/7596, Batch Loss: 38.4668\n",
      "Epoch 1/5, Batch 2150/7596, Batch Loss: 43.1035\n",
      "Epoch 1/5, Batch 2151/7596, Batch Loss: 39.2670\n",
      "Epoch 1/5, Batch 2152/7596, Batch Loss: 1035933.3750\n",
      "Epoch 1/5, Batch 2153/7596, Batch Loss: 37.7283\n",
      "Epoch 1/5, Batch 2154/7596, Batch Loss: 35.6221\n",
      "Epoch 1/5, Batch 2155/7596, Batch Loss: 7723789.5000\n",
      "Epoch 1/5, Batch 2156/7596, Batch Loss: 2670005.5000\n",
      "Epoch 1/5, Batch 2157/7596, Batch Loss: 6024101.5000\n",
      "Epoch 1/5, Batch 2158/7596, Batch Loss: 388279.4062\n",
      "Epoch 1/5, Batch 2159/7596, Batch Loss: 57249.5781\n",
      "Epoch 1/5, Batch 2160/7596, Batch Loss: 70936.5312\n",
      "Epoch 1/5, Batch 2161/7596, Batch Loss: 1261468.0000\n",
      "Epoch 1/5, Batch 2162/7596, Batch Loss: 1849195.6250\n",
      "Epoch 1/5, Batch 2163/7596, Batch Loss: 4285268.5000\n",
      "Epoch 1/5, Batch 2164/7596, Batch Loss: 20185815040.0000\n",
      "Epoch 1/5, Batch 2165/7596, Batch Loss: 2019832.5000\n",
      "Epoch 1/5, Batch 2166/7596, Batch Loss: 92736.8125\n",
      "Epoch 1/5, Batch 2167/7596, Batch Loss: 777201.6250\n",
      "Epoch 1/5, Batch 2168/7596, Batch Loss: 35.8634\n",
      "Epoch 1/5, Batch 2169/7596, Batch Loss: 558763.0000\n",
      "Epoch 1/5, Batch 2170/7596, Batch Loss: 1014302.2500\n",
      "Epoch 1/5, Batch 2171/7596, Batch Loss: 15679530.0000\n",
      "Epoch 1/5, Batch 2172/7596, Batch Loss: 884745.6875\n",
      "Epoch 1/5, Batch 2173/7596, Batch Loss: 104270.5078\n",
      "Epoch 1/5, Batch 2174/7596, Batch Loss: 145882.4844\n",
      "Epoch 1/5, Batch 2175/7596, Batch Loss: 40.1167\n",
      "Epoch 1/5, Batch 2176/7596, Batch Loss: 34.8993\n",
      "Epoch 1/5, Batch 2177/7596, Batch Loss: 1695984.7500\n",
      "Epoch 1/5, Batch 2178/7596, Batch Loss: 92677.7031\n",
      "Epoch 1/5, Batch 2179/7596, Batch Loss: 26000.8223\n",
      "Epoch 1/5, Batch 2180/7596, Batch Loss: 17511.4199\n",
      "Epoch 1/5, Batch 2181/7596, Batch Loss: 37.5314\n",
      "Epoch 1/5, Batch 2182/7596, Batch Loss: 10381742.0000\n",
      "Epoch 1/5, Batch 2183/7596, Batch Loss: 72864.1562\n",
      "Epoch 1/5, Batch 2184/7596, Batch Loss: 317890.6875\n",
      "Epoch 1/5, Batch 2185/7596, Batch Loss: 37.1212\n",
      "Epoch 1/5, Batch 2186/7596, Batch Loss: 3996520.2500\n",
      "Epoch 1/5, Batch 2187/7596, Batch Loss: 20.0373\n",
      "Epoch 1/5, Batch 2188/7596, Batch Loss: 786617.0625\n",
      "Epoch 1/5, Batch 2189/7596, Batch Loss: 913298.6875\n",
      "Epoch 1/5, Batch 2190/7596, Batch Loss: 795484.0000\n",
      "Epoch 1/5, Batch 2191/7596, Batch Loss: 34.6208\n",
      "Epoch 1/5, Batch 2192/7596, Batch Loss: 220819.4531\n",
      "Epoch 1/5, Batch 2193/7596, Batch Loss: 182247.8438\n",
      "Epoch 1/5, Batch 2194/7596, Batch Loss: 188339.9531\n",
      "Epoch 1/5, Batch 2195/7596, Batch Loss: 281853.2812\n",
      "Epoch 1/5, Batch 2196/7596, Batch Loss: 24230.7656\n",
      "Epoch 1/5, Batch 2197/7596, Batch Loss: 146860.7656\n",
      "Epoch 1/5, Batch 2198/7596, Batch Loss: 45401.3906\n",
      "Epoch 1/5, Batch 2199/7596, Batch Loss: 94871.9688\n",
      "Epoch 1/5, Batch 2200/7596, Batch Loss: 97.4192\n",
      "Epoch 1/5, Batch 2201/7596, Batch Loss: 407700.8750\n",
      "Epoch 1/5, Batch 2202/7596, Batch Loss: 48248.7148\n",
      "Epoch 1/5, Batch 2203/7596, Batch Loss: 80819.0625\n",
      "Epoch 1/5, Batch 2204/7596, Batch Loss: 41277.4180\n",
      "Epoch 1/5, Batch 2205/7596, Batch Loss: 98530.0391\n",
      "Epoch 1/5, Batch 2206/7596, Batch Loss: 479830.9375\n",
      "Epoch 1/5, Batch 2207/7596, Batch Loss: 102249.7188\n",
      "Epoch 1/5, Batch 2208/7596, Batch Loss: 1502003.0000\n",
      "Epoch 1/5, Batch 2209/7596, Batch Loss: 169563.6250\n",
      "Epoch 1/5, Batch 2210/7596, Batch Loss: 45840.7266\n",
      "Epoch 1/5, Batch 2211/7596, Batch Loss: 90939.6953\n",
      "Epoch 1/5, Batch 2212/7596, Batch Loss: 3066.8704\n",
      "Epoch 1/5, Batch 2213/7596, Batch Loss: 33.7738\n",
      "Epoch 1/5, Batch 2214/7596, Batch Loss: 12176.8057\n",
      "Epoch 1/5, Batch 2215/7596, Batch Loss: 1524.6139\n",
      "Epoch 1/5, Batch 2216/7596, Batch Loss: 711556.1875\n",
      "Epoch 1/5, Batch 2217/7596, Batch Loss: 734.1660\n",
      "Epoch 1/5, Batch 2218/7596, Batch Loss: 24063.3047\n",
      "Epoch 1/5, Batch 2219/7596, Batch Loss: 3751.6021\n",
      "Epoch 1/5, Batch 2220/7596, Batch Loss: 714396.5625\n",
      "Epoch 1/5, Batch 2221/7596, Batch Loss: 2082.9387\n",
      "Epoch 1/5, Batch 2222/7596, Batch Loss: 151707.3750\n",
      "Epoch 1/5, Batch 2223/7596, Batch Loss: 401035.7500\n",
      "Epoch 1/5, Batch 2224/7596, Batch Loss: 429364.1875\n",
      "Epoch 1/5, Batch 2225/7596, Batch Loss: 35.2756\n",
      "Epoch 1/5, Batch 2226/7596, Batch Loss: 19.2733\n",
      "Epoch 1/5, Batch 2227/7596, Batch Loss: 993392.8750\n",
      "Epoch 1/5, Batch 2228/7596, Batch Loss: 129196.0391\n",
      "Epoch 1/5, Batch 2229/7596, Batch Loss: 210750.0781\n",
      "Epoch 1/5, Batch 2230/7596, Batch Loss: 163529.3125\n",
      "Epoch 1/5, Batch 2231/7596, Batch Loss: 119207.1094\n",
      "Epoch 1/5, Batch 2232/7596, Batch Loss: 55641.0898\n",
      "Epoch 1/5, Batch 2233/7596, Batch Loss: 68034.5234\n",
      "Epoch 1/5, Batch 2234/7596, Batch Loss: 54709.1094\n",
      "Epoch 1/5, Batch 2235/7596, Batch Loss: 103862.8516\n",
      "Epoch 1/5, Batch 2236/7596, Batch Loss: 112423.3828\n",
      "Epoch 1/5, Batch 2237/7596, Batch Loss: 190000.0469\n",
      "Epoch 1/5, Batch 2238/7596, Batch Loss: 38.2071\n",
      "Epoch 1/5, Batch 2239/7596, Batch Loss: 36.4960\n",
      "Epoch 1/5, Batch 2240/7596, Batch Loss: 1352344.2500\n",
      "Epoch 1/5, Batch 2241/7596, Batch Loss: 193345.5000\n",
      "Epoch 1/5, Batch 2242/7596, Batch Loss: 47473.7539\n",
      "Epoch 1/5, Batch 2243/7596, Batch Loss: 26373.8750\n",
      "Epoch 1/5, Batch 2244/7596, Batch Loss: 24399.2715\n",
      "Epoch 1/5, Batch 2245/7596, Batch Loss: 30.6410\n",
      "Epoch 1/5, Batch 2246/7596, Batch Loss: 107.5083\n",
      "Epoch 1/5, Batch 2247/7596, Batch Loss: 83985.0234\n",
      "Epoch 1/5, Batch 2248/7596, Batch Loss: 22263.8184\n",
      "Epoch 1/5, Batch 2249/7596, Batch Loss: 255139744.0000\n",
      "Epoch 1/5, Batch 2250/7596, Batch Loss: 1263571.3750\n",
      "Epoch 1/5, Batch 2251/7596, Batch Loss: 1317949.7500\n",
      "Epoch 1/5, Batch 2252/7596, Batch Loss: 34.1140\n",
      "Epoch 1/5, Batch 2253/7596, Batch Loss: 10886353.0000\n",
      "Epoch 1/5, Batch 2254/7596, Batch Loss: 36.6748\n",
      "Epoch 1/5, Batch 2255/7596, Batch Loss: 61982424.0000\n",
      "Epoch 1/5, Batch 2256/7596, Batch Loss: 1072971.5000\n",
      "Epoch 1/5, Batch 2257/7596, Batch Loss: 3625031.5000\n",
      "Epoch 1/5, Batch 2258/7596, Batch Loss: 112606.3984\n",
      "Epoch 1/5, Batch 2259/7596, Batch Loss: 7615459.5000\n",
      "Epoch 1/5, Batch 2260/7596, Batch Loss: 188713.0312\n",
      "Epoch 1/5, Batch 2261/7596, Batch Loss: 1512564.5000\n",
      "Epoch 1/5, Batch 2262/7596, Batch Loss: 2689295.5000\n",
      "Epoch 1/5, Batch 2263/7596, Batch Loss: 244.1715\n",
      "Epoch 1/5, Batch 2264/7596, Batch Loss: 36.3377\n",
      "Epoch 1/5, Batch 2265/7596, Batch Loss: 36.7225\n",
      "Epoch 1/5, Batch 2266/7596, Batch Loss: 3379994.7500\n",
      "Epoch 1/5, Batch 2267/7596, Batch Loss: 208944873472.0000\n",
      "Epoch 1/5, Batch 2268/7596, Batch Loss: 5779788.5000\n",
      "Epoch 1/5, Batch 2269/7596, Batch Loss: 37.2287\n",
      "Epoch 1/5, Batch 2270/7596, Batch Loss: 90.7022\n",
      "Epoch 1/5, Batch 2271/7596, Batch Loss: 1436127.2500\n",
      "Epoch 1/5, Batch 2272/7596, Batch Loss: 2508215.5000\n",
      "Epoch 1/5, Batch 2273/7596, Batch Loss: 1012233.6875\n",
      "Epoch 1/5, Batch 2274/7596, Batch Loss: 22.0705\n",
      "Epoch 1/5, Batch 2275/7596, Batch Loss: 310647.4375\n",
      "Epoch 1/5, Batch 2276/7596, Batch Loss: 6444315.5000\n",
      "Epoch 1/5, Batch 2277/7596, Batch Loss: 2975641.7500\n",
      "Epoch 1/5, Batch 2278/7596, Batch Loss: 31.9429\n",
      "Epoch 1/5, Batch 2279/7596, Batch Loss: 6101713.5000\n",
      "Epoch 1/5, Batch 2280/7596, Batch Loss: 1512658.3750\n",
      "Epoch 1/5, Batch 2281/7596, Batch Loss: 19.1927\n",
      "Epoch 1/5, Batch 2282/7596, Batch Loss: 40.8228\n",
      "Epoch 1/5, Batch 2283/7596, Batch Loss: 2835836.7500\n",
      "Epoch 1/5, Batch 2284/7596, Batch Loss: 2207117.0000\n",
      "Epoch 1/5, Batch 2285/7596, Batch Loss: 36.8099\n",
      "Epoch 1/5, Batch 2286/7596, Batch Loss: 499291.2812\n",
      "Epoch 1/5, Batch 2287/7596, Batch Loss: 1244874.8750\n",
      "Epoch 1/5, Batch 2288/7596, Batch Loss: 4131457.5000\n",
      "Epoch 1/5, Batch 2289/7596, Batch Loss: 3071879.5000\n",
      "Epoch 1/5, Batch 2290/7596, Batch Loss: 415515.2500\n",
      "Epoch 1/5, Batch 2291/7596, Batch Loss: 4118819.5000\n",
      "Epoch 1/5, Batch 2292/7596, Batch Loss: 2759999.7500\n",
      "Epoch 1/5, Batch 2293/7596, Batch Loss: 2865368.7500\n",
      "Epoch 1/5, Batch 2294/7596, Batch Loss: 5784687.5000\n",
      "Epoch 1/5, Batch 2295/7596, Batch Loss: 11899803.0000\n",
      "Epoch 1/5, Batch 2296/7596, Batch Loss: 37.1095\n",
      "Epoch 1/5, Batch 2297/7596, Batch Loss: 84726592.0000\n",
      "Epoch 1/5, Batch 2298/7596, Batch Loss: 1839802.5000\n",
      "Epoch 1/5, Batch 2299/7596, Batch Loss: 2959558.2500\n",
      "Epoch 1/5, Batch 2300/7596, Batch Loss: 20.7151\n",
      "Epoch 1/5, Batch 2301/7596, Batch Loss: 6517107.0000\n",
      "Epoch 1/5, Batch 2302/7596, Batch Loss: 3102123.0000\n",
      "Epoch 1/5, Batch 2303/7596, Batch Loss: 2200593.5000\n",
      "Epoch 1/5, Batch 2304/7596, Batch Loss: 692770.3125\n",
      "Epoch 1/5, Batch 2305/7596, Batch Loss: 986415.3125\n",
      "Epoch 1/5, Batch 2306/7596, Batch Loss: 90808.1406\n",
      "Epoch 1/5, Batch 2307/7596, Batch Loss: 7189.5796\n",
      "Epoch 1/5, Batch 2308/7596, Batch Loss: 29098.8535\n",
      "Epoch 1/5, Batch 2309/7596, Batch Loss: 26856.9648\n",
      "Epoch 1/5, Batch 2310/7596, Batch Loss: 694.4990\n",
      "Epoch 1/5, Batch 2311/7596, Batch Loss: 38.6389\n",
      "Epoch 1/5, Batch 2312/7596, Batch Loss: 547069.7500\n",
      "Epoch 1/5, Batch 2313/7596, Batch Loss: 1412412.6250\n",
      "Epoch 1/5, Batch 2314/7596, Batch Loss: 3358748.0000\n",
      "Epoch 1/5, Batch 2315/7596, Batch Loss: 1008.7123\n",
      "Epoch 1/5, Batch 2316/7596, Batch Loss: 49225.8945\n",
      "Epoch 1/5, Batch 2317/7596, Batch Loss: 1564.8677\n",
      "Epoch 1/5, Batch 2318/7596, Batch Loss: 1035.8394\n",
      "Epoch 1/5, Batch 2319/7596, Batch Loss: 1926.2417\n",
      "Epoch 1/5, Batch 2320/7596, Batch Loss: 117796.0312\n",
      "Epoch 1/5, Batch 2321/7596, Batch Loss: 7403.5029\n",
      "Epoch 1/5, Batch 2322/7596, Batch Loss: 2227401.5000\n",
      "Epoch 1/5, Batch 2323/7596, Batch Loss: 1525353.1250\n",
      "Epoch 1/5, Batch 2324/7596, Batch Loss: 617710.9375\n",
      "Epoch 1/5, Batch 2325/7596, Batch Loss: 45208656.0000\n",
      "Epoch 1/5, Batch 2326/7596, Batch Loss: 783579.6875\n",
      "Epoch 1/5, Batch 2327/7596, Batch Loss: 142575.2188\n",
      "Epoch 1/5, Batch 2328/7596, Batch Loss: 21.1016\n",
      "Epoch 1/5, Batch 2329/7596, Batch Loss: 36.2928\n",
      "Epoch 1/5, Batch 2330/7596, Batch Loss: 832538.8750\n",
      "Epoch 1/5, Batch 2331/7596, Batch Loss: 33.5393\n",
      "Epoch 1/5, Batch 2332/7596, Batch Loss: 25.7958\n",
      "Epoch 1/5, Batch 2333/7596, Batch Loss: 84.3547\n",
      "Epoch 1/5, Batch 2334/7596, Batch Loss: 70884.7969\n",
      "Epoch 1/5, Batch 2335/7596, Batch Loss: 20392.3555\n",
      "Epoch 1/5, Batch 2336/7596, Batch Loss: 27883.2109\n",
      "Epoch 1/5, Batch 2337/7596, Batch Loss: 206370.9375\n",
      "Epoch 1/5, Batch 2338/7596, Batch Loss: 197367.7500\n",
      "Epoch 1/5, Batch 2339/7596, Batch Loss: 31.9659\n",
      "Epoch 1/5, Batch 2340/7596, Batch Loss: 33.2310\n",
      "Epoch 1/5, Batch 2341/7596, Batch Loss: 43.3600\n",
      "Epoch 1/5, Batch 2342/7596, Batch Loss: 14.5863\n",
      "Epoch 1/5, Batch 2343/7596, Batch Loss: 26961356.0000\n",
      "Epoch 1/5, Batch 2344/7596, Batch Loss: 82587.5234\n",
      "Epoch 1/5, Batch 2345/7596, Batch Loss: 14223.0479\n",
      "Epoch 1/5, Batch 2346/7596, Batch Loss: 9819.8945\n",
      "Epoch 1/5, Batch 2347/7596, Batch Loss: 15481.0957\n",
      "Epoch 1/5, Batch 2348/7596, Batch Loss: 165561.5156\n",
      "Epoch 1/5, Batch 2349/7596, Batch Loss: 65555.5391\n",
      "Epoch 1/5, Batch 2350/7596, Batch Loss: 47.6597\n",
      "Epoch 1/5, Batch 2351/7596, Batch Loss: 170209.9062\n",
      "Epoch 1/5, Batch 2352/7596, Batch Loss: 26950.0195\n",
      "Epoch 1/5, Batch 2353/7596, Batch Loss: 222800.8281\n",
      "Epoch 1/5, Batch 2354/7596, Batch Loss: 26662.0137\n",
      "Epoch 1/5, Batch 2355/7596, Batch Loss: 17301.2949\n",
      "Epoch 1/5, Batch 2356/7596, Batch Loss: 83033.0469\n",
      "Epoch 1/5, Batch 2357/7596, Batch Loss: 23533.8262\n",
      "Epoch 1/5, Batch 2358/7596, Batch Loss: 591.4235\n",
      "Epoch 1/5, Batch 2359/7596, Batch Loss: 93520.2500\n",
      "Epoch 1/5, Batch 2360/7596, Batch Loss: 30.6757\n",
      "Epoch 1/5, Batch 2361/7596, Batch Loss: 62.1576\n",
      "Epoch 1/5, Batch 2362/7596, Batch Loss: 86079.2812\n",
      "Epoch 1/5, Batch 2363/7596, Batch Loss: 68180.1641\n",
      "Epoch 1/5, Batch 2364/7596, Batch Loss: 62094.0859\n",
      "Epoch 1/5, Batch 2365/7596, Batch Loss: 300675328.0000\n",
      "Epoch 1/5, Batch 2366/7596, Batch Loss: 22919.0312\n",
      "Epoch 1/5, Batch 2367/7596, Batch Loss: 22.6168\n",
      "Epoch 1/5, Batch 2368/7596, Batch Loss: 41.7197\n",
      "Epoch 1/5, Batch 2369/7596, Batch Loss: 189099.0781\n",
      "Epoch 1/5, Batch 2370/7596, Batch Loss: 160406.4688\n",
      "Epoch 1/5, Batch 2371/7596, Batch Loss: 2557188.5000\n",
      "Epoch 1/5, Batch 2372/7596, Batch Loss: 177004.8281\n",
      "Epoch 1/5, Batch 2373/7596, Batch Loss: 235098.7188\n",
      "Epoch 1/5, Batch 2374/7596, Batch Loss: 2372.2441\n",
      "Epoch 1/5, Batch 2375/7596, Batch Loss: 32789.3555\n",
      "Epoch 1/5, Batch 2376/7596, Batch Loss: 289941.3438\n",
      "Epoch 1/5, Batch 2377/7596, Batch Loss: 12890.2197\n",
      "Epoch 1/5, Batch 2378/7596, Batch Loss: 23.6337\n",
      "Epoch 1/5, Batch 2379/7596, Batch Loss: 29048.7773\n",
      "Epoch 1/5, Batch 2380/7596, Batch Loss: 490261.4062\n",
      "Epoch 1/5, Batch 2381/7596, Batch Loss: 4598862.0000\n",
      "Epoch 1/5, Batch 2382/7596, Batch Loss: 641746.4375\n",
      "Epoch 1/5, Batch 2383/7596, Batch Loss: 704471.7500\n",
      "Epoch 1/5, Batch 2384/7596, Batch Loss: 54156.7969\n",
      "Epoch 1/5, Batch 2385/7596, Batch Loss: 61971.8398\n",
      "Epoch 1/5, Batch 2386/7596, Batch Loss: 32.4557\n",
      "Epoch 1/5, Batch 2387/7596, Batch Loss: 37224.1875\n",
      "Epoch 1/5, Batch 2388/7596, Batch Loss: 14.0748\n",
      "Epoch 1/5, Batch 2389/7596, Batch Loss: 242307.1562\n",
      "Epoch 1/5, Batch 2390/7596, Batch Loss: 9189.2441\n",
      "Epoch 1/5, Batch 2391/7596, Batch Loss: 126467.4688\n",
      "Epoch 1/5, Batch 2392/7596, Batch Loss: 9968101.0000\n",
      "Epoch 1/5, Batch 2393/7596, Batch Loss: 26080.8379\n",
      "Epoch 1/5, Batch 2394/7596, Batch Loss: 158169.9688\n",
      "Epoch 1/5, Batch 2395/7596, Batch Loss: 188030.8438\n",
      "Epoch 1/5, Batch 2396/7596, Batch Loss: 2401216.2500\n",
      "Epoch 1/5, Batch 2397/7596, Batch Loss: 35.2526\n",
      "Epoch 1/5, Batch 2398/7596, Batch Loss: 39811720.0000\n",
      "Epoch 1/5, Batch 2399/7596, Batch Loss: 30.2362\n",
      "Epoch 1/5, Batch 2400/7596, Batch Loss: 158587776.0000\n",
      "Epoch 1/5, Batch 2401/7596, Batch Loss: 44267612.0000\n",
      "Epoch 1/5, Batch 2402/7596, Batch Loss: 3304494.7500\n",
      "Epoch 1/5, Batch 2403/7596, Batch Loss: 96964.5703\n",
      "Epoch 1/5, Batch 2404/7596, Batch Loss: 1796719.5000\n",
      "Epoch 1/5, Batch 2405/7596, Batch Loss: 448917.9688\n",
      "Epoch 1/5, Batch 2406/7596, Batch Loss: 15764.3408\n",
      "Epoch 1/5, Batch 2407/7596, Batch Loss: 2062429.5000\n",
      "Epoch 1/5, Batch 2408/7596, Batch Loss: 884002.3750\n",
      "Epoch 1/5, Batch 2409/7596, Batch Loss: 647163.5000\n",
      "Epoch 1/5, Batch 2410/7596, Batch Loss: 143366.9062\n",
      "Epoch 1/5, Batch 2411/7596, Batch Loss: 675025.8750\n",
      "Epoch 1/5, Batch 2412/7596, Batch Loss: 37.0269\n",
      "Epoch 1/5, Batch 2413/7596, Batch Loss: 806672.0000\n",
      "Epoch 1/5, Batch 2414/7596, Batch Loss: 74777.4375\n",
      "Epoch 1/5, Batch 2415/7596, Batch Loss: 13.2573\n",
      "Epoch 1/5, Batch 2416/7596, Batch Loss: 37.6521\n",
      "Epoch 1/5, Batch 2417/7596, Batch Loss: 1147379.1250\n",
      "Epoch 1/5, Batch 2418/7596, Batch Loss: 34.6626\n",
      "Epoch 1/5, Batch 2419/7596, Batch Loss: 1186091.3750\n",
      "Epoch 1/5, Batch 2420/7596, Batch Loss: 289759.0312\n",
      "Epoch 1/5, Batch 2421/7596, Batch Loss: 3763056.0000\n",
      "Epoch 1/5, Batch 2422/7596, Batch Loss: 96431.9062\n",
      "Epoch 1/5, Batch 2423/7596, Batch Loss: 567334.0000\n",
      "Epoch 1/5, Batch 2424/7596, Batch Loss: 36.4107\n",
      "Epoch 1/5, Batch 2425/7596, Batch Loss: 36.8712\n",
      "Epoch 1/5, Batch 2426/7596, Batch Loss: 28.8332\n",
      "Epoch 1/5, Batch 2427/7596, Batch Loss: 36.1065\n",
      "Epoch 1/5, Batch 2428/7596, Batch Loss: 6266961.5000\n",
      "Epoch 1/5, Batch 2429/7596, Batch Loss: 2417271.5000\n",
      "Epoch 1/5, Batch 2430/7596, Batch Loss: 32.7425\n",
      "Epoch 1/5, Batch 2431/7596, Batch Loss: 103896.5156\n",
      "Epoch 1/5, Batch 2432/7596, Batch Loss: 199141.2500\n",
      "Epoch 1/5, Batch 2433/7596, Batch Loss: 9031.4883\n",
      "Epoch 1/5, Batch 2434/7596, Batch Loss: 18.1997\n",
      "Epoch 1/5, Batch 2435/7596, Batch Loss: 1614383.0000\n",
      "Epoch 1/5, Batch 2436/7596, Batch Loss: 284682.3125\n",
      "Epoch 1/5, Batch 2437/7596, Batch Loss: 145653.6719\n",
      "Epoch 1/5, Batch 2438/7596, Batch Loss: 91900.1016\n",
      "Epoch 1/5, Batch 2439/7596, Batch Loss: 169825.0781\n",
      "Epoch 1/5, Batch 2440/7596, Batch Loss: 913.2251\n",
      "Epoch 1/5, Batch 2441/7596, Batch Loss: 390959.8438\n",
      "Epoch 1/5, Batch 2442/7596, Batch Loss: 166652.7031\n",
      "Epoch 1/5, Batch 2443/7596, Batch Loss: 122333.2578\n",
      "Epoch 1/5, Batch 2444/7596, Batch Loss: 956947.1875\n",
      "Epoch 1/5, Batch 2445/7596, Batch Loss: 570218.0625\n",
      "Epoch 1/5, Batch 2446/7596, Batch Loss: 822606.3125\n",
      "Epoch 1/5, Batch 2447/7596, Batch Loss: 1038666.1875\n",
      "Epoch 1/5, Batch 2448/7596, Batch Loss: 205878.7500\n",
      "Epoch 1/5, Batch 2449/7596, Batch Loss: 26681.6035\n",
      "Epoch 1/5, Batch 2450/7596, Batch Loss: 35.5715\n",
      "Epoch 1/5, Batch 2451/7596, Batch Loss: 626780.0000\n",
      "Epoch 1/5, Batch 2452/7596, Batch Loss: 9140093.0000\n",
      "Epoch 1/5, Batch 2453/7596, Batch Loss: 208216.3906\n",
      "Epoch 1/5, Batch 2454/7596, Batch Loss: 1909292.5000\n",
      "Epoch 1/5, Batch 2455/7596, Batch Loss: 125793.7344\n",
      "Epoch 1/5, Batch 2456/7596, Batch Loss: 324551.3750\n",
      "Epoch 1/5, Batch 2457/7596, Batch Loss: 33.1262\n",
      "Epoch 1/5, Batch 2458/7596, Batch Loss: 19.6328\n",
      "Epoch 1/5, Batch 2459/7596, Batch Loss: 4508543.0000\n",
      "Epoch 1/5, Batch 2460/7596, Batch Loss: 26.4107\n",
      "Epoch 1/5, Batch 2461/7596, Batch Loss: 232628.5625\n",
      "Epoch 1/5, Batch 2462/7596, Batch Loss: 2196588.0000\n",
      "Epoch 1/5, Batch 2463/7596, Batch Loss: 2847337.0000\n",
      "Epoch 1/5, Batch 2464/7596, Batch Loss: 24.5705\n",
      "Epoch 1/5, Batch 2465/7596, Batch Loss: 22.4648\n",
      "Epoch 1/5, Batch 2466/7596, Batch Loss: 192623.6250\n",
      "Epoch 1/5, Batch 2467/7596, Batch Loss: 96615.5234\n",
      "Epoch 1/5, Batch 2468/7596, Batch Loss: 42714.4727\n",
      "Epoch 1/5, Batch 2469/7596, Batch Loss: 106966.6250\n",
      "Epoch 1/5, Batch 2470/7596, Batch Loss: 174417.9375\n",
      "Epoch 1/5, Batch 2471/7596, Batch Loss: 161499.1719\n",
      "Epoch 1/5, Batch 2472/7596, Batch Loss: 240864.4531\n",
      "Epoch 1/5, Batch 2473/7596, Batch Loss: 23.5950\n",
      "Epoch 1/5, Batch 2474/7596, Batch Loss: 15224.8008\n",
      "Epoch 1/5, Batch 2475/7596, Batch Loss: 20625.8105\n",
      "Epoch 1/5, Batch 2476/7596, Batch Loss: 121804.0859\n",
      "Epoch 1/5, Batch 2477/7596, Batch Loss: 36640.9258\n",
      "Epoch 1/5, Batch 2478/7596, Batch Loss: 33.3501\n",
      "Epoch 1/5, Batch 2479/7596, Batch Loss: 246094.4375\n",
      "Epoch 1/5, Batch 2480/7596, Batch Loss: 357342.2188\n",
      "Epoch 1/5, Batch 2481/7596, Batch Loss: 614509.3750\n",
      "Epoch 1/5, Batch 2482/7596, Batch Loss: 226420.5625\n",
      "Epoch 1/5, Batch 2483/7596, Batch Loss: 660633.0000\n",
      "Epoch 1/5, Batch 2484/7596, Batch Loss: 34.9309\n",
      "Epoch 1/5, Batch 2485/7596, Batch Loss: 340767.1562\n",
      "Epoch 1/5, Batch 2486/7596, Batch Loss: 38.0897\n",
      "Epoch 1/5, Batch 2487/7596, Batch Loss: 32.2890\n",
      "Epoch 1/5, Batch 2488/7596, Batch Loss: 37.3903\n",
      "Epoch 1/5, Batch 2489/7596, Batch Loss: 41.5319\n",
      "Epoch 1/5, Batch 2490/7596, Batch Loss: 13878.6396\n",
      "Epoch 1/5, Batch 2491/7596, Batch Loss: 72769.7969\n",
      "Epoch 1/5, Batch 2492/7596, Batch Loss: 71560.9609\n",
      "Epoch 1/5, Batch 2493/7596, Batch Loss: 3014.9905\n",
      "Epoch 1/5, Batch 2494/7596, Batch Loss: 191224.1094\n",
      "Epoch 1/5, Batch 2495/7596, Batch Loss: 213762.0000\n",
      "Epoch 1/5, Batch 2496/7596, Batch Loss: 621525.8750\n",
      "Epoch 1/5, Batch 2497/7596, Batch Loss: 3120.8870\n",
      "Epoch 1/5, Batch 2498/7596, Batch Loss: 231683.7188\n",
      "Epoch 1/5, Batch 2499/7596, Batch Loss: 91915.8750\n",
      "Epoch 1/5, Batch 2500/7596, Batch Loss: 44788.8125\n",
      "Epoch 1/5, Batch 2501/7596, Batch Loss: 70993.4219\n",
      "Epoch 1/5, Batch 2502/7596, Batch Loss: 38.6134\n",
      "Epoch 1/5, Batch 2503/7596, Batch Loss: 248150.3438\n",
      "Epoch 1/5, Batch 2504/7596, Batch Loss: 462334.4688\n",
      "Epoch 1/5, Batch 2505/7596, Batch Loss: 24.0248\n",
      "Epoch 1/5, Batch 2506/7596, Batch Loss: 432625.3750\n",
      "Epoch 1/5, Batch 2507/7596, Batch Loss: 318314.8125\n",
      "Epoch 1/5, Batch 2508/7596, Batch Loss: 199258.5938\n",
      "Epoch 1/5, Batch 2509/7596, Batch Loss: 37.2401\n",
      "Epoch 1/5, Batch 2510/7596, Batch Loss: 38.9584\n",
      "Epoch 1/5, Batch 2511/7596, Batch Loss: 5102505.5000\n",
      "Epoch 1/5, Batch 2512/7596, Batch Loss: 35.4517\n",
      "Epoch 1/5, Batch 2513/7596, Batch Loss: 15853717.0000\n",
      "Epoch 1/5, Batch 2514/7596, Batch Loss: 43.4493\n",
      "Epoch 1/5, Batch 2515/7596, Batch Loss: 203534.8594\n",
      "Epoch 1/5, Batch 2516/7596, Batch Loss: 358387.0000\n",
      "Epoch 1/5, Batch 2517/7596, Batch Loss: 36.7649\n",
      "Epoch 1/5, Batch 2518/7596, Batch Loss: 174901.2031\n",
      "Epoch 1/5, Batch 2519/7596, Batch Loss: 1082471.7500\n",
      "Epoch 1/5, Batch 2520/7596, Batch Loss: 2578667.5000\n",
      "Epoch 1/5, Batch 2521/7596, Batch Loss: 24546088.0000\n",
      "Epoch 1/5, Batch 2522/7596, Batch Loss: 892828.1250\n",
      "Epoch 1/5, Batch 2523/7596, Batch Loss: 2261135.0000\n",
      "Epoch 1/5, Batch 2524/7596, Batch Loss: 32151704.0000\n",
      "Epoch 1/5, Batch 2525/7596, Batch Loss: 111771440.0000\n",
      "Epoch 1/5, Batch 2526/7596, Batch Loss: 9530690.0000\n",
      "Epoch 1/5, Batch 2527/7596, Batch Loss: 187139.4844\n",
      "Epoch 1/5, Batch 2528/7596, Batch Loss: 36.2222\n",
      "Epoch 1/5, Batch 2529/7596, Batch Loss: 647907.1250\n",
      "Epoch 1/5, Batch 2530/7596, Batch Loss: 12.4028\n",
      "Epoch 1/5, Batch 2531/7596, Batch Loss: 1102219.8750\n",
      "Epoch 1/5, Batch 2532/7596, Batch Loss: 418897.5312\n",
      "Epoch 1/5, Batch 2533/7596, Batch Loss: 736454.1875\n",
      "Epoch 1/5, Batch 2534/7596, Batch Loss: 957464.6875\n",
      "Epoch 1/5, Batch 2535/7596, Batch Loss: 451918.8438\n",
      "Epoch 1/5, Batch 2536/7596, Batch Loss: 1269991.7500\n",
      "Epoch 1/5, Batch 2537/7596, Batch Loss: 37.0082\n",
      "Epoch 1/5, Batch 2538/7596, Batch Loss: 984681.1250\n",
      "Epoch 1/5, Batch 2539/7596, Batch Loss: 252794.3594\n",
      "Epoch 1/5, Batch 2540/7596, Batch Loss: 454499.7188\n",
      "Epoch 1/5, Batch 2541/7596, Batch Loss: 36.3121\n",
      "Epoch 1/5, Batch 2542/7596, Batch Loss: 2721965.0000\n",
      "Epoch 1/5, Batch 2543/7596, Batch Loss: 2016502.6250\n",
      "Epoch 1/5, Batch 2544/7596, Batch Loss: 39.7673\n",
      "Epoch 1/5, Batch 2545/7596, Batch Loss: 4714224.0000\n",
      "Epoch 1/5, Batch 2546/7596, Batch Loss: 875034.1250\n",
      "Epoch 1/5, Batch 2547/7596, Batch Loss: 34.8213\n",
      "Epoch 1/5, Batch 2548/7596, Batch Loss: 30797898.0000\n",
      "Epoch 1/5, Batch 2549/7596, Batch Loss: 11240715.0000\n",
      "Epoch 1/5, Batch 2550/7596, Batch Loss: 33.0392\n",
      "Epoch 1/5, Batch 2551/7596, Batch Loss: 14315831.0000\n",
      "Epoch 1/5, Batch 2552/7596, Batch Loss: 44.0327\n",
      "Epoch 1/5, Batch 2553/7596, Batch Loss: 36.9382\n",
      "Epoch 1/5, Batch 2554/7596, Batch Loss: 36.2819\n",
      "Epoch 1/5, Batch 2555/7596, Batch Loss: 599720768.0000\n",
      "Epoch 1/5, Batch 2556/7596, Batch Loss: 2436434.5000\n",
      "Epoch 1/5, Batch 2557/7596, Batch Loss: 3250953472.0000\n",
      "Epoch 1/5, Batch 2558/7596, Batch Loss: 28132478.0000\n",
      "Epoch 1/5, Batch 2559/7596, Batch Loss: 5576754.5000\n",
      "Epoch 1/5, Batch 2560/7596, Batch Loss: 43101788.0000\n",
      "Epoch 1/5, Batch 2561/7596, Batch Loss: 70726096.0000\n",
      "Epoch 1/5, Batch 2562/7596, Batch Loss: 33779820.0000\n",
      "Epoch 1/5, Batch 2563/7596, Batch Loss: 73744112.0000\n",
      "Epoch 1/5, Batch 2564/7596, Batch Loss: 2170803968.0000\n",
      "Epoch 1/5, Batch 2565/7596, Batch Loss: 279955936.0000\n",
      "Epoch 1/5, Batch 2566/7596, Batch Loss: 165437376.0000\n",
      "Epoch 1/5, Batch 2567/7596, Batch Loss: 10370.9561\n",
      "Epoch 1/5, Batch 2568/7596, Batch Loss: 9970.6270\n",
      "Epoch 1/5, Batch 2569/7596, Batch Loss: 24393578.0000\n",
      "Epoch 1/5, Batch 2570/7596, Batch Loss: 6688039.0000\n",
      "Epoch 1/5, Batch 2571/7596, Batch Loss: 3539446.7500\n",
      "Epoch 1/5, Batch 2572/7596, Batch Loss: 1418602.0000\n",
      "Epoch 1/5, Batch 2573/7596, Batch Loss: 934229.3125\n",
      "Epoch 1/5, Batch 2574/7596, Batch Loss: 123.3659\n",
      "Epoch 1/5, Batch 2575/7596, Batch Loss: 861505.8750\n",
      "Epoch 1/5, Batch 2576/7596, Batch Loss: 1070320.0000\n",
      "Epoch 1/5, Batch 2577/7596, Batch Loss: 1890929.2500\n",
      "Epoch 1/5, Batch 2578/7596, Batch Loss: 34804936.0000\n",
      "Epoch 1/5, Batch 2579/7596, Batch Loss: 406776.3750\n",
      "Epoch 1/5, Batch 2580/7596, Batch Loss: 159961.3438\n",
      "Epoch 1/5, Batch 2581/7596, Batch Loss: 25514.5977\n",
      "Epoch 1/5, Batch 2582/7596, Batch Loss: 501334.0312\n",
      "Epoch 1/5, Batch 2583/7596, Batch Loss: 32033.4766\n",
      "Epoch 1/5, Batch 2584/7596, Batch Loss: 4771190.0000\n",
      "Epoch 1/5, Batch 2585/7596, Batch Loss: 6869478.0000\n",
      "Epoch 1/5, Batch 2586/7596, Batch Loss: 288036.8438\n",
      "Epoch 1/5, Batch 2587/7596, Batch Loss: 31558.5703\n",
      "Epoch 1/5, Batch 2588/7596, Batch Loss: 36272072.0000\n",
      "Epoch 1/5, Batch 2589/7596, Batch Loss: 316.6578\n",
      "Epoch 1/5, Batch 2590/7596, Batch Loss: 3648172.0000\n",
      "Epoch 1/5, Batch 2591/7596, Batch Loss: 3477722.5000\n",
      "Epoch 1/5, Batch 2592/7596, Batch Loss: 226443.3906\n",
      "Epoch 1/5, Batch 2593/7596, Batch Loss: 512314.8438\n",
      "Epoch 1/5, Batch 2594/7596, Batch Loss: 1311212.0000\n",
      "Epoch 1/5, Batch 2595/7596, Batch Loss: 3810570.5000\n",
      "Epoch 1/5, Batch 2596/7596, Batch Loss: 1166569.6250\n",
      "Epoch 1/5, Batch 2597/7596, Batch Loss: 91580.4062\n",
      "Epoch 1/5, Batch 2598/7596, Batch Loss: 218423.1250\n",
      "Epoch 1/5, Batch 2599/7596, Batch Loss: 7662.9048\n",
      "Epoch 1/5, Batch 2600/7596, Batch Loss: 6388859.5000\n",
      "Epoch 1/5, Batch 2601/7596, Batch Loss: 4413722.0000\n",
      "Epoch 1/5, Batch 2602/7596, Batch Loss: 41.3096\n",
      "Epoch 1/5, Batch 2603/7596, Batch Loss: 9369570.0000\n",
      "Epoch 1/5, Batch 2604/7596, Batch Loss: 1428608.2500\n",
      "Epoch 1/5, Batch 2605/7596, Batch Loss: 193879.9219\n",
      "Epoch 1/5, Batch 2606/7596, Batch Loss: 494.9872\n",
      "Epoch 1/5, Batch 2607/7596, Batch Loss: 49.9443\n",
      "Epoch 1/5, Batch 2608/7596, Batch Loss: 34.8520\n",
      "Epoch 1/5, Batch 2609/7596, Batch Loss: 170.9359\n",
      "Epoch 1/5, Batch 2610/7596, Batch Loss: 36.8437\n",
      "Epoch 1/5, Batch 2611/7596, Batch Loss: 18688152.0000\n",
      "Epoch 1/5, Batch 2612/7596, Batch Loss: 253314.6562\n",
      "Epoch 1/5, Batch 2613/7596, Batch Loss: 8700859392.0000\n",
      "Epoch 1/5, Batch 2614/7596, Batch Loss: 1035869.6250\n",
      "Epoch 1/5, Batch 2615/7596, Batch Loss: 1128131.0000\n",
      "Epoch 1/5, Batch 2616/7596, Batch Loss: 7792633.5000\n",
      "Epoch 1/5, Batch 2617/7596, Batch Loss: 4459.0034\n",
      "Epoch 1/5, Batch 2618/7596, Batch Loss: 27566944.0000\n",
      "Epoch 1/5, Batch 2619/7596, Batch Loss: 33.6333\n",
      "Epoch 1/5, Batch 2620/7596, Batch Loss: 110.7098\n",
      "Epoch 1/5, Batch 2621/7596, Batch Loss: 34579888.0000\n",
      "Epoch 1/5, Batch 2622/7596, Batch Loss: 235310400.0000\n",
      "Epoch 1/5, Batch 2623/7596, Batch Loss: 35.7857\n",
      "Epoch 1/5, Batch 2624/7596, Batch Loss: 947304448.0000\n",
      "Epoch 1/5, Batch 2625/7596, Batch Loss: 15.1540\n",
      "Epoch 1/5, Batch 2626/7596, Batch Loss: 35.4415\n",
      "Epoch 1/5, Batch 2627/7596, Batch Loss: 1751996.2500\n",
      "Epoch 1/5, Batch 2628/7596, Batch Loss: 130289696.0000\n",
      "Epoch 1/5, Batch 2629/7596, Batch Loss: 5332.9873\n",
      "Epoch 1/5, Batch 2630/7596, Batch Loss: 1444575744.0000\n",
      "Epoch 1/5, Batch 2631/7596, Batch Loss: 35.7264\n",
      "Epoch 1/5, Batch 2632/7596, Batch Loss: 478065344.0000\n",
      "Epoch 1/5, Batch 2633/7596, Batch Loss: 4961813.0000\n",
      "Epoch 1/5, Batch 2634/7596, Batch Loss: 33.7325\n",
      "Epoch 1/5, Batch 2635/7596, Batch Loss: 3131219.2500\n",
      "Epoch 1/5, Batch 2636/7596, Batch Loss: 21265132.0000\n",
      "Epoch 1/5, Batch 2637/7596, Batch Loss: 2620165.7500\n",
      "Epoch 1/5, Batch 2638/7596, Batch Loss: 9922992.0000\n",
      "Epoch 1/5, Batch 2639/7596, Batch Loss: 3144889.2500\n",
      "Epoch 1/5, Batch 2640/7596, Batch Loss: 3020796.0000\n",
      "Epoch 1/5, Batch 2641/7596, Batch Loss: 46917300.0000\n",
      "Epoch 1/5, Batch 2642/7596, Batch Loss: 24926376.0000\n",
      "Epoch 1/5, Batch 2643/7596, Batch Loss: 46567530496.0000\n",
      "Epoch 1/5, Batch 2644/7596, Batch Loss: 44292236.0000\n",
      "Epoch 1/5, Batch 2645/7596, Batch Loss: 4198931.0000\n",
      "Epoch 1/5, Batch 2646/7596, Batch Loss: 41000.8633\n",
      "Epoch 1/5, Batch 2647/7596, Batch Loss: 1439037.7500\n",
      "Epoch 1/5, Batch 2648/7596, Batch Loss: 271400.0625\n",
      "Epoch 1/5, Batch 2649/7596, Batch Loss: 330861.9688\n",
      "Epoch 1/5, Batch 2650/7596, Batch Loss: 6291776.0000\n",
      "Epoch 1/5, Batch 2651/7596, Batch Loss: 842063.8750\n",
      "Epoch 1/5, Batch 2652/7596, Batch Loss: 1010244.9375\n",
      "Epoch 1/5, Batch 2653/7596, Batch Loss: 101021440.0000\n",
      "Epoch 1/5, Batch 2654/7596, Batch Loss: 2377367.2500\n",
      "Epoch 1/5, Batch 2655/7596, Batch Loss: 3074487.0000\n",
      "Epoch 1/5, Batch 2656/7596, Batch Loss: 171605632.0000\n",
      "Epoch 1/5, Batch 2657/7596, Batch Loss: 172783184.0000\n",
      "Epoch 1/5, Batch 2658/7596, Batch Loss: 2343966.2500\n",
      "Epoch 1/5, Batch 2659/7596, Batch Loss: 61339832.0000\n",
      "Epoch 1/5, Batch 2660/7596, Batch Loss: 124217.0312\n",
      "Epoch 1/5, Batch 2661/7596, Batch Loss: 10360175.0000\n",
      "Epoch 1/5, Batch 2662/7596, Batch Loss: 5416140.0000\n",
      "Epoch 1/5, Batch 2663/7596, Batch Loss: 1275889280.0000\n",
      "Epoch 1/5, Batch 2664/7596, Batch Loss: 56157688.0000\n",
      "Epoch 1/5, Batch 2665/7596, Batch Loss: 25905182.0000\n",
      "Epoch 1/5, Batch 2666/7596, Batch Loss: 460538.0312\n",
      "Epoch 1/5, Batch 2667/7596, Batch Loss: 3225132.2500\n",
      "Epoch 1/5, Batch 2668/7596, Batch Loss: 10911346.0000\n",
      "Epoch 1/5, Batch 2669/7596, Batch Loss: 7287067.0000\n",
      "Epoch 1/5, Batch 2670/7596, Batch Loss: 14442099.0000\n",
      "Epoch 1/5, Batch 2671/7596, Batch Loss: 79537576.0000\n",
      "Epoch 1/5, Batch 2672/7596, Batch Loss: 440863328.0000\n",
      "Epoch 1/5, Batch 2673/7596, Batch Loss: 19165172.0000\n",
      "Epoch 1/5, Batch 2674/7596, Batch Loss: 148699600.0000\n",
      "Epoch 1/5, Batch 2675/7596, Batch Loss: 2673074432.0000\n",
      "Epoch 1/5, Batch 2676/7596, Batch Loss: 1620993.3750\n",
      "Epoch 1/5, Batch 2677/7596, Batch Loss: 82717320.0000\n",
      "Epoch 1/5, Batch 2678/7596, Batch Loss: 492825.5625\n",
      "Epoch 1/5, Batch 2679/7596, Batch Loss: 620175.8125\n",
      "Epoch 1/5, Batch 2680/7596, Batch Loss: 274990.1250\n",
      "Epoch 1/5, Batch 2681/7596, Batch Loss: 568976.2500\n",
      "Epoch 1/5, Batch 2682/7596, Batch Loss: 1295.3201\n",
      "Epoch 1/5, Batch 2683/7596, Batch Loss: 8134386.0000\n",
      "Epoch 1/5, Batch 2684/7596, Batch Loss: 39.8770\n",
      "Epoch 1/5, Batch 2685/7596, Batch Loss: 1044922.9375\n",
      "Epoch 1/5, Batch 2686/7596, Batch Loss: 151534.1719\n",
      "Epoch 1/5, Batch 2687/7596, Batch Loss: 2839439.2500\n",
      "Epoch 1/5, Batch 2688/7596, Batch Loss: 1876321.2500\n",
      "Epoch 1/5, Batch 2689/7596, Batch Loss: 4152967.5000\n",
      "Epoch 1/5, Batch 2690/7596, Batch Loss: 43948764.0000\n",
      "Epoch 1/5, Batch 2691/7596, Batch Loss: 300529.7812\n",
      "Epoch 1/5, Batch 2692/7596, Batch Loss: 619718.2500\n",
      "Epoch 1/5, Batch 2693/7596, Batch Loss: 289538.0000\n",
      "Epoch 1/5, Batch 2694/7596, Batch Loss: 1104030.7500\n",
      "Epoch 1/5, Batch 2695/7596, Batch Loss: 96044.0156\n",
      "Epoch 1/5, Batch 2696/7596, Batch Loss: 244268.8438\n",
      "Epoch 1/5, Batch 2697/7596, Batch Loss: 4836.6011\n",
      "Epoch 1/5, Batch 2698/7596, Batch Loss: 1508481.8750\n",
      "Epoch 1/5, Batch 2699/7596, Batch Loss: 121837.6953\n",
      "Epoch 1/5, Batch 2700/7596, Batch Loss: 512047.8438\n",
      "Epoch 1/5, Batch 2701/7596, Batch Loss: 260285.1406\n",
      "Epoch 1/5, Batch 2702/7596, Batch Loss: 20234.7949\n",
      "Epoch 1/5, Batch 2703/7596, Batch Loss: 36088.1719\n",
      "Epoch 1/5, Batch 2704/7596, Batch Loss: 142221.9062\n",
      "Epoch 1/5, Batch 2705/7596, Batch Loss: 266.8395\n",
      "Epoch 1/5, Batch 2706/7596, Batch Loss: 118542.4922\n",
      "Epoch 1/5, Batch 2707/7596, Batch Loss: 228177.1094\n",
      "Epoch 1/5, Batch 2708/7596, Batch Loss: 38.0926\n",
      "Epoch 1/5, Batch 2709/7596, Batch Loss: 21306.7070\n",
      "Epoch 1/5, Batch 2710/7596, Batch Loss: 32.1910\n",
      "Epoch 1/5, Batch 2711/7596, Batch Loss: 17472.9941\n",
      "Epoch 1/5, Batch 2712/7596, Batch Loss: 13604.7695\n",
      "Epoch 1/5, Batch 2713/7596, Batch Loss: 29247.6387\n",
      "Epoch 1/5, Batch 2714/7596, Batch Loss: 36194.9414\n",
      "Epoch 1/5, Batch 2715/7596, Batch Loss: 22841.2227\n",
      "Epoch 1/5, Batch 2716/7596, Batch Loss: 6441.9297\n",
      "Epoch 1/5, Batch 2717/7596, Batch Loss: 5433.6226\n",
      "Epoch 1/5, Batch 2718/7596, Batch Loss: 2274.7437\n",
      "Epoch 1/5, Batch 2719/7596, Batch Loss: 40577.7461\n",
      "Epoch 1/5, Batch 2720/7596, Batch Loss: 1189386.5000\n",
      "Epoch 1/5, Batch 2721/7596, Batch Loss: 311.5135\n",
      "Epoch 1/5, Batch 2722/7596, Batch Loss: 10532994.0000\n",
      "Epoch 1/5, Batch 2723/7596, Batch Loss: 298599.7812\n",
      "Epoch 1/5, Batch 2724/7596, Batch Loss: 305199.2500\n",
      "Epoch 1/5, Batch 2725/7596, Batch Loss: 62485.0859\n",
      "Epoch 1/5, Batch 2726/7596, Batch Loss: 1201989.0000\n",
      "Epoch 1/5, Batch 2727/7596, Batch Loss: 4341185.0000\n",
      "Epoch 1/5, Batch 2728/7596, Batch Loss: 77.8410\n",
      "Epoch 1/5, Batch 2729/7596, Batch Loss: 323388.7188\n",
      "Epoch 1/5, Batch 2730/7596, Batch Loss: 34.7129\n",
      "Epoch 1/5, Batch 2731/7596, Batch Loss: 237081.1875\n",
      "Epoch 1/5, Batch 2732/7596, Batch Loss: 39.4331\n",
      "Epoch 1/5, Batch 2733/7596, Batch Loss: 22.4757\n",
      "Epoch 1/5, Batch 2734/7596, Batch Loss: 127935.8750\n",
      "Epoch 1/5, Batch 2735/7596, Batch Loss: 269834.7188\n",
      "Epoch 1/5, Batch 2736/7596, Batch Loss: 111343.8281\n",
      "Epoch 1/5, Batch 2737/7596, Batch Loss: 90436.1875\n",
      "Epoch 1/5, Batch 2738/7596, Batch Loss: 22.7282\n",
      "Epoch 1/5, Batch 2739/7596, Batch Loss: 54.8479\n",
      "Epoch 1/5, Batch 2740/7596, Batch Loss: 53.5112\n",
      "Epoch 1/5, Batch 2741/7596, Batch Loss: 53623.3672\n",
      "Epoch 1/5, Batch 2742/7596, Batch Loss: 151221.1094\n",
      "Epoch 1/5, Batch 2743/7596, Batch Loss: 32.6365\n",
      "Epoch 1/5, Batch 2744/7596, Batch Loss: 174090.3750\n",
      "Epoch 1/5, Batch 2745/7596, Batch Loss: 264819.8438\n",
      "Epoch 1/5, Batch 2746/7596, Batch Loss: 58220.8867\n",
      "Epoch 1/5, Batch 2747/7596, Batch Loss: 31744.8379\n",
      "Epoch 1/5, Batch 2748/7596, Batch Loss: 360589.8438\n",
      "Epoch 1/5, Batch 2749/7596, Batch Loss: 178071.2812\n",
      "Epoch 1/5, Batch 2750/7596, Batch Loss: 235781.9375\n",
      "Epoch 1/5, Batch 2751/7596, Batch Loss: 14065.0488\n",
      "Epoch 1/5, Batch 2752/7596, Batch Loss: 75176.5469\n",
      "Epoch 1/5, Batch 2753/7596, Batch Loss: 26670.4316\n",
      "Epoch 1/5, Batch 2754/7596, Batch Loss: 40.4887\n",
      "Epoch 1/5, Batch 2755/7596, Batch Loss: 102164.7812\n",
      "Epoch 1/5, Batch 2756/7596, Batch Loss: 19646.3340\n",
      "Epoch 1/5, Batch 2757/7596, Batch Loss: 183928.5938\n",
      "Epoch 1/5, Batch 2758/7596, Batch Loss: 75841.6719\n",
      "Epoch 1/5, Batch 2759/7596, Batch Loss: 80223.6797\n",
      "Epoch 1/5, Batch 2760/7596, Batch Loss: 48057.5117\n",
      "Epoch 1/5, Batch 2761/7596, Batch Loss: 57230.4531\n",
      "Epoch 1/5, Batch 2762/7596, Batch Loss: 13516.7480\n",
      "Epoch 1/5, Batch 2763/7596, Batch Loss: 68331.3672\n",
      "Epoch 1/5, Batch 2764/7596, Batch Loss: 52207.0156\n",
      "Epoch 1/5, Batch 2765/7596, Batch Loss: 124637.6719\n",
      "Epoch 1/5, Batch 2766/7596, Batch Loss: 24561.6777\n",
      "Epoch 1/5, Batch 2767/7596, Batch Loss: 23468.9434\n",
      "Epoch 1/5, Batch 2768/7596, Batch Loss: 26658.4648\n",
      "Epoch 1/5, Batch 2769/7596, Batch Loss: 52516.3320\n",
      "Epoch 1/5, Batch 2770/7596, Batch Loss: 35403.8867\n",
      "Epoch 1/5, Batch 2771/7596, Batch Loss: 1105.5374\n",
      "Epoch 1/5, Batch 2772/7596, Batch Loss: 525160.4375\n",
      "Epoch 1/5, Batch 2773/7596, Batch Loss: 77750.9531\n",
      "Epoch 1/5, Batch 2774/7596, Batch Loss: 169110.7812\n",
      "Epoch 1/5, Batch 2775/7596, Batch Loss: 396335.8750\n",
      "Epoch 1/5, Batch 2776/7596, Batch Loss: 51164.1875\n",
      "Epoch 1/5, Batch 2777/7596, Batch Loss: 186377.2812\n",
      "Epoch 1/5, Batch 2778/7596, Batch Loss: 320158.5000\n",
      "Epoch 1/5, Batch 2779/7596, Batch Loss: 159798.4062\n",
      "Epoch 1/5, Batch 2780/7596, Batch Loss: 48316.7305\n",
      "Epoch 1/5, Batch 2781/7596, Batch Loss: 40506.1992\n",
      "Epoch 1/5, Batch 2782/7596, Batch Loss: 489822.6875\n",
      "Epoch 1/5, Batch 2783/7596, Batch Loss: 39843.3633\n",
      "Epoch 1/5, Batch 2784/7596, Batch Loss: 34962.9531\n",
      "Epoch 1/5, Batch 2785/7596, Batch Loss: 18272.7715\n",
      "Epoch 1/5, Batch 2786/7596, Batch Loss: 8095.9702\n",
      "Epoch 1/5, Batch 2787/7596, Batch Loss: 44585.4531\n",
      "Epoch 1/5, Batch 2788/7596, Batch Loss: 64862.3711\n",
      "Epoch 1/5, Batch 2789/7596, Batch Loss: 17384.8320\n",
      "Epoch 1/5, Batch 2790/7596, Batch Loss: 8550.6406\n",
      "Epoch 1/5, Batch 2791/7596, Batch Loss: 34618.8359\n",
      "Epoch 1/5, Batch 2792/7596, Batch Loss: 19898.0469\n",
      "Epoch 1/5, Batch 2793/7596, Batch Loss: 45853.1719\n",
      "Epoch 1/5, Batch 2794/7596, Batch Loss: 18433.3164\n",
      "Epoch 1/5, Batch 2795/7596, Batch Loss: 40813.0508\n",
      "Epoch 1/5, Batch 2796/7596, Batch Loss: 1542.9551\n",
      "Epoch 1/5, Batch 2797/7596, Batch Loss: 4043.0950\n",
      "Epoch 1/5, Batch 2798/7596, Batch Loss: 7913.2295\n",
      "Epoch 1/5, Batch 2799/7596, Batch Loss: 23166.3848\n",
      "Epoch 1/5, Batch 2800/7596, Batch Loss: 8133.7847\n",
      "Epoch 1/5, Batch 2801/7596, Batch Loss: 19793.1094\n",
      "Epoch 1/5, Batch 2802/7596, Batch Loss: 18268.5156\n",
      "Epoch 1/5, Batch 2803/7596, Batch Loss: 8616.1680\n",
      "Epoch 1/5, Batch 2804/7596, Batch Loss: 24950.2617\n",
      "Epoch 1/5, Batch 2805/7596, Batch Loss: 11616.9707\n",
      "Epoch 1/5, Batch 2806/7596, Batch Loss: 11854.5293\n",
      "Epoch 1/5, Batch 2807/7596, Batch Loss: 9290.9062\n",
      "Epoch 1/5, Batch 2808/7596, Batch Loss: 1108812.3750\n",
      "Epoch 1/5, Batch 2809/7596, Batch Loss: 26284.0195\n",
      "Epoch 1/5, Batch 2810/7596, Batch Loss: 74184.0234\n",
      "Epoch 1/5, Batch 2811/7596, Batch Loss: 61442.6719\n",
      "Epoch 1/5, Batch 2812/7596, Batch Loss: 297849.9688\n",
      "Epoch 1/5, Batch 2813/7596, Batch Loss: 56288.8438\n",
      "Epoch 1/5, Batch 2814/7596, Batch Loss: 21445.3828\n",
      "Epoch 1/5, Batch 2815/7596, Batch Loss: 18043.2324\n",
      "Epoch 1/5, Batch 2816/7596, Batch Loss: 80225.8594\n",
      "Epoch 1/5, Batch 2817/7596, Batch Loss: 153683.1719\n",
      "Epoch 1/5, Batch 2818/7596, Batch Loss: 24330.9648\n",
      "Epoch 1/5, Batch 2819/7596, Batch Loss: 97536.7031\n",
      "Epoch 1/5, Batch 2820/7596, Batch Loss: 115646.4844\n",
      "Epoch 1/5, Batch 2821/7596, Batch Loss: 16906.2031\n",
      "Epoch 1/5, Batch 2822/7596, Batch Loss: 39.6971\n",
      "Epoch 1/5, Batch 2823/7596, Batch Loss: 26556.3770\n",
      "Epoch 1/5, Batch 2824/7596, Batch Loss: 148568.0156\n",
      "Epoch 1/5, Batch 2825/7596, Batch Loss: 48293.1992\n",
      "Epoch 1/5, Batch 2826/7596, Batch Loss: 13502.3438\n",
      "Epoch 1/5, Batch 2827/7596, Batch Loss: 72717.4766\n",
      "Epoch 1/5, Batch 2828/7596, Batch Loss: 3611.4849\n",
      "Epoch 1/5, Batch 2829/7596, Batch Loss: 22062.6582\n",
      "Epoch 1/5, Batch 2830/7596, Batch Loss: 28467.6387\n",
      "Epoch 1/5, Batch 2831/7596, Batch Loss: 4992.3594\n",
      "Epoch 1/5, Batch 2832/7596, Batch Loss: 123910.1875\n",
      "Epoch 1/5, Batch 2833/7596, Batch Loss: 105210.5781\n",
      "Epoch 1/5, Batch 2834/7596, Batch Loss: 5097.1675\n",
      "Epoch 1/5, Batch 2835/7596, Batch Loss: 12215.8398\n",
      "Epoch 1/5, Batch 2836/7596, Batch Loss: 139186.8438\n",
      "Epoch 1/5, Batch 2837/7596, Batch Loss: 142517.3125\n",
      "Epoch 1/5, Batch 2838/7596, Batch Loss: 111.4434\n",
      "Epoch 1/5, Batch 2839/7596, Batch Loss: 35489.1094\n",
      "Epoch 1/5, Batch 2840/7596, Batch Loss: 45734.1719\n",
      "Epoch 1/5, Batch 2841/7596, Batch Loss: 186305.7344\n",
      "Epoch 1/5, Batch 2842/7596, Batch Loss: 185731.9219\n",
      "Epoch 1/5, Batch 2843/7596, Batch Loss: 2667349.5000\n",
      "Epoch 1/5, Batch 2844/7596, Batch Loss: 1678891.5000\n",
      "Epoch 1/5, Batch 2845/7596, Batch Loss: 234368.7188\n",
      "Epoch 1/5, Batch 2846/7596, Batch Loss: 28313.7520\n",
      "Epoch 1/5, Batch 2847/7596, Batch Loss: 48438.5820\n",
      "Epoch 1/5, Batch 2848/7596, Batch Loss: 19350.2773\n",
      "Epoch 1/5, Batch 2849/7596, Batch Loss: 25058.1055\n",
      "Epoch 1/5, Batch 2850/7596, Batch Loss: 30.1736\n",
      "Epoch 1/5, Batch 2851/7596, Batch Loss: 47162.5625\n",
      "Epoch 1/5, Batch 2852/7596, Batch Loss: 11212.5449\n",
      "Epoch 1/5, Batch 2853/7596, Batch Loss: 154.7500\n",
      "Epoch 1/5, Batch 2854/7596, Batch Loss: 11335.9678\n",
      "Epoch 1/5, Batch 2855/7596, Batch Loss: 8131.9028\n",
      "Epoch 1/5, Batch 2856/7596, Batch Loss: 23960.0312\n",
      "Epoch 1/5, Batch 2857/7596, Batch Loss: 7056.3521\n",
      "Epoch 1/5, Batch 2858/7596, Batch Loss: 16107.6855\n",
      "Epoch 1/5, Batch 2859/7596, Batch Loss: 1093.0312\n",
      "Epoch 1/5, Batch 2860/7596, Batch Loss: 10306.8984\n",
      "Epoch 1/5, Batch 2861/7596, Batch Loss: 90.9237\n",
      "Epoch 1/5, Batch 2862/7596, Batch Loss: 140.9056\n",
      "Epoch 1/5, Batch 2863/7596, Batch Loss: 15339.8584\n",
      "Epoch 1/5, Batch 2864/7596, Batch Loss: 28075.3730\n",
      "Epoch 1/5, Batch 2865/7596, Batch Loss: 18239.3184\n",
      "Epoch 1/5, Batch 2866/7596, Batch Loss: 39293.4453\n",
      "Epoch 1/5, Batch 2867/7596, Batch Loss: 51560.3789\n",
      "Epoch 1/5, Batch 2868/7596, Batch Loss: 28064.5234\n",
      "Epoch 1/5, Batch 2869/7596, Batch Loss: 29976.7734\n",
      "Epoch 1/5, Batch 2870/7596, Batch Loss: 19209.5625\n",
      "Epoch 1/5, Batch 2871/7596, Batch Loss: 31655.8438\n",
      "Epoch 1/5, Batch 2872/7596, Batch Loss: 44847.1992\n",
      "Epoch 1/5, Batch 2873/7596, Batch Loss: 124821.1094\n",
      "Epoch 1/5, Batch 2874/7596, Batch Loss: 66849.2109\n",
      "Epoch 1/5, Batch 2875/7596, Batch Loss: 40047.4492\n",
      "Epoch 1/5, Batch 2876/7596, Batch Loss: 443686.5312\n",
      "Epoch 1/5, Batch 2877/7596, Batch Loss: 193671.1406\n",
      "Epoch 1/5, Batch 2878/7596, Batch Loss: 1595737.5000\n",
      "Epoch 1/5, Batch 2879/7596, Batch Loss: 7894.1641\n",
      "Epoch 1/5, Batch 2880/7596, Batch Loss: 515621.8750\n",
      "Epoch 1/5, Batch 2881/7596, Batch Loss: 240603.2656\n",
      "Epoch 1/5, Batch 2882/7596, Batch Loss: 77531.3281\n",
      "Epoch 1/5, Batch 2883/7596, Batch Loss: 188370.9844\n",
      "Epoch 1/5, Batch 2884/7596, Batch Loss: 81616.5938\n",
      "Epoch 1/5, Batch 2885/7596, Batch Loss: 38194.3281\n",
      "Epoch 1/5, Batch 2886/7596, Batch Loss: 82056.7188\n",
      "Epoch 1/5, Batch 2887/7596, Batch Loss: 23471.2051\n",
      "Epoch 1/5, Batch 2888/7596, Batch Loss: 14396.2910\n",
      "Epoch 1/5, Batch 2889/7596, Batch Loss: 20.2985\n",
      "Epoch 1/5, Batch 2890/7596, Batch Loss: 30.2546\n",
      "Epoch 1/5, Batch 2891/7596, Batch Loss: 35.0189\n",
      "Epoch 1/5, Batch 2892/7596, Batch Loss: 209367.7344\n",
      "Epoch 1/5, Batch 2893/7596, Batch Loss: 38.2916\n",
      "Epoch 1/5, Batch 2894/7596, Batch Loss: 87339.9141\n",
      "Epoch 1/5, Batch 2895/7596, Batch Loss: 239408.2031\n",
      "Epoch 1/5, Batch 2896/7596, Batch Loss: 49296.0938\n",
      "Epoch 1/5, Batch 2897/7596, Batch Loss: 36634.3203\n",
      "Epoch 1/5, Batch 2898/7596, Batch Loss: 64963.3516\n",
      "Epoch 1/5, Batch 2899/7596, Batch Loss: 31869.1289\n",
      "Epoch 1/5, Batch 2900/7596, Batch Loss: 122720.7109\n",
      "Epoch 1/5, Batch 2901/7596, Batch Loss: 24267.3320\n",
      "Epoch 1/5, Batch 2902/7596, Batch Loss: 40415.2070\n",
      "Epoch 1/5, Batch 2903/7596, Batch Loss: 22250.2441\n",
      "Epoch 1/5, Batch 2904/7596, Batch Loss: 14094.2822\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Initialize total training and validation times\n",
    "total_training_time = 0\n",
    "total_validation_time = 0\n",
    "\n",
    "# Open a file to write logs\n",
    "with open('training_log_2.txt', 'a') as log_file:\n",
    "    for epoch in range(0, num_epochs):\n",
    "        try:\n",
    "            # Start timing for training\n",
    "            epoch_training_start_time = time.time()\n",
    "            model.train()  # Setting model to training mode\n",
    "            train_loss = 0  # Initialize epoch-level loss\n",
    "            batch_losses = []  # List to store batch losses for logging\n",
    "\n",
    "            for batch_idx, (images, targets) in enumerate(train_loader):  # Iterate over data\n",
    "                # Skip if the batch is empty (due to missing images)\n",
    "                if images is None or targets is None:\n",
    "                    log_file.write(f\"Skipping empty batch {batch_idx + 1}\\n\")\n",
    "                    continue\n",
    "\n",
    "                images = list(image.to(device) for image in images)  # Moving all the images to device\n",
    "                targets = [{key: value.to(device) for key, value in t.items()} for t in targets]  # Target will have labels and BB, used dict comprehension\n",
    "\n",
    "                # Loss dictionary\n",
    "                optimizer.zero_grad()  # Makes gradient zero for next iteration\n",
    "                loss_dict = model(images, targets)  # Gets the loss\n",
    "                losses = sum(loss for loss in loss_dict.values())  # Adds the loss\n",
    "\n",
    "                losses.backward()  # Backprop\n",
    "                optimizer.step()  # Step\n",
    "\n",
    "                train_loss += losses.item()\n",
    "\n",
    "                batch_loss = losses.item()\n",
    "                batch_losses.append(batch_loss)\n",
    "\n",
    "                # Write batch loss to file\n",
    "                log_file.write(f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}/{len(train_loader)}, Batch Loss: {batch_loss:.4f}\\n\")\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}/{len(train_loader)}, Batch Loss: {batch_loss:.4f}\")\n",
    "\n",
    "            # Calculate average epoch loss\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "            # End timing for training\n",
    "            epoch_training_end_time = time.time()\n",
    "            epoch_training_time = epoch_training_end_time - epoch_training_start_time\n",
    "            total_training_time += epoch_training_time\n",
    "\n",
    "            log_file.write(f\"Epoch {epoch + 1}/{num_epochs} - Average Train Loss: {avg_train_loss:.4f}\\n\")\n",
    "            log_file.write(f\"Epoch {epoch + 1}/{num_epochs} - Training Time: {epoch_training_time:.2f} seconds\\n\")\n",
    "\n",
    "            # Update learning rate\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # Start timing for validation\n",
    "            epoch_validation_start_time = time.time()\n",
    "\n",
    "            mAP = compute_mAP(model, val_loader, device, ground_truths, confidence_threshold, iou_threshold, iou_function)\n",
    "\n",
    "            # End timing for validation\n",
    "            epoch_validation_end_time = time.time()\n",
    "            epoch_validation_time = epoch_validation_end_time - epoch_validation_start_time\n",
    "            total_validation_time += epoch_validation_time\n",
    "\n",
    "            log_file.write(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, mAP: {mAP:.4f}\\n\")\n",
    "            log_file.write(f\"Epoch {epoch + 1}/{num_epochs} - Validation Time: {epoch_validation_time:.2f} seconds\\n\")\n",
    "\n",
    "            # Save the best model based on mAP\n",
    "            if mAP > best_mAP:\n",
    "                best_mAP = mAP\n",
    "                # torch.save(model.state_dict(), 'best_model.pth')\n",
    "                # Save the model's state dictionary after every epoch\n",
    "            model_path = f\"fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            # Optional log for model save\n",
    "            log_file.write(f\"Epoch {epoch + 1}/{num_epochs} - Model saved to {model_path}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # If any error occurs, log it\n",
    "            log_file.write(f\"Error during epoch {epoch + 1}: {str(e)}\\n\")\n",
    "            print(f\"Error during epoch {epoch + 1}: {str(e)}\")\n",
    "\n",
    "# Log the total training and validation times\n",
    "with open('training_log_2.txt', 'a') as log_file:\n",
    "    log_file.write(f\"Total Training Time: {total_training_time:.2f} seconds\\n\")\n",
    "    log_file.write(f\"Total Validation Time: {total_validation_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86a658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0347c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "844e22d1",
   "metadata": {},
   "source": [
    "## Training finished now only inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3f1c8",
   "metadata": {},
   "source": [
    "#### Loading the saved model and setting it for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_faster_rcnn_model_with_class(num_classes)  # 1 classes + background\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.to(device)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4631d7",
   "metadata": {},
   "source": [
    "#### visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23947068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, boxes, labels, scores, threshold=0.5, size = (12, 8)):\n",
    "    fig, ax = plt.subplots(1, figsize= size) #initialize the axis\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold: #score check\n",
    "            x, y, w, h = box\n",
    "            rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x, y, f'{label}: {score:.2f}', color='white', backgroundcolor='red', fontsize=8)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title('Predicted bounding boxes', fontsize = 10)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ea151",
   "metadata": {},
   "source": [
    "#### If you want to run the loop on all the validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d607bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on a few validation images\n",
    "\n",
    "fig_size = (10,10) \n",
    "threshold_value = 0.1\n",
    "\n",
    "for images, targets in val_loader:\n",
    "    images = list(image.to(device) for image in images) #send to device\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    for i, (image, prediction) in enumerate(zip(images, predictions)):\n",
    "#         image = np.transpose(image.cpu().numpy(), (1,2,0))\n",
    "        image = image.cpu().permute(1, 2, 0).numpy()  # Convert to X,Y,Z format as image is stored as channel, X, Y\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        labels = prediction['labels'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "        visualize_predictions(image, boxes, labels, scores, threshold=threshold_value, size = fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069197e",
   "metadata": {},
   "source": [
    "## Validation on 1 image at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image with bounding boxes\n",
    "def plot_image_with_pred_boxes(image, boxes, labels):\n",
    "    fig, ax = plt.subplots(1, figsize= (10,10)) #no of rows, figure size\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='w', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(xmin, ymin, f'Class: {category_id[label]}', color='white', fontsize=12)\n",
    "    \n",
    "    plt.title('Image with predicted bounding boxes', fontsize= 10)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_path = 'data/test/q5khrnrwimu71.jpg'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "category_id = ('Background', 'Vegeta')\n",
    "confidence_threshold = 0.002\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    prediction = model(image_tensor)\n",
    "\n",
    "# Extract the predictions\n",
    "prediction = prediction[0]  # Remove batch dimension\n",
    "boxes = prediction['boxes'].cpu().numpy()\n",
    "scores = prediction['scores'].cpu().numpy()\n",
    "labels = prediction['labels'].cpu().numpy()\n",
    "\n",
    "# Filter predictions by confidence threshold\n",
    "\n",
    "filtered_boxes = boxes[scores > confidence_threshold]\n",
    "filtered_labels = labels[scores > confidence_threshold]\n",
    "\n",
    "# Visualize the results\n",
    "plot_image_with_pred_boxes(image, filtered_boxes, filtered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d2914",
   "metadata": {},
   "source": [
    "#### if you want to get both predicted and original bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e93eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image with predicted and ground truth bounding boxes\n",
    "def plot_image_with_gt_and_pred_boxes(image, pred_boxes, pred_labels, gt_boxes, gt_labels):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Draw predicted boxes (in red)\n",
    "    for box, label in zip(pred_boxes, pred_labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(xmin, ymin, f'Pred: {label}', color='r', fontsize=10, backgroundcolor='white')\n",
    "    \n",
    "    # Draw ground truth boxes (in green)\n",
    "    for box, label in zip(gt_boxes, gt_labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(xmin, ymin, f'GT: {label}', color='g', fontsize=10, backgroundcolor='white')\n",
    "    \n",
    "    plt.title('Image with predicted and ground truth bounding boxes', fontsize= 10)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define transforms for the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_path = 'data/val/majin-vegeta-and-other-dragon-ball-characters-tajyteiulwizv8zz.jpg'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "\n",
    "annotation_path = 'data/val_annot/val_an_coco.json'\n",
    "coco = COCO(annotation_path)\n",
    "\n",
    "# Find the image ID by matching the filename\n",
    "image_filename = image_path.split('/')[-1]  # Extract filename from path\n",
    "image_id = None\n",
    "for img_info in coco.dataset['images']:\n",
    "    if img_info['file_name'] == image_filename: # stop when you find the image\n",
    "        image_id = img_info['id']\n",
    "        break\n",
    "\n",
    "if image_id is None:\n",
    "    raise ValueError(f\"Image filename '{image_filename}' not found in annotations.\")\n",
    "\n",
    "# Extract ground truth boxes and labels\n",
    "ann_ids = coco.getAnnIds(imgIds=image_id)\n",
    "gt_annotations = coco.loadAnns(ann_ids) #load the annotations\n",
    "\n",
    "# Extract ground truth boxes and labels\n",
    "gt_boxes = []\n",
    "gt_labels = []\n",
    "for ann in gt_annotations:\n",
    "    xmin, ymin, width, height = ann['bbox']\n",
    "    xmax = xmin + width\n",
    "    ymax = ymin + height\n",
    "    gt_boxes.append([xmin, ymin, xmax, ymax])\n",
    "    gt_labels.append(ann['category_id'])\n",
    "    \n",
    "\n",
    "# Visualize the results\n",
    "plot_image_with_gt_and_pred_boxes(image, filtered_boxes, filtered_labels, gt_boxes, gt_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
